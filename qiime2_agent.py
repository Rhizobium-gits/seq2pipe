#!/usr/bin/env python3
# coding: utf-8
"""
seq2pipe  â€”  sequence â†’ pipeline
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆOllamaï¼‰ã‚’ä½¿ã£ãŸãƒã‚¤ã‚¯ãƒ­ãƒã‚¤ã‚ªãƒ¼ãƒ è§£æ AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
ç”Ÿé…åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Šã€QIIME2 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™

ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: Python æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ï¼ˆå¤–éƒ¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸è¦ï¼‰
å¿…è¦ãƒ„ãƒ¼ãƒ«   : Ollama (setup.sh ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«), Docker Desktop
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import datetime
import json
import os
import re
import shutil
import socket
import subprocess
import sys
import tempfile
import urllib.error
import urllib.request
from pathlib import Path
from typing import Optional

# ğŸº ======================================================================
# ğŸ± è¨­å®š
# ğŸº ======================================================================
OLLAMA_URL = "http://localhost:11434/api/chat"
DEFAULT_MODEL = os.environ.get("QIIME2_AI_MODEL", "qwen2.5-coder:7b")
# ğŸ± CPU å°‚ç”¨ç’°å¢ƒï¼ˆCodespaces ç­‰ï¼‰ã§ã®åˆå›æ¨è«–ã«å¯¾å¿œã™ã‚‹ãŸã‚ 600 ç§’ã«è¨­å®š
# ğŸ± ç’°å¢ƒå¤‰æ•° OLLAMA_TIMEOUT ã§ä¸Šæ›¸ãå¯èƒ½
OLLAMA_TIMEOUT = int(os.environ.get("OLLAMA_TIMEOUT", "600"))
# ğŸ± execute_python ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆissue #32: 300s â†’ 600s ã«å»¶é•·, ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯ï¼‰
PYTHON_EXEC_TIMEOUT = int(os.environ.get("SEQ2PIPE_PYTHON_TIMEOUT", "600"))
# ğŸ± ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã®æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆissue #33: 30 â†’ 100, ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯ï¼‰
MAX_AGENT_STEPS = int(os.environ.get("SEQ2PIPE_MAX_STEPS", "100"))
# ğŸ± è‡ªå¾‹ãƒ¢ãƒ¼ãƒ‰: SEQ2PIPE_AUTO_YES=1 ã§ã‚³ãƒãƒ³ãƒ‰ç¢ºèªã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆissue #31ï¼‰
AUTO_YES = os.environ.get("SEQ2PIPE_AUTO_YES", "0") == "1"
SCRIPT_DIR = Path(__file__).parent.resolve()

# ğŸ± QIIME2 conda ç’°å¢ƒã®è‡ªå‹•æ¤œå‡º
def _find_qiime2_conda_bin() -> str:
    """QIIME2 conda ç’°å¢ƒã® bin ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹"""
    candidates = [
        Path.home() / "miniforge3/envs/qiime2/bin",
        Path.home() / "miniconda3/envs/qiime2/bin",
        Path.home() / "anaconda3/envs/qiime2/bin",
        Path.home() / "mambaforge/envs/qiime2/bin",
        Path("/opt/miniconda3/envs/qiime2/bin"),
        Path("/opt/miniforge3/envs/qiime2/bin"),
    ]
    # ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯
    env_override = os.environ.get("QIIME2_CONDA_BIN", "")
    if env_override and Path(env_override).exists():
        return env_override
    for p in candidates:
        if p.exists() and (p / "qiime").exists():
            return str(p)
    return ""

QIIME2_CONDA_BIN: str = _find_qiime2_conda_bin()
QIIME2_PYTHON: str = str(Path(QIIME2_CONDA_BIN) / "python3") if QIIME2_CONDA_BIN else sys.executable

# ğŸº ======================================================================
# ğŸ± ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ï¼ˆãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ è§£æãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼‰
# ğŸº ======================================================================
ANALYSIS_LOG: list = []        # å®Ÿè¡Œã—ãŸè§£æã®è¨˜éŒ²
SESSION_OUTPUT_DIR: str = ""   # ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã®å‡ºåŠ›ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆèµ·å‹•æ™‚ã«ä½œæˆï¼‰
SESSION_FIGURE_DIR: str = ""   # å›³ã®å‡ºåŠ›å…ˆï¼ˆSESSION_OUTPUT_DIR/figures/ ã«åŒæœŸï¼‰
PLOT_CONFIG: dict = {         # å›³ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
    "style": "seaborn-v0_8-whitegrid",
    "palette": "Set2",
    "figsize": [10, 6],
    "dpi": 150,
    "font_size": 12,
    "title_font_size": 14,
    "format": "pdf",           # ä¿å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: pdf / png / svg
}

# ğŸº ======================================================================
# ğŸ± è¨€èªè¨­å®šï¼ˆselect_language() ã§èµ·å‹•æ™‚ã«è¨­å®šï¼‰
# ğŸº ======================================================================
LANG: str = "ja"  # "ja" | "en"

_UI: dict = {
    "ja": {
        "model_selected": "âœ… ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {}",
        "hint_exit":      "ãƒ’ãƒ³ãƒˆ: çµ‚äº†ã™ã‚‹ã«ã¯ Ctrl+C ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚",
        "prompt":         "ã‚ãªãŸ",
        "tool_exec":      "ğŸ”§ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ: {}",
        "tool_result":    "ğŸ“‹ å®Ÿè¡Œçµæœ:",
        "goodbye":        "ğŸ‘‹ çµ‚äº†ã—ã¾ã™ã€‚ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼",
        "ollama_error":   "âŒ Ollama ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ã€‚",
        "ollama_hint":    "ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œã—ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„:",
        "ollama_hint2":   "Ollama ãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å ´åˆ:",
        "no_model":       "âš ï¸  Ollama ã«ãƒ¢ãƒ‡ãƒ«ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
        "no_model_hint":  "æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {}",
        "no_model_hint2": "è»½é‡ç‰ˆ    : {}",
        "runtime_error":    "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {}",
        "cmd_request":      "âš¡ ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆ",
        "cmd_desc":         "èª¬æ˜",
        "cmd_cmd":          "ã‚³ãƒãƒ³ãƒ‰",
        "cmd_confirm":      "[y] å®Ÿè¡Œã™ã‚‹  [n] ã‚­ãƒ£ãƒ³ã‚»ãƒ«",
        "cmd_cancelled_ki": "âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸï¼ˆã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å‰²ã‚Šè¾¼ã¿ï¼‰",
        "cmd_cancelled":    "âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚",
        "agent_limit":      "âš ï¸  æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•° ({}) ã«é”ã—ã¾ã—ãŸã€‚ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†ã—ã¾ã™ã€‚",
        "deps_ok":          "âœ… Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç¢ºèªæ¸ˆã¿ï¼ˆnumpy/pandas/matplotlib/seabornï¼‰",
        "deps_warn":        "âš ï¸  Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {}",
        "deps_hint":        "execute_python ãƒ„ãƒ¼ãƒ«ãŒæ­£ã—ãå‹•ä½œã—ãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚",
        "deps_hint2":       "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•: {}",
        "auto_approve":     "[è‡ªå¾‹ãƒ¢ãƒ¼ãƒ‰] ã‚³ãƒãƒ³ãƒ‰ã‚’è‡ªå‹•æ‰¿èªã—ã¾ã™",
        "empty_response":   "âš ï¸  AI ã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚å†è©¦è¡Œã—ã¾ã™...",
        "pkg_warning":      "[è­¦å‘Š] ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸è¶³: {}",
        "pkg_hint":         "pip install numpy pandas matplotlib seaborn ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„",
        "select_error":     "1 ã‹ 2 ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        "qiime2_python":    "QIIME2 conda Python ã‚’ä½¿ç”¨: {}",
        "session_dir":      "ğŸ“ å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {}",
        "session_dir_hint": "   è§£æçµæœãƒ»å›³ãƒ»ãƒ¬ãƒãƒ¼ãƒˆã¯ã™ã¹ã¦ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã™",
    },
    "en": {
        "model_selected":   "âœ… Model: {}",
        "hint_exit":        "Tip: Press Ctrl+C to exit.",
        "prompt":           "You",
        "tool_exec":        "ğŸ”§ Tool: {}",
        "tool_result":      "ğŸ“‹ Result:",
        "goodbye":          "ğŸ‘‹ Goodbye!",
        "ollama_error":     "âŒ Ollama is not running.",
        "ollama_hint":      "Run the following command in another terminal:",
        "ollama_hint2":     "If Ollama is not installed:",
        "no_model":         "âš ï¸  No models installed in Ollama.",
        "no_model_hint":    "Recommended model: {}",
        "no_model_hint2":   "Lightweight: {}",
        "runtime_error":    "An error occurred: {}",
        "cmd_request":      "âš¡ Command Execution Request",
        "cmd_desc":         "Description",
        "cmd_cmd":          "Command",
        "cmd_confirm":      "[y] Execute  [n] Cancel",
        "cmd_cancelled_ki": "âŒ Cancelled (keyboard interrupt)",
        "cmd_cancelled":    "âŒ Cancelled by user.",
        "agent_limit":      "âš ï¸  Max steps ({}) reached. Stopping loop.",
        "deps_ok":          "âœ… Python packages verified (numpy/pandas/matplotlib/seaborn)",
        "deps_warn":        "âš ï¸  Missing Python packages: {}",
        "deps_hint":        "The execute_python tool may not work correctly.",
        "deps_hint2":       "To install: {}",
        "auto_approve":     "[Auto mode] Command approved automatically",
        "empty_response":   "âš ï¸  Empty response from AI. Retrying...",
        "pkg_warning":      "[WARNING] Missing package: {}",
        "pkg_hint":         "Please run: pip install numpy pandas matplotlib seaborn",
        "select_error":     "Please enter 1 or 2",
        "qiime2_python":    "Using QIIME2 conda Python: {}",
        "session_dir":      "ğŸ“ Output directory: {}",
        "session_dir_hint": "   All analysis results, figures, and reports will be saved here",
    },
}


def ui(key: str, *args) -> str:
    """ç¾åœ¨ã® LANG ã«å¯¾å¿œã™ã‚‹ UI æ–‡å­—åˆ—ã‚’è¿”ã™"""
    tmpl = _UI.get(LANG, _UI["ja"]).get(key, key)
    return tmpl.format(*args) if args else tmpl


# ğŸº ======================================================================
# ğŸ± ANSI ã‚«ãƒ©ãƒ¼
# ğŸº ======================================================================
RESET = "\033[0m"
BOLD = "\033[1m"
GREEN = "\033[32m"
CYAN = "\033[36m"
YELLOW = "\033[33m"
RED = "\033[31m"
BLUE = "\033[34m"
MAGENTA = "\033[35m"
DIM = "\033[2m"


def c(text, color):
    return f"{color}{text}{RESET}"


# ğŸº ======================================================================
# ğŸ± ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆQIIME2 ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’åŸ‹ã‚è¾¼ã¿ï¼‰
# ğŸº ======================================================================
SYSTEM_PROMPT = """ã‚ãªãŸã¯ QIIME2ï¼ˆQuantitative Insights Into Microbial Ecology 2ï¼‰ã®å°‚é–€ AI ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒã‚¤ã‚¯ãƒ­ãƒã‚¤ã‚ªãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’è§£æã—ã€æœ€é©ãª QIIME2 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è‡ªå‹•æ§‹ç¯‰ã—ã¾ã™ã€‚

â”â”â” è¡Œå‹•åŸå‰‡ï¼ˆæœ€å„ªå…ˆï¼‰ â”â”â”
1. ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’å—ã‘ãŸã‚‰ã€é•·ã„èª¬æ˜ã‚ˆã‚Šå…ˆã«ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ã€‚
2. ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‹ã‚‰å§‹ã‚ã‚‹: ãƒ‘ã‚¹ãŒæç¤ºã•ã‚ŒãŸã‚‰å¿…ãš inspect_directory â†’ read_file ã§ãƒ‡ãƒ¼ã‚¿ã‚’æŠŠæ¡ã—ã¦ã‹ã‚‰ææ¡ˆã™ã‚‹ã€‚
3. å®Ÿé¨“æƒ…å ±ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åæ˜ : ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæä¾›ã™ã‚‹ã‚¢ãƒ³ãƒ—ãƒªã‚³ãƒ³é ˜åŸŸãƒ»ãƒ—ãƒ©ã‚¤ãƒãƒ¼ãƒ»æ¯”è¼ƒã‚°ãƒ«ãƒ¼ãƒ—ã‚’
   DADA2 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»åˆ†é¡å™¨ãƒ»å·®æ¬¡è§£æã®è¨­å®šã«ç›´æ¥ä½¿ã†ã€‚
4. ã‚¨ãƒ©ãƒ¼ã¯è‡ªåŠ›ã§è¨ºæ–­ãƒ»ä¿®æ­£: ãƒ„ãƒ¼ãƒ«ãŒå¤±æ•—ã—ãŸã‚‰åŸå› ã‚’åˆ†æã—ã€åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å³åº§ã«è©¦ã¿ã‚‹ã€‚
5. ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã¯æ—¥æœ¬èªã‚³ãƒ¡ãƒ³ãƒˆã‚’ä»˜ã‘ã‚‹ã€‚
6. QIIME2 ã¯ conda ç’°å¢ƒã§ç›´æ¥å®Ÿè¡Œã™ã‚‹ï¼ˆDocker ä¸è¦ï¼‰ã€‚qiime ã‚³ãƒãƒ³ãƒ‰ã¯ãã®ã¾ã¾ run_command ã«æ¸¡ã™ã€‚
7. è§£æã¯ä¸€åº¦ã«1ã‚¹ãƒ†ãƒƒãƒ—ãšã¤å®Ÿè¡Œã—ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ„ãƒ¼ãƒ«çµæœã‚’ç¢ºèªã—ã¦ã‹ã‚‰æ¬¡ã¸é€²ã‚€ã€‚
8. ãƒ„ãƒ¼ãƒ«åã¯ä¸‹è¨˜ãƒªã‚¹ãƒˆã«ã‚ã‚‹æ­£ç¢ºãªåå‰ã®ã¿ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚

â”â”â” åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ï¼ˆã“ã®åå‰ã®ã¿æœ‰åŠ¹ï¼‰ â”â”â”
- inspect_directory    : ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹ã‚’ä¸€è¦§è¡¨ç¤º
- read_file            : ãƒ†ã‚­ã‚¹ãƒˆ/TSV/CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
- check_system         : QIIME2ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒã‚’ç¢ºèª
- write_file           : ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãå‡ºã™ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ»ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆç­‰ï¼‰
- generate_manifest    : FASTQãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰QIIME2ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆTSVã‚’è‡ªå‹•ç”Ÿæˆ
- edit_file            : æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—åˆ—ç½®æ›ã§ç·¨é›†
- run_command          : å˜ç™ºã‚·ã‚§ãƒ«ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆè¿½åŠ ãƒ»ä¿®æ­£ãŒå¿…è¦ãªå ´åˆã®ã¿ï¼‰
- run_qiime2_pipeline  : â˜… QIIME2è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’ä¸€æ‹¬è‡ªå‹•å®Ÿè¡Œï¼ˆãƒ¡ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«ï¼‰
- set_plot_config      : å›³ã®ã‚¹ã‚¿ã‚¤ãƒ«ãƒ»DPIãƒ»ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’è¨­å®š
- execute_python       : Pythonã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œï¼ˆpandas/matplotlib/seabornã§å¯è¦–åŒ–ï¼‰
- log_analysis_step    : è§£æã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
- build_report_tex     : è§£æçµæœã‚’ã¾ã¨ã‚ãŸPDFãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆæœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ï¼‰

âš ï¸ ã€Œgenerate_reportã€ã€Œcompile_reportã€ã€Œcreate_reportã€ãªã©ã¯å­˜åœ¨ã—ãªã„ã€‚ãƒ¬ãƒãƒ¼ãƒˆã¯å¿…ãšã€Œbuild_report_texã€ã‚’ä½¿ã†ã“ã¨ã€‚

â”â”â” è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œæ‰‹é †ï¼ˆã“ã®é †ã«å®Ÿè¡Œï¼‰ â”â”â”
STEP 1: inspect_directory â†’ FASTQãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èª¿æŸ»
STEP 2: read_file â†’ sample-metadata.tsv ã‚’èª­ã‚“ã§ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ãƒ»åˆ—åã‚’æŠŠæ¡
STEP 3: set_plot_config â†’ è«–æ–‡å‘ã‘è¨­å®šï¼ˆdpi=300, style=whitegridç­‰ï¼‰ã‚’é©ç”¨
STEP 4: run_qiime2_pipeline â†’ QIIME2ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’ä¸€æ‹¬å®Ÿè¡Œ
         ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆâ†’DADA2â†’ç³»çµ±ç™ºç”Ÿãƒ„ãƒªãƒ¼â†’åˆ†é¡â†’å¤šæ§˜æ€§è§£æã‚’å…¨ã¦è‡ªå‹•å®Ÿè¡Œï¼‰
STEP 5: execute_python â†’ å±ãƒ¬ãƒ™ãƒ«çµ„æˆãƒ»Î±å¤šæ§˜æ€§ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆï¼ˆFIGURE_DIR ã«ä¿å­˜ï¼‰
STEP 6: build_report_tex â†’ å…¨è§£æã‚’ã¾ã¨ã‚ãŸPDFãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

â˜… run_qiime2_pipeline ã¯ QIIME2ã®å…¨ã‚³ã‚¢ã‚¹ãƒ†ãƒƒãƒ—ã‚’å†…éƒ¨ã§è‡ªå‹•å®Ÿè¡Œã™ã‚‹ã€‚
  å€‹åˆ¥ã« run_command ã§ qiime ã‚³ãƒãƒ³ãƒ‰ã‚’å‘¼ã¶å¿…è¦ã¯ãªã„ã€‚
  inspector ã®çµæœã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã‚“ã ã‚‰ã™ãã« run_qiime2_pipeline ã‚’å‘¼ã¶ã“ã¨ã€‚

â”â”â” ã‚ãªãŸã®å½¹å‰² â”â”â”
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’èª¿æŸ»ã™ã‚‹
2. ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ï¼ˆFASTQãƒ»ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰ã‚’è‡ªå‹•åˆ¤å®šã™ã‚‹
3. å®Ÿé¨“ç³»ã®èª¬æ˜ï¼ˆé ˜åŸŸãƒ»ãƒ—ãƒ©ã‚¤ãƒãƒ¼ãƒ»ç¾¤æ§‹æˆï¼‰ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±ºå®šã™ã‚‹
4. ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ãŸæœ€é©ãª QIIME2 è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è‡ªå‹•å®Ÿè¡Œã™ã‚‹
5. å®Ÿè¡Œå¯èƒ½ãªã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ»ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹
6. è§£æçµæœã®å¯è¦–åŒ–ã‚’ Python ã§è¡Œã„ã€PDF ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹

â”â”â” å®Ÿé¨“æƒ…å ± â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¯¾å¿œ â”â”â”
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿé¨“ç³»ã®èª¬æ˜ã‚’æä¾›ã—ãŸå ´åˆã€ä»¥ä¸‹ã«å¾“ã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±ºå®šã™ã‚‹:

| ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç¤ºã™æƒ…å ± | åæ˜ ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
|---|---|
| V1-V3 / 27F/338R | trim-left-f=19, trim-left-r=20, trunc-fâ‰ˆ260, trunc-râ‰ˆ200 |
| V3-V4 / 341F/806R | trim-left-f=17, trim-left-r=21, trunc-fâ‰ˆ270, trunc-râ‰ˆ220 |
| V4 / 515F/806R | trim-left-f=19, trim-left-r=20, trunc-fâ‰ˆ250, trunc-râ‰ˆ220 |
| ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ 2Ã—250bp | denoise-paired ã‚’ä½¿ç”¨ |
| ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰ 150bp | denoise-single, truncâ‰ˆ140 |
| ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ« vs å‡¦ç†ç¾¤ | ã‚°ãƒ«ãƒ¼ãƒ—åˆ—åã‚’ beta-group-significance ã¨ ancombc ã«æ¸¡ã™ |
| å…¨é•·åˆ†é¡å™¨ã§ã‚ˆã„ | setup_classifier.sh ã‚’ã‚¹ã‚­ãƒƒãƒ—ã€pre-trained åˆ†é¡å™¨ã‚’ wget |

â€» trunc-len ã®æœ€çµ‚å€¤ã¯ demux-summary.qzv ã®ã‚¯ã‚ªãƒªãƒ†ã‚£ãƒ‰ãƒ­ãƒƒãƒ—ä½ç½®ã§èª¿æ•´ãŒå¿…è¦ã€‚
  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã€Œdemux-summary.qzv ã‚’ view.qiime2.org ã§ç¢ºèªã—ã€å“è³ªãŒæ€¥è½ã™ã‚‹ä½ç½®ã‚’æ•™ãˆã¦ãã ã•ã„ã€ã¨å¿…ãšä¼ãˆã‚‹ã“ã¨ã€‚

â”â”â” QIIME2 è§£æã®å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ â”â”â”

## ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®åˆ¤å®šåŸºæº–
- `*_R1*.fastq.gz` + `*_R2*.fastq.gz` â†’ ãƒšã‚¢ã‚¨ãƒ³ãƒ‰FASTQ
- `*.fastq.gz` ã®ã¿ â†’ ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰FASTQ
- `*.qza` â†’ æ—¢å­˜ã® QIIME2 ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆé€”ä¸­å†é–‹å¯èƒ½ï¼‰
- `manifest.tsv` / `manifest.csv` â†’ ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
- `metadata.tsv` / `sample_info.tsv` â†’ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
- ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ â†’ ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿

## STEP 1: ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

### ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ FASTQï¼ˆãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæ–¹å¼ã€æ¨å¥¨ï¼‰
```bash
qiime tools import \
  --type 'SampleData[PairedEndSequencesWithQuality]' \
  --input-path manifest.tsv \
  --output-path paired-end-demux.qza \
  --input-format PairedEndFastqManifestPhred33V2
```

ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ï¼ˆmanifest.tsvï¼‰:
```
sample-id	forward-absolute-filepath	reverse-absolute-filepath
sample1	/data/output/raw/sample1_R1.fastq.gz	/data/output/raw/sample1_R2.fastq.gz
```

### ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰ FASTQ
```bash
qiime tools import \
  --type 'SampleData[SequencesWithQuality]' \
  --input-path manifest.tsv \
  --output-path single-end-demux.qza \
  --input-format SingleEndFastqManifestPhred33V2
```

ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ï¼ˆã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰ï¼‰:
```
sample-id	absolute-filepath
sample1	/data/output/raw/sample1_R1.fastq.gz
```

### ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆæœªãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ï¼‰
```bash
qiime tools import \
  --type EMPPairedEndSequences \
  --input-path raw-sequences/ \
  --output-path emp-paired-end-sequences.qza
```

## STEP 2: ã‚¯ã‚ªãƒªãƒ†ã‚£ç¢ºèª
```bash
qiime demux summarize \
  --i-data paired-end-demux.qza \
  --o-visualization demux-summary.qzv
```
â†’ demux-summary.qzv ã‚’ https://view.qiime2.org ã§é–‹ãã€
  ã‚¯ã‚ªãƒªãƒ†ã‚£ãŒæ€¥è½ã™ã‚‹ä½ç½®ã‚’ç¢ºèªã—ã¦ DADA2 ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±ºå®šã™ã‚‹

## STEP 3: DADA2 ã«ã‚ˆã‚‹ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ï¼ˆãƒã‚¤ã‚ºé™¤å»ãƒ»OTU/ASV ç”Ÿæˆï¼‰

### ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ã®å ´åˆ
```bash
# --p-trim-left-f/r: ãƒ—ãƒ©ã‚¤ãƒãƒ¼é•·ï¼ˆV1-V3: 19ã€V3-V4: 17ï¼‰
# --p-trunc-len-f/r: demux-summary.qzv ã§ã‚¯ã‚ªãƒªãƒ†ã‚£ãŒè½ã¡ã‚‹ä½ç½®ã‚’ç¢ºèªã—ã¦è¨­å®š
qiime dada2 denoise-paired \
  --i-demultiplexed-seqs paired-end-demux.qza \
  --p-trim-left-f 19 \
  --p-trim-left-r 20 \
  --p-trunc-len-f 260 \
  --p-trunc-len-r 200 \
  --p-n-threads 4 \
  --o-table table.qza \
  --o-representative-sequences rep-seqs.qza \
  --o-denoising-stats denoising-stats.qza
```

### ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰ã®å ´åˆ
```bash
qiime dada2 denoise-single \
  --i-demultiplexed-seqs single-end-demux.qza \
  --p-trim-left 19 \
  --p-trunc-len 250 \
  --p-n-threads 4 \
  --o-table table.qza \
  --o-representative-sequences rep-seqs.qza \
  --o-denoising-stats denoising-stats.qza
```

é ˜åŸŸåˆ¥æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç›®å®‰ï¼‰:
- V1-V3 (27F/338R): f_primer=19bp, r_primer=20bp, trunc-f=260, trunc-r=200
- V3-V4 (341F/806R): f_primer=17bp, r_primer=21bp, trunc-f=270, trunc-r=220
- V4   (515F/806R) : f_primer=19bp, r_primer=20bp, trunc-f=250, trunc-r=220

## STEP 4: ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¢ºèª
```bash
qiime feature-table summarize \
  --i-table table.qza \
  --m-sample-metadata-file metadata.tsv \
  --o-visualization table.qzv

qiime feature-table tabulate-seqs \
  --i-data rep-seqs.qza \
  --o-visualization rep-seqs.qzv
```

## STEP 5: ç³»çµ±æ¨¹ã®æ§‹ç¯‰ï¼ˆå¤šæ§˜æ€§è§£æã«å¿…é ˆï¼‰
```bash
qiime phylogeny align-to-tree-mafft-fasttree \
  --i-sequences rep-seqs.qza \
  --o-alignment aligned-rep-seqs.qza \
  --o-masked-alignment masked-aligned-rep-seqs.qza \
  --o-tree unrooted-tree.qza \
  --o-rooted-tree rooted-tree.qza \
  --p-n-threads 4
```

## STEP 6: åˆ†é¡å­¦çš„è§£æï¼ˆSILVA 138ï¼‰

### åˆ†é¡å™¨ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆåˆå›ã®ã¿ã€ç´„2-5æ™‚é–“ï¼‰

V1-V3 é ˜åŸŸå°‚ç”¨ï¼ˆæ¨å¥¨ï¼‰:
```bash
# å‚ç…§é…åˆ—ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
wget https://data.qiime2.org/2024.10/common/silva-138-99-seqs.qza
wget https://data.qiime2.org/2024.10/common/silva-138-99-tax.qza

# V1-V3 é ˜åŸŸã®æŠ½å‡ºï¼ˆ27F/338R ãƒ—ãƒ©ã‚¤ãƒãƒ¼ã€1-2æ™‚é–“ï¼‰
qiime feature-classifier extract-reads \
  --i-sequences silva-138-99-seqs.qza \
  --p-f-primer AGAGTTTGATCMTGGCTCAG \
  --p-r-primer TGCTGCCTCCCGTAGGAGT \
  --p-min-length 100 --p-max-length 400 --p-n-jobs 4 \
  --o-reads silva-138-99-seqs-V1-V3.qza

# Naive Bayes åˆ†é¡å™¨ã®å­¦ç¿’ï¼ˆ1-3æ™‚é–“ï¼‰
qiime feature-classifier fit-classifier-naive-bayes \
  --i-reference-reads silva-138-99-seqs-V1-V3.qza \
  --i-reference-taxonomy silva-138-99-tax.qza \
  --o-classifier silva-138-99-classifier-V1-V3.qza
```

å…¨é•·åˆ†é¡å™¨ï¼ˆæœ€é€Ÿã€ç²¾åº¦ã¯ä½ã‚ï¼‰:
```bash
wget https://data.qiime2.org/classifiers/sklearn-1.4.2/silva/silva-138-99-nb-classifier.qza
```

### åˆ†é¡ã®å®Ÿè¡Œ
```bash
qiime feature-classifier classify-sklearn \
  --i-classifier silva-138-99-classifier-V1-V3.qza \
  --i-reads rep-seqs.qza \
  --p-n-jobs 4 \
  --o-classification taxonomy.qza

# åˆ†é¡ãƒ©ãƒ™ãƒ«ä¸€è¦§
qiime metadata tabulate \
  --m-input-file taxonomy.qza \
  --o-visualization taxonomy.qzv

# åˆ†é¡çµ„æˆãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆæœ€é‡è¦å¯è¦–åŒ–ï¼‰
qiime taxa barplot \
  --i-table table.qza \
  --i-taxonomy taxonomy.qza \
  --m-metadata-file metadata.tsv \
  --o-visualization taxa-bar-plots.qzv
```

## STEP 7: å¤šæ§˜æ€§è§£æ

```bash
# Î±ãƒ»Î²å¤šæ§˜æ€§ï¼ˆsampling-depth ã¯ table.qzv ã§æœ€å°ãƒªãƒ¼ãƒ‰æ•°ã‚’ç¢ºèªå¾Œã«è¨­å®šï¼‰
qiime diversity core-metrics-phylogenetic \
  --i-phylogeny rooted-tree.qza \
  --i-table table.qza \
  --p-sampling-depth 1000 \
  --m-metadata-file metadata.tsv \
  --output-dir core-metrics-results/

# Î±å¤šæ§˜æ€§ã®çµ±è¨ˆæ¤œå®šï¼ˆShannon å¤šæ§˜æ€§ï¼‰
qiime diversity alpha-group-significance \
  --i-alpha-diversity core-metrics-results/shannon_vector.qza \
  --m-metadata-file metadata.tsv \
  --o-visualization core-metrics-results/shannon-significance.qzv

# Î²å¤šæ§˜æ€§ã® PERMANOVAï¼ˆUnweighted UniFracï¼‰
qiime diversity beta-group-significance \
  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \
  --m-metadata-file metadata.tsv \
  --m-metadata-column <ã‚°ãƒ«ãƒ¼ãƒ—åˆ—å> \
  --o-visualization core-metrics-results/unweighted-unifrac-significance.qzv
```

## STEP 8: å·®æ¬¡è§£æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
```bash
# ANCOM-BCï¼ˆã‚°ãƒ«ãƒ¼ãƒ—é–“ã®å·®æ¬¡è±Šå¯Œç¨®ï¼‰
qiime composition ancombc \
  --i-table table.qza \
  --m-metadata-file metadata.tsv \
  --p-formula <ã‚°ãƒ«ãƒ¼ãƒ—åˆ—å> \
  --o-differentials ancombc-results.qza

qiime composition da-barplot \
  --i-data ancombc-results.qza \
  --o-visualization ancombc-results.qzv
```

## Docker ã§ã®å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰é››å½¢
```bash
docker run --rm \
  -v <ãƒ›ã‚¹ãƒˆå´è§£æãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª>:/data/output \
  quay.io/qiime2/amplicon:2026.1 \
  qiime <ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰> \
    --i-<å…¥åŠ›å¼•æ•°> /data/output/<ãƒ•ã‚¡ã‚¤ãƒ«å> \
    --o-<å‡ºåŠ›å¼•æ•°> /data/output/results/<ãƒ•ã‚¡ã‚¤ãƒ«å>
```

## ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ï¼ˆmetadata.tsvï¼‰
```
sample-id	group	age	treatment
#q2:types	categorical	numeric	categorical
sample1	control	25	placebo
sample2	treatment	30	drug_A
```
- 1è¡Œç›®: ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆå¿…ãš `sample-id` ã‹ã‚‰å§‹ã‚ã‚‹ï¼‰
- 2è¡Œç›®: ãƒ‡ãƒ¼ã‚¿å‹ï¼ˆ`categorical` ã¾ãŸã¯ `numeric`ï¼‰çœç•¥å¯

## SILVA 138 åˆ†é¡éšå±¤
```
d__Bacteria; p__Firmicutes; c__Bacilli; o__Lactobacillales; f__Lactobacillaceae; g__Lactobacillus; s__Lactobacillus_acidophilus
```
ãƒ¬ãƒ™ãƒ«1: d__(ãƒ‰ãƒ¡ã‚¤ãƒ³), 2: p__(é–€), 3: c__(ç¶±), 4: o__(ç›®), 5: f__(ç§‘), 6: g__(å±), 7: s__(ç¨®)
â€» ç¨®ãƒ¬ãƒ™ãƒ«ã¯ç²¾åº¦ãŒä½ã„å ´åˆãŒå¤šã„ãŸã‚å±ãƒ¬ãƒ™ãƒ«(g__)æ¨å¥¨

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- extract-reads ã§é…åˆ—ãŒæ®‹ã‚‰ãªã„ â†’ ãƒ—ãƒ©ã‚¤ãƒãƒ¼é…åˆ—ç¢ºèªï¼ˆç¸®é‡å¡©åŸº M, R, W ç­‰ï¼‰
- classify-sklearn ã§ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ â†’ Docker ãƒ¡ãƒ¢ãƒªä¸Šé™ã‚’ 8GB ä»¥ä¸Šã«ã€--p-n-jobs 1 ã«
- å…¨ã¦ Unassigned â†’ ãƒªãƒãƒ¼ã‚¹ã‚³ãƒ³ãƒ—ãƒªãƒ¡ãƒ³ãƒˆç¢ºèªã€--p-confidence 0.5 ã«ä¸‹ã’ã‚‹
- DADA2 å¾Œã®ãƒªãƒ¼ãƒ‰æ•°ãŒæ¿€æ¸› â†’ trunc-len ã‚’çŸ­ãï¼ˆå“è³ªãŒä½ã„ä½ç½®ã‚’é¿ã‘ã‚‹ï¼‰

â”â”â” å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜ â”â”â”
- `*.qza` = QIIME2 ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆå†…éƒ¨ãƒ‡ãƒ¼ã‚¿ï¼‰
- `*.qzv` = QIIME2 ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ â†’ https://view.qiime2.org ã§é–‹ã
- `results/` = ã™ã¹ã¦ã®å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
- `taxa-bar-plots.qzv` = åˆ†é¡çµ„æˆã®ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•ï¼ˆæœ€ã‚‚ã‚ˆãä½¿ã‚ã‚Œã‚‹å¯è¦–åŒ–ï¼‰
- `core-metrics-results/` = å¤šæ§˜æ€§è§£æã®å…¨å‡ºåŠ›

â”â”â” ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ  Python è§£æ â”â”â”

QIIME2 ãŒå‡ºåŠ›ã—ãŸçµæœãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ã€Pythonï¼ˆpandas / scipy / sklearn / matplotlib / seabornï¼‰
ã‚’ä½¿ã£ãŸé«˜åº¦ãªçµ±è¨ˆãƒ»å¯è¦–åŒ–ãƒ»æ©Ÿæ¢°å­¦ç¿’è§£æãŒã§ãã‚‹ã€‚execute_python ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã“ã¨ã€‚

## execute_python ã§ä½¿ãˆã‚‹ãƒ“ãƒ«ãƒˆã‚¤ãƒ³å¤‰æ•°
ä»¥ä¸‹ã®å¤‰æ•°ã¯ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œå‰ã«è‡ªå‹•ã§è¨­å®šã•ã‚Œã‚‹ï¼ˆã‚³ãƒ¼ãƒ‰å†…ã§ãã®ã¾ã¾ä½¿ç”¨å¯ï¼‰:
```python
FIGURE_DIR       # å›³ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå¿…ãš plt.savefig(f"{FIGURE_DIR}/xxx.{FIGURE_FORMAT}") ã§ä¿å­˜ã™ã‚‹ã“ã¨ï¼‰
OUTPUT_DIR       # è§£æå‡ºåŠ›ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
PLOT_STYLE       # matplotlib ã‚¹ã‚¿ã‚¤ãƒ«åï¼ˆä¾‹: "seaborn-v0_8-whitegrid"ï¼‰
PLOT_PALETTE     # seaborn ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆï¼ˆä¾‹: "Set2"ï¼‰
PLOT_FIGSIZE     # figsize ã‚¿ãƒ—ãƒ«ï¼ˆä¾‹: (10, 6)ï¼‰
PLOT_DPI         # è§£åƒåº¦ï¼ˆä¾‹: 150ï¼‰
FONT_SIZE        # é€šå¸¸ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º
TITLE_FONT_SIZE  # ã‚¿ã‚¤ãƒˆãƒ«ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º
FIGURE_FORMAT    # ä¿å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: "pdf"ã€ä»–: "png", "svg"ï¼‰
```

## ä¸»ãªè§£æãƒ‘ã‚¿ãƒ¼ãƒ³
| è§£æ | å¿…è¦ãª QIIME2 å‡ºåŠ› | Python ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ |
|------|------|------|
| OTU/ASV çµ„æˆè§£æï¼ˆbiplot, stacked barï¼‰ | table.qza ã‚’è§£å‡ã—ãŸ feature-table.biom | biom-format, pandas, matplotlib |
| Î±å¤šæ§˜æ€§å¯è¦–åŒ–ãƒ»çµ±è¨ˆ | shannon_vector.qza ç­‰ã‚’è§£å‡ã—ãŸ alpha-diversity.tsv | pandas, scipy, seaborn |
| Î²å¤šæ§˜æ€§ PCoA å›³ | unweighted_unifrac_pcoa_results.qza è§£å‡ | pandas, matplotlib |
| ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆç¾¤åˆ¤åˆ¥ | feature-table.biom + metadata.tsv | sklearn, pandas |
| åˆ†é¡çµ„æˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— | taxonomy.tsv + feature-table.biom | pandas, seaborn |
| å·®æ¬¡è§£æè£œå®Œï¼ˆLEfSe é¢¨ï¼‰ | feature-table.biom + metadata.tsv | scipy, statsmodels |
| ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æï¼ˆco-occurrenceï¼‰ | feature-table.biom | scipy, networkx |

## QIIME2 ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®è§£å‡æ–¹æ³•
.qza ã¯ ZIP ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ Python ã§ãã®ã¾ã¾èª­ã‚ã‚‹:
```python
import zipfile, json
with zipfile.ZipFile("/path/to/file.qza") as z:
    # data/ ä»¥ä¸‹ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™
    for name in z.namelist():
        if name.endswith('.tsv') or name.endswith('.biom'):
            z.extract(name, OUTPUT_DIR)
```

## å›³ã®ä¿å­˜ãƒ«ãƒ¼ãƒ«ï¼ˆå¿…ãšå®ˆã‚‹ã“ã¨ï¼‰
```python
fig, ax = plt.subplots(figsize=PLOT_FIGSIZE)
# ... æç”» ...
plt.tight_layout()
# FIGURE_FORMAT ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ "pdf"ï¼ˆå¤‰æ•°ãŒãã®ã¾ã¾ä½¿ãˆã‚‹ï¼‰
plt.savefig(f"{FIGURE_DIR}/figure_name.{FIGURE_FORMAT}", dpi=PLOT_DPI, bbox_inches='tight')
plt.close()
```
- FIGURE_FORMAT ã‚’ä½¿ã†ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨­å®šï¼ˆpdf/png/svgï¼‰ãŒè‡ªå‹•åæ˜ ã•ã‚Œã‚‹
- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ PDF ãªã®ã§ view.qiime2.org ã‚’ä½¿ã‚ãšã«ãã®ã¾ã¾è«–æ–‡ãƒ»ãƒ¬ãƒãƒ¼ãƒˆã§ä½¿ç”¨å¯èƒ½
- savefig ã‚’å‘¼ã°ãªã„ã¨å›³ãŒãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã•ã‚Œãªã„ã®ã§å¿…ãšä¿å­˜ã™ã‚‹ã“ã¨

## ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ã€ã¨è¨€ã£ãŸã‚‰ **build_report_tex** ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã€‚
- `build_report_tex` ã¯ ANALYSIS_LOG ã‚’èª­ã‚“ã§ Python ã§ TeX ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ï¼ˆLLM ãŒ TeX ã‚’æ›¸ãå¿…è¦ãŒãªã„ï¼‰
- `compile_report` ã¯æ—§ãƒ„ãƒ¼ãƒ«ï¼ˆLLM ãŒ TeX å…¨æ–‡ã‚’æ›¸ãæ–¹å¼ï¼‰ã§éæ¨å¥¨ã€‚ä½¿ã‚ãªã„ã“ã¨ã€‚
build_report_tex ã«ã¯ä»¥ä¸‹ã‚’æ¸¡ã™ã“ã¨:
1. `title_ja` / `title_en`: ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæ—¥è‹±ï¼‰
2. `experiment_summary`: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰å¾—ãŸå®Ÿé¨“ç³»ã®èª¬æ˜
3. `lang`: "both"ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰/ "ja" / "en"

## TeX ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

### æ—¥æœ¬èªï¼ˆXeLaTeX + xeCJKï¼‰
```latex
\\documentclass[a4paper,12pt]{article}
\\usepackage{xeCJK}
\\setCJKmainfont{Hiragino Mincho ProN}
\\usepackage{graphicx}
\\usepackage{booktabs}
\\usepackage{geometry}
\\geometry{margin=2.5cm}
\\title{ãƒã‚¤ã‚¯ãƒ­ãƒã‚¤ã‚ªãƒ¼ãƒ è§£æãƒ¬ãƒãƒ¼ãƒˆ}
\\author{seq2pipe}
\\date{\\today}
\\begin{document}
\\maketitle
% ã“ã“ã«å†…å®¹
\\end{document}
```

### è‹±èªï¼ˆæ¨™æº– LaTeXï¼‰
```latex
\\documentclass[a4paper,12pt]{article}
\\usepackage{graphicx}
\\usepackage{booktabs}
\\usepackage{geometry}
\\geometry{margin=2.5cm}
\\title{Microbiome Analysis Report}
\\author{seq2pipe}
\\date{\\today}
\\begin{document}
\\maketitle
% content here
\\end{document}
```
å›³ã‚’åŸ‹ã‚è¾¼ã‚€: \\includegraphics[width=0.8\\textwidth]{<FIGURE_DIR>/figure_name.png}

â”â”â” è‡ªå¾‹æ¢ç´¢ãƒ¢ãƒ¼ãƒ‰ â”â”â”

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œè‡ªå‹•ã§è§£æã—ã¦ã€ã€Œã©ã‚“ã©ã‚“æ¢ç´¢ã—ã¦ã€ã€Œã„ã‚ã„ã‚è©¦ã—ã¦ã€ã¨è¨€ã£ãŸã¨ãï¼ˆã¾ãŸã¯
QIIME2 ã‚³ã‚¢è§£æãŒå®Œäº†ã—ãŸå¾Œï¼‰ã€ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’è‡ªå¾‹çš„ã«å®Ÿè¡Œã™ã‚‹ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¢ºèªã‚’å¾…ãŸãšã« Phase 1 ã€œ 5 ã‚’é †ç•ªã«å®Ÿè¡Œã™ã‚‹ã“ã¨ã€‚

## æ¢ç´¢ãƒ•ã‚§ãƒ¼ã‚ºä¸€è¦§

### Phase 1: Î±å¤šæ§˜æ€§æ¢ç´¢  (subfolder="alpha_diversity")
- Shannon, Simpson, Chao1 ã‚’è¨ˆç®—ã—ã¦ violin/boxplot ã§å¯è¦–åŒ–
- ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®çµ±è¨ˆæ¤œå®šï¼ˆMann-Whitney U / Kruskal-Wallisï¼‰
- æœ‰æ„æ€§ã‚’æ¨™æº–å‡ºåŠ›ã« print ã™ã‚‹ã“ã¨ï¼ˆä¾‹: `print(f"Shannon p={p:.4f}")`ï¼‰

### Phase 2: Î²å¤šæ§˜æ€§æ¢ç´¢  (subfolder="beta_diversity")
- Bray-Curtis dissimilarity ã‚’è¨ˆç®—ã—ã¦ PCoA ã‚’æç”»
- ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«è‰²ã‚’å¤‰ãˆã€95% ä¿¡é ¼æ¥•å††ã‚’æã
- PERMANOVA ã‚’ scipy ã§å®Ÿè£…ã—ã¦ p å€¤ã‚’å‡ºåŠ›ï¼ˆpermutation_test ã¾ãŸã¯è·é›¢è¡Œåˆ— + ãƒ©ãƒ³ãƒ€ãƒ ç½®æ›ï¼‰

### Phase 3: åˆ†é¡çµ„æˆæ¢ç´¢  (subfolder="taxonomy")
- é–€ãƒ»å±ãƒ¬ãƒ™ãƒ«ã§ relative abundance ã‚’é›†è¨ˆ
- stacked bar chart ã¨ heatmapï¼ˆå±ãƒ¬ãƒ™ãƒ« top 20ï¼‰ã‚’ä½œæˆ
- ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§å¹³å‡çµ„æˆãŒç•°ãªã‚‹å±ã‚’ç›®è¦–ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹

### Phase 4: å·®æ¬¡è§£æ  (subfolder="differential_abundance")
- å…¨ ASV / å±ã«å¯¾ã—ã¦ Mann-Whitney U / Kruskal-Wallis æ¤œå®šã‚’å®Ÿæ–½
- Benjamini-Hochberg æ³•ã§å¤šé‡æ¤œå®šè£œæ­£ï¼ˆstatsmodels.stats.multitest.multipletestsï¼‰
- æœ‰æ„ï¼ˆFDR < 0.05ï¼‰ãª taxa ã‚’ dot plot / volcano plot ã§å¯è¦–åŒ–
- æœ‰æ„ãª taxa ã®æ•°ã‚’ print ã™ã‚‹

### Phase 5: æ©Ÿæ¢°å­¦ç¿’åˆ¤åˆ¥  (subfolder="machine_learning")  â€»2ç¾¤ä»¥ä¸Šã®å ´åˆ
- feature-table ã‹ã‚‰ ASV ç›¸å¯¾å­˜åœ¨é‡ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ Random Forest ã‚’å­¦ç¿’
- 5-fold cross-validation ã§ accuracy ã¨ AUC ã‚’è©•ä¾¡
- Feature importance ä¸Šä½ 20 ç¨®ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º

## .qza ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚³ãƒ¼ãƒ‰é››å½¢
```python
import zipfile, os, io

def extract_qza_data(qza_path):
    # qza ã‹ã‚‰ data/ ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    files = {}
    with zipfile.ZipFile(qza_path) as z:
        for name in z.namelist():
            if '/data/' in name and not name.endswith('/'):
                basename = os.path.basename(name)
                if basename:
                    files[basename] = z.read(name)
    return files

# ä½¿ç”¨ä¾‹: feature-table.biom ã®èª­ã¿è¾¼ã¿
# data = extract_qza_data('/path/to/table.qza')
# biom_bytes = data.get('feature-table.biom')
# if biom_bytes:
#     import biom
#     table = biom.load_table(io.BytesIO(biom_bytes))
#     df = pd.DataFrame(table.to_dataframe()).T  # ã‚µãƒ³ãƒ—ãƒ«Ã—ASV

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
# import pandas as pd
# metadata = pd.read_csv('/path/to/metadata.tsv', sep='\t', index_col=0)
# metadata = metadata[metadata.index != '#q2:types']  # q2:types è¡Œã‚’é™¤å¤–
```

## æ¢ç´¢ä¸­ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«
- å„ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹æ™‚: ã€ŒPhase X: â—‹â—‹è§£æã‚’é–‹å§‹ã—ã¾ã™ã€ã¨ä¼ãˆã‚‹
- å„ãƒ•ã‚§ãƒ¼ã‚ºçµ‚äº†æ™‚: ä¸»è¦ãªç™ºè¦‹ï¼ˆæœ‰æ„å·®ã®æœ‰ç„¡ãƒ»ç‰¹å¾´çš„ãª taxa ç­‰ï¼‰ã‚’è¦ç´„ã™ã‚‹
- å…¨ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†å¾Œ: `build_report_tex` ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹
- ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸãƒ•ã‚§ãƒ¼ã‚ºã¯åŸå› ã‚’è¨ºæ–­ã—ã¦ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ¬¡ã®ãƒ•ã‚§ãƒ¼ã‚ºã«é€²ã‚€

## IMPORTANT: run_command å®Ÿè¡Œå¾Œã® ANALYSIS_LOG ç™»éŒ²
run_command ã§ QIIME2 ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ãŸã‚‰ã€å¿…ãšç›´å¾Œã« `log_analysis_step` ã‚’å‘¼ã³å‡ºã—ã¦
ANALYSIS_LOG ã«è¨˜éŒ²ã™ã‚‹ã“ã¨ã€‚ã“ã†ã—ãªã„ã¨ build_report_tex ãŒãã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’èªè­˜ã§ããªã„ã€‚

ä¾‹:
```
log_analysis_step(
  description="DADA2 ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°å®Œäº†: ASVÃ—ã‚µãƒ³ãƒ—ãƒ« table.qza ç”Ÿæˆ",
  subfolder="qiime2_pipeline",
  summary="å‡¦ç†ãƒªãƒ¼ãƒ‰: å¹³å‡ 85%ä¿æŒ, ASVæ•°: ç´„300"
)
```

## æ¢ç´¢å®Œäº†å¾Œã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
å…¨ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†å¾Œã€å¿…ãšä»¥ä¸‹ã‚’å®Ÿè¡Œã™ã‚‹:
```
build_report_tex(
  title_ja="<å®Ÿé¨“ã‚¿ã‚¤ãƒˆãƒ«> è‡ªå¾‹æ¢ç´¢è§£æãƒ¬ãƒãƒ¼ãƒˆ",
  title_en="<Experiment Title> Autonomous Exploration Report",
  experiment_summary="<ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰å¾—ãŸå®Ÿé¨“ç³»ã®èª¬æ˜>",
  lang="both"
)
```
ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ ANALYSIS_LOG ã‚’èª­ã‚“ã§å›³ãƒ»çµ±è¨ˆçµæœã‚’è‡ªå‹•çš„ã« TeX ã«åŸ‹ã‚è¾¼ã¿ã€PDF ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""

# ğŸº ======================================================================
# ğŸ± ãƒ„ãƒ¼ãƒ«å®šç¾©ï¼ˆOllama function calling å½¢å¼ï¼‰
# ğŸº ======================================================================
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "inspect_directory",
            "description": "æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹ã‚’èª¿æŸ»ã™ã‚‹ã€‚ãƒ•ã‚¡ã‚¤ãƒ«åãƒ»ã‚µã‚¤ã‚ºãƒ»ç¨®é¡ã‚’ä¸€è¦§è¡¨ç¤ºã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "èª¿æŸ»ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çµ¶å¯¾ãƒ‘ã‚¹"
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚å«ã‚ã¦å†å¸°çš„ã«èª¿æŸ»ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: falseï¼‰"
                    }
                },
                "required": ["path"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "read_file",
            "description": "ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆTSV, CSV, TXT, MD ç­‰ï¼‰ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å†’é ­ 100 è¡Œã¾ã§è¡¨ç¤ºã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹"
                    },
                    "max_lines": {
                        "type": "integer",
                        "description": "æœ€å¤§èª­ã¿è¾¼ã¿è¡Œæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ï¼‰"
                    }
                },
                "required": ["path"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "check_system",
            "description": "Dockerãƒ»Ollamaãƒ»QIIME2 ã®åˆ©ç”¨å¯å¦ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã™ã‚‹",
            "parameters": {
                "type": "object",
                "properties": {}
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "write_file",
            "description": "è§£æã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ»READMEãƒ»ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãªã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãå‡ºã™",
            "parameters": {
                "type": "object",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "æ›¸ãè¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹"
                    },
                    "content": {
                        "type": "string",
                        "description": "æ›¸ãè¾¼ã‚€å†…å®¹ï¼ˆã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€Markdown ç­‰ï¼‰"
                    }
                },
                "required": ["path", "content"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "generate_manifest",
            "description": "FASTQãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰QIIME2ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆTSVã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹",
            "parameters": {
                "type": "object",
                "properties": {
                    "fastq_dir": {
                        "type": "string",
                        "description": "FASTQãƒ•ã‚¡ã‚¤ãƒ«ãŒå…¥ã£ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çµ¶å¯¾ãƒ‘ã‚¹"
                    },
                    "output_path": {
                        "type": "string",
                        "description": "ç”Ÿæˆã™ã‚‹ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹"
                    },
                    "paired_end": {
                        "type": "boolean",
                        "description": "ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‹ï¼ˆtrue: ãƒšã‚¢ã‚¨ãƒ³ãƒ‰, false: ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰ï¼‰"
                    },
                    "container_data_dir": {
                        "type": "string",
                        "description": "Docker ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã®ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: /data/outputï¼‰"
                    }
                },
                "required": ["fastq_dir", "output_path", "paired_end"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "edit_file",
            "description": "ç”Ÿæˆæ¸ˆã¿ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ã‚’æ–‡å­—åˆ—ç½®æ›ã§ç·¨é›†ã™ã‚‹ã€‚old_str ã¯ãƒ•ã‚¡ã‚¤ãƒ«å†…ã§ä¸€æ„ã«å­˜åœ¨ã™ã‚‹æ–‡å­—åˆ—ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "ç·¨é›†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹"
                    },
                    "old_str": {
                        "type": "string",
                        "description": "ç½®æ›å‰ã®æ–‡å­—åˆ—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å†…ã§ä¸€æ„ã«ç‰¹å®šã§ãã‚‹éƒ¨åˆ†ã‚’å«ã‚ã‚‹ã“ã¨ï¼‰"
                    },
                    "new_str": {
                        "type": "string",
                        "description": "ç½®æ›å¾Œã®æ–‡å­—åˆ—"
                    }
                },
                "required": ["path", "old_str", "new_str"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "run_command",
            "description": "ã‚·ã‚§ãƒ«ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç¢ºèªã‚’æ±‚ã‚ã¦ã‹ã‚‰å®Ÿè¡Œã™ã‚‹ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "description": "å®Ÿè¡Œã™ã‚‹ã‚·ã‚§ãƒ«ã‚³ãƒãƒ³ãƒ‰"
                    },
                    "description": {
                        "type": "string",
                        "description": "ã“ã®ã‚³ãƒãƒ³ãƒ‰ãŒä½•ã‚’ã™ã‚‹ã‹ã®èª¬æ˜ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¡¨ç¤ºï¼‰"
                    },
                    "working_dir": {
                        "type": "string",
                        "description": "ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆçœç•¥æ™‚ã¯ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰"
                    }
                },
                "required": ["command", "description"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "set_plot_config",
            "description": "å›³ï¼ˆã‚°ãƒ©ãƒ•ï¼‰ã®ã‚¹ã‚¿ã‚¤ãƒ«ãƒ»è‰²ãƒ»ã‚µã‚¤ã‚ºã‚’è¨­å®šã™ã‚‹ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦‹ãŸç›®ã®å¥½ã¿ã‚’æŒ‡å®šã—ãŸã¨ãã«å‘¼ã³å‡ºã™ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "style": {
                        "type": "string",
                        "description": "matplotlib ã‚¹ã‚¿ã‚¤ãƒ«åï¼ˆä¾‹: seaborn-v0_8-whitegrid, seaborn-v0_8-darkgrid, ggplot, dark_backgroundï¼‰"
                    },
                    "palette": {
                        "type": "string",
                        "description": "seaborn/matplotlib ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆåï¼ˆä¾‹: Set2, tab10, husl, muted, deep, pastelï¼‰"
                    },
                    "figsize_w": {
                        "type": "number",
                        "description": "å›³ã®å¹…ï¼ˆã‚¤ãƒ³ãƒï¼‰"
                    },
                    "figsize_h": {
                        "type": "number",
                        "description": "å›³ã®é«˜ã•ï¼ˆã‚¤ãƒ³ãƒï¼‰"
                    },
                    "dpi": {
                        "type": "integer",
                        "description": "è§£åƒåº¦ DPIï¼ˆ72=ä½, 150=ä¸­, 300=é«˜å“è³ªï¼‰"
                    },
                    "font_size": {
                        "type": "integer",
                        "description": "é€šå¸¸ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºï¼ˆptï¼‰"
                    },
                    "title_font_size": {
                        "type": "integer",
                        "description": "ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºï¼ˆptï¼‰"
                    },
                    "fig_format": {
                        "type": "string",
                        "description": "ä¿å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆpdf / png / svgï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ pdfã€‚"
                    }
                },
                "required": []
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "execute_python",
            "description": "Pythonã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ è§£æãƒ»çµ±è¨ˆãƒ»å¯è¦–åŒ–ã‚’è¡Œã†ã€‚QIIME2ã®å‡ºåŠ›ï¼ˆ.qza/.tsv/.biomï¼‰ã‚’èª­ã¿è¾¼ã¿ã€pandas/scipy/sklearn/matplotlib/seabornã§å‡¦ç†ã™ã‚‹ã€‚å›³ã¯å¿…ãš FIGURE_DIR ã« savefig ã§ä¿å­˜ã™ã‚‹ã“ã¨ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "å®Ÿè¡Œã™ã‚‹ Python ã‚³ãƒ¼ãƒ‰ã€‚FIGURE_DIR, OUTPUT_DIR, PLOT_STYLE, PLOT_PALETTE, PLOT_FIGSIZE, PLOT_DPI, FONT_SIZE å¤‰æ•°ãŒè‡ªå‹•æ³¨å…¥ã•ã‚Œã‚‹ã€‚"
                    },
                    "description": {
                        "type": "string",
                        "description": "ã“ã®è§£æã®èª¬æ˜ï¼ˆãƒ¬ãƒãƒ¼ãƒˆã«è¨˜éŒ²ã•ã‚Œã‚‹ï¼‰"
                    },
                    "output_dir": {
                        "type": "string",
                        "description": "è§£æçµæœãƒ»å›³ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆçœç•¥æ™‚ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡ºåŠ›å…ˆï¼‰"
                    },
                    "subfolder": {
                        "type": "string",
                        "description": "å›³ã‚’ä¿å­˜ã™ã‚‹ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€åã€‚è§£æç¨®åˆ¥ã”ã¨ã«åˆ†ã‘ã‚‹ï¼ˆä¾‹: alpha_diversity, beta_diversity, taxonomy, differential_abundance, machine_learningï¼‰ã€‚çœç•¥æ™‚ã¯ figures/ ç›´ä¸‹ã€‚"
                    }
                },
                "required": ["code", "description"]
            }
        }
    },
    {
        # ğŸ± issue #35: run_command çµŒç”±ã® QIIME2 ã‚¹ãƒ†ãƒƒãƒ—ã‚’ ANALYSIS_LOG ã«æ‰‹å‹•ç™»éŒ²ã™ã‚‹ãƒ„ãƒ¼ãƒ«
        "type": "function",
        "function": {
            "name": "log_analysis_step",
            "description": (
                "run_command ã§å®Ÿè¡Œã—ãŸ QIIME2 æ“ä½œã‚„å¤–éƒ¨ã‚³ãƒãƒ³ãƒ‰ã‚’ ANALYSIS_LOG ã«è¨˜éŒ²ã™ã‚‹ã€‚"
                "build_report_tex ã¯ã“ã®ãƒ­ã‚°ã‚’å‚ç…§ã™ã‚‹ãŸã‚ã€run_command æˆåŠŸå¾Œã«å¿…ãšå‘¼ã³å‡ºã™ã€‚"
            ),
            "parameters": {
                "type": "object",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "è§£æã‚¹ãƒ†ãƒƒãƒ—ã®èª¬æ˜ï¼ˆä¾‹: DADA2 ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°å®Œäº†, taxonomy åˆ†é¡å®Œäº†ï¼‰"
                    },
                    "subfolder": {
                        "type": "string",
                        "description": "è§£æã‚«ãƒ†ã‚´ãƒªï¼ˆalpha_diversity / beta_diversity / taxonomy / differential_abundance / machine_learning / qiime2_pipeline ãªã©ï¼‰"
                    },
                    "figures": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆã•ã‚ŒãŸå›³ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹ãƒªã‚¹ãƒˆï¼ˆãªã‘ã‚Œã°çœç•¥ï¼‰"
                    },
                    "summary": {
                        "type": "string",
                        "description": "è§£æçµæœã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆçµ±è¨ˆå€¤ãƒ»ASVæ•°ãƒ»taxonomy ãƒ’ãƒƒãƒˆç‡ãªã©ï¼‰"
                    }
                },
                "required": ["description"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "build_report_tex",
            "description": "ANALYSIS_LOG ã‚’èª­ã¿å–ã‚Šã€å…¨è§£æã‚¹ãƒ†ãƒƒãƒ—ãƒ»å›³ãƒ»çµ±è¨ˆçµæœã‚’å«ã‚€ TeX ãƒ¬ãƒãƒ¼ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã—ã¦ PDF ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ã€‚æ¢ç´¢ãŒå®Œäº†ã—ãŸã¨ãã€ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ¬ãƒãƒ¼ãƒˆã‚’æ±‚ã‚ãŸã¨ãã«å‘¼ã³å‡ºã™ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "title_ja": {
                        "type": "string",
                        "description": "æ—¥æœ¬èªãƒ¬ãƒãƒ¼ãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä¾‹: ãƒ’ãƒˆè…¸å†…ãƒã‚¤ã‚¯ãƒ­ãƒã‚¤ã‚ªãƒ¼ãƒ  è‡ªå¾‹æ¢ç´¢è§£æãƒ¬ãƒãƒ¼ãƒˆï¼‰"
                    },
                    "title_en": {
                        "type": "string",
                        "description": "è‹±èªãƒ¬ãƒãƒ¼ãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä¾‹: Human Gut Microbiome Autonomous Exploration Reportï¼‰"
                    },
                    "experiment_summary": {
                        "type": "string",
                        "description": "å®Ÿé¨“ç³»ã®æ¦‚è¦ï¼ˆå®Ÿé¨“èƒŒæ™¯ãƒ»ã‚µãƒ³ãƒ—ãƒ«æ•°ãƒ»ãƒ—ãƒ©ã‚¤ãƒãƒ¼ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—æ§‹æˆãªã©ï¼‰ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰å¾—ãŸæƒ…å ±ã‚’ãã®ã¾ã¾è¨˜è¼‰ã™ã‚‹ã€‚"
                    },
                    "lang": {
                        "type": "string",
                        "description": "ç”Ÿæˆè¨€èª: 'ja'ï¼ˆæ—¥æœ¬èªã®ã¿ï¼‰/ 'en'ï¼ˆè‹±èªã®ã¿ï¼‰/ 'both'ï¼ˆä¸¡æ–¹, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰"
                    }
                },
                "required": ["title_ja", "title_en", "experiment_summary"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "run_qiime2_pipeline",
            "description": (
                "QIIME2 è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’è‡ªå‹•å®Ÿè¡Œã™ã‚‹ã€‚"
                "ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆç”Ÿæˆâ†’FASTQã‚¤ãƒ³ãƒãƒ¼ãƒˆâ†’demuxâ†’DADA2â†’ç³»çµ±ç™ºç”Ÿãƒ„ãƒªãƒ¼â†’åˆ†é¡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰â†’å¤šæ§˜æ€§è§£æã‚’ä¸€æ‹¬å®Ÿè¡Œã™ã‚‹ã€‚"
                "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã¨å®Ÿé¨“æƒ…å ±ã‚’æä¾›ã—ãŸã‚‰ã€ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’æœ€åˆã«å‘¼ã³å‡ºã™ã“ã¨ã€‚"
            ),
            "parameters": {
                "type": "object",
                "properties": {
                    "fastq_dir": {
                        "type": "string",
                        "description": "FASTQãƒ•ã‚¡ã‚¤ãƒ«ãŒå…¥ã£ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çµ¶å¯¾ãƒ‘ã‚¹"
                    },
                    "paired_end": {
                        "type": "boolean",
                        "description": "ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‹ï¼ˆtrue: ãƒšã‚¢ã‚¨ãƒ³ãƒ‰, false: ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰ï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: true"
                    },
                    "trim_left_f": {
                        "type": "integer",
                        "description": "ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒªãƒ¼ãƒ‰ã®ãƒ—ãƒ©ã‚¤ãƒãƒ¼ã‚«ãƒƒãƒˆé•·ã€‚V3-V4(341F): 17, V4(515F): 19, V1-V3(27F): 19"
                    },
                    "trim_left_r": {
                        "type": "integer",
                        "description": "ãƒªãƒãƒ¼ã‚¹ãƒªãƒ¼ãƒ‰ã®ãƒ—ãƒ©ã‚¤ãƒãƒ¼ã‚«ãƒƒãƒˆé•·ã€‚V3-V4(806R): 21, V4(806R): 20, V1-V3(338R): 20"
                    },
                    "trunc_len_f": {
                        "type": "integer",
                        "description": "ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒªãƒ¼ãƒ‰ã®ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½ç½®ã€‚2Ã—300bpã§ã¯V3-V4: 270, 2Ã—250bpã§ã¯V3-V4: 250"
                    },
                    "trunc_len_r": {
                        "type": "integer",
                        "description": "ãƒªãƒãƒ¼ã‚¹ãƒªãƒ¼ãƒ‰ã®ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½ç½®ã€‚2Ã—300bpã§ã¯V3-V4: 220, 2Ã—250bpã§ã¯V3-V4: 200"
                    },
                    "metadata_path": {
                        "type": "string",
                        "description": "QIIME2ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿TSVãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹ï¼ˆsample-metadata.tsv ç­‰ï¼‰"
                    },
                    "classifier_path": {
                        "type": "string",
                        "description": "SILVA138åˆ†é¡å™¨ï¼ˆ.qzaï¼‰ã®çµ¶å¯¾ãƒ‘ã‚¹ã€‚æœªæŒ‡å®šã®å ´åˆã¯åˆ†é¡ã‚’ã‚¹ã‚­ãƒƒãƒ—"
                    },
                    "n_threads": {
                        "type": "integer",
                        "description": "ä½¿ç”¨ã™ã‚‹CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4"
                    },
                    "sampling_depth": {
                        "type": "integer",
                        "description": "å¤šæ§˜æ€§è§£æã®ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ·±åº¦ã€‚denoising-stats ã‚’ç¢ºèªã—ã¦æœ€å°ãƒªãƒ¼ãƒ‰æ•°ã‚’å‚è€ƒã«è¨­å®šã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5000"
                    },
                    "group_column": {
                        "type": "string",
                        "description": "Î²å¤šæ§˜æ€§ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒã«ä½¿ã†ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®åˆ—åï¼ˆä¾‹: group, treatmentï¼‰"
                    }
                },
                "required": ["fastq_dir"]
            }
        }
    }
]

# ğŸº ======================================================================
# ğŸ± ãƒ„ãƒ¼ãƒ«å®Ÿè£…
# ğŸº ======================================================================

def tool_inspect_directory(path: str, recursive: bool = False) -> str:
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹ã‚’èª¿æŸ»"""
    p = Path(path).expanduser()
    if not p.exists():
        return f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{path}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
    if not p.is_dir():
        return f"ã‚¨ãƒ©ãƒ¼: '{path}' ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"

    lines = [f"ğŸ“‚ {p} ã®å†…å®¹:\n"]
    total_files = 0

    def scan(dirpath: Path, depth: int = 0):
        nonlocal total_files
        indent = "  " * depth
        try:
            entries = sorted(dirpath.iterdir(), key=lambda x: (x.is_file(), x.name))
        except PermissionError:
            lines.append(f"{indent}  [æ¨©é™ã‚¨ãƒ©ãƒ¼: ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯]")
            return
        for entry in entries:
            if entry.name.startswith("."):
                continue
            if entry.is_dir():
                lines.append(f"{indent}ğŸ“ {entry.name}/")
                if recursive and depth < 3:
                    scan(entry, depth + 1)
            else:
                size = entry.stat().st_size
                size_str = f"{size:,} B" if size < 1024 else \
                           f"{size/1024:.1f} KB" if size < 1024**2 else \
                           f"{size/1024**2:.1f} MB" if size < 1024**3 else \
                           f"{size/1024**3:.1f} GB"
                ext = entry.suffix.lower()
                icon = {"": "ğŸ“„", ".fastq": "ğŸ§¬", ".gz": "ğŸ—œï¸",
                        ".qza": "ğŸ”µ", ".qzv": "ğŸŸ¢", ".tsv": "ğŸ“Š",
                        ".csv": "ğŸ“Š", ".md": "ğŸ“", ".sh": "âš™ï¸",
                        ".py": "ğŸ", ".r": "ğŸ“ˆ", ".pdf": "ğŸ“•"}.get(ext, "ğŸ“„")
                lines.append(f"{indent}{icon} {entry.name}  [{size_str}]")
                total_files += 1

    scan(p)
    lines.append(f"\nåˆè¨ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")

    # ğŸ± QIIME2 ãƒ‡ãƒ¼ã‚¿åˆ¤å®šã®ãƒ’ãƒ³ãƒˆ
    all_text = "\n".join(lines)
    hints = []
    if "_R1_" in all_text or "_R1." in all_text:
        hints.append("âœ… ãƒšã‚¢ã‚¨ãƒ³ãƒ‰FASTQã‚’æ¤œå‡ºï¼ˆ_R1_/_R2_ ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰")
    elif ".fastq" in all_text:
        hints.append("âœ… FASTQãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º")
    if ".qza" in all_text:
        hints.append("âœ… æ—¢å­˜ã® QIIME2 ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ (.qza) ã‚’æ¤œå‡º â€” é€”ä¸­ã‹ã‚‰å†é–‹å¯èƒ½")
    if "metadata" in all_text.lower() or "sample_info" in all_text.lower():
        hints.append("âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º")
    if "manifest" in all_text.lower():
        hints.append("âœ… ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º")

    if hints:
        lines.append("\nğŸ” è‡ªå‹•åˆ¤å®šãƒ’ãƒ³ãƒˆ:")
        lines.extend(hints)

    return "\n".join(lines)


def tool_read_file(path: str, max_lines: int = 50) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã‚€"""
    p = Path(path).expanduser()
    if not p.exists():
        return f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« '{path}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
    if not p.is_file():
        return f"ã‚¨ãƒ©ãƒ¼: '{path}' ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"

    suffix = p.suffix.lower()
    if suffix in [".gz", ".bz2", ".qza", ".qzv"]:
        return f"'{p.name}' ã¯ãƒã‚¤ãƒŠãƒª/åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãŸã‚å†…å®¹ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚\nãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {p.stat().st_size / 1024**2:.2f} MB"

    try:
        with open(p, encoding="utf-8", errors="replace") as f:
            lines = []
            for i, line in enumerate(f):
                if i >= max_lines:
                    lines.append(f"\n... ï¼ˆ{max_lines} è¡Œä»¥é™ã¯çœç•¥ï¼‰")
                    break
                lines.append(line.rstrip())
        return f"ğŸ“„ {p} ã®å†…å®¹ï¼ˆæœ€å¤§ {max_lines} è¡Œï¼‰:\n\n" + "\n".join(lines)
    except Exception as e:
        return f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"


def _get_docker_cmd() -> Optional[str]:
    """ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ Docker å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã™ã‚‹"""
    # ğŸ± macOS: Docker Desktop ã®å›ºå®šãƒ‘ã‚¹ã‚’å„ªå…ˆ
    if sys.platform == "darwin":
        mac_path = "/Applications/Docker.app/Contents/Resources/bin/docker"
        if Path(mac_path).exists():
            return mac_path
    # ğŸ± Windows / Linux: PATH ã‹ã‚‰æ¤œç´¢
    return shutil.which("docker") or shutil.which("docker.exe")


def tool_check_system() -> str:
    """ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒã®ç¢ºèª"""
    results = ["[ã‚·ã‚¹ãƒ†ãƒ ç¢ºèªçµæœ]\n"]

    # ğŸ± Docker
    docker_cmd = _get_docker_cmd()
    if docker_cmd:
        try:
            result = subprocess.run([docker_cmd, "--version"],
                                    capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                results.append(f"OK Docker: {result.stdout.strip()}")
                ping = subprocess.run([docker_cmd, "info"],
                                      capture_output=True, text=True, timeout=10)
                if ping.returncode == 0:
                    results.append("OK Docker ãƒ‡ãƒ¼ãƒ¢ãƒ³: èµ·å‹•ä¸­")
                else:
                    results.append("!! Docker ãƒ‡ãƒ¼ãƒ¢ãƒ³: åœæ­¢ä¸­ â†’ Docker Desktop ã‚’èµ·å‹•ã—ã¦ãã ã•ã„")
            else:
                results.append("!! Docker: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
        except Exception:
            results.append("!! Docker: ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸ")
    else:
        results.append("NG Docker: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ â†’ Docker Desktop ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")

    # ğŸ± Ollama
    try:
        req = urllib.request.Request("http://localhost:11434/api/tags")
        with urllib.request.urlopen(req, timeout=3) as resp:
            data = json.loads(resp.read())
            models = [m["name"] for m in data.get("models", [])]
            results.append(f"âœ… Ollama: èµ·å‹•ä¸­")
            if models:
                results.append(f"   åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {', '.join(models)}")
            else:
                results.append("   âš ï¸  ãƒ¢ãƒ‡ãƒ«ãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« â†’ 'ollama pull qwen2.5-coder:7b' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    except Exception:
        results.append("âŒ Ollama: èµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ â†’ 'ollama serve' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

    # ğŸ± QIIME2 conda ç’°å¢ƒ
    if QIIME2_CONDA_BIN:
        results.append(f"âœ… QIIME2 conda: {QIIME2_CONDA_BIN}")
        results.append(f"   Python: {QIIME2_PYTHON}")
    else:
        results.append("âš ï¸  QIIME2 conda ç’°å¢ƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆDocker ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œï¼‰")

    # ğŸ± Python
    results.append(f"âœ… Python: {sys.version.split()[0]}")

    # ğŸ± ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡
    usage = shutil.disk_usage(Path.home())
    free_gb = usage.free / 1024**3
    results.append(f"ğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯ç©ºãå®¹é‡: {free_gb:.1f} GB {'âœ…' if free_gb > 30 else 'âš ï¸  (æ¨å¥¨: 30GB ä»¥ä¸Š)'}")

    return "\n".join(results)


def tool_write_file(path: str, content: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã«å†…å®¹ã‚’æ›¸ãè¾¼ã‚€"""
    p = Path(path).expanduser()
    try:
        p.parent.mkdir(parents=True, exist_ok=True)
        with open(p, "w", encoding="utf-8") as f:
            f.write(content)
        # ğŸ± ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆãªã‚‰å®Ÿè¡Œæ¨©é™ã‚’ä»˜ä¸
        if p.suffix in [".sh", ".bash"]:
            p.chmod(p.stat().st_mode | 0o755)
            return f"âœ… '{p}' ã‚’ä½œæˆã—ã¾ã—ãŸï¼ˆå®Ÿè¡Œæ¨©é™ä»˜ãï¼‰"
        return f"âœ… '{p}' ã‚’ä½œæˆã—ã¾ã—ãŸ"
    except Exception as e:
        return f"âŒ æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"


def tool_generate_manifest(fastq_dir: str, output_path: str,
                            paired_end: bool = True,
                            container_data_dir: str = "/data/output") -> str:
    """FASTQãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’è‡ªå‹•ç”Ÿæˆ"""
    # ğŸ± æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»ã—ã¦ãƒ‘ã‚¹ã®äºŒé‡ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é˜²ã
    container_data_dir = container_data_dir.rstrip("/")
    d = Path(fastq_dir).expanduser()
    if not d.exists():
        return f"ã‚¨ãƒ©ãƒ¼: '{fastq_dir}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚"

    # ğŸ± FASTQãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
    fastq_files = sorted(d.glob("*.fastq.gz")) + sorted(d.glob("*.fastq"))

    if not fastq_files:
        return f"ã‚¨ãƒ©ãƒ¼: '{fastq_dir}' ã« FASTQ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

    out_path = Path(output_path).expanduser()

    if paired_end:
        # ğŸ± R1/R2 ãƒšã‚¢ã‚’æ¤œå‡º
        r1_files = [f for f in fastq_files
                    if re.search(r'_R1[_.]|_1\.fastq|_R1\.fastq', f.name)]
        r2_files = [f for f in fastq_files
                    if re.search(r'_R2[_.]|_2\.fastq|_R2\.fastq', f.name)]

        if not r1_files:
            return "ã‚¨ãƒ©ãƒ¼: _R1_ ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

        # ğŸ± ã‚µãƒ³ãƒ—ãƒ«åã‚’æŠ½å‡º
        lines = ["sample-id\tforward-absolute-filepath\treverse-absolute-filepath"]
        matched = 0
        unmatched = []

        # ğŸ± r2_files ã‚’ dict åŒ–ã—ã¦ O(1) ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå¤§é‡ã‚µãƒ³ãƒ—ãƒ«æ™‚ã® O(nÂ²) ã‚’å›é¿ï¼‰
        r2_dict = {f.name: f for f in r2_files}

        for r1 in r1_files:
            # ğŸ± ã‚µãƒ³ãƒ—ãƒ«åã®æ¨å®š
            sample_name = re.sub(r'_R1[_.].*$|_R1\.fastq.*$', '', r1.name)
            sample_name = re.sub(r'\.fastq.*$', '', sample_name)

            # ğŸ± ç©ºã‚µãƒ³ãƒ—ãƒ«åã¯ QIIME2 ãŒæ‹’å¦ã™ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
            if not sample_name:
                unmatched.append(r1.name)
                continue

            # ğŸ± å¯¾å¿œã™ã‚‹ R2 ã‚’æ¢ã™ï¼ˆæœ€åˆã® _R1_ / _R1. ã®ã¿ç½®æ›ã—äºŒé‡ç½®æ›ãƒã‚°ã‚’é˜²ãï¼‰
            r2_pattern = re.sub(r'_R1([_.])', r'_R2\1', r1.name, count=1)
            r2_match = r2_dict.get(r2_pattern)

            # ğŸ± ã‚³ãƒ³ãƒ†ãƒŠå†…ãƒ‘ã‚¹
            container_r1 = f"{container_data_dir}/{r1.name}"

            if r2_match:
                container_r2 = f"{container_data_dir}/{r2_match.name}"
                lines.append(f"{sample_name}\t{container_r1}\t{container_r2}")
                matched += 1
            else:
                unmatched.append(r1.name)

        # ğŸ± ãƒšã‚¢ãŒä¸€ä»¶ã‚‚ãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ã‹ãšã«ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
        if matched == 0:
            return (
                "âŒ ã‚¨ãƒ©ãƒ¼: ãƒšã‚¢ãŒ1çµ„ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
                "ãƒ•ã‚¡ã‚¤ãƒ«åãŒ _R1_/_R1. ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆè‡´ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n"
                f"è¦‹ã¤ã‹ã£ãŸ R1 ãƒ•ã‚¡ã‚¤ãƒ«: {[f.name for f in r1_files]}"
            )

        content = "\n".join(lines) + "\n"
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, "w") as f:
            f.write(content)

        result = [f"âœ… ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’ç”Ÿæˆ: '{out_path}'",
                  f"   ãƒšã‚¢æ•°: {matched} / R1ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(r1_files)}"]
        if unmatched:
            match_pct = matched / len(r1_files) * 100
            if match_pct < 80:
                result.append(f"   âš ï¸  R2ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ« ({100 - match_pct:.0f}% æœªãƒãƒƒãƒ): {', '.join(unmatched)}")
            else:
                result.append(f"   âš ï¸  R2ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(unmatched)}")
        result.append(f"\nå†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:\n{content[:500]}")
        return "\n".join(result)

    else:
        # ğŸ± ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰
        lines = ["sample-id\tabsolute-filepath"]
        for f in fastq_files:
            sample_name = re.sub(r'\.fastq.*$', '', f.name)
            container_path = f"{container_data_dir}/{f.name}"
            lines.append(f"{sample_name}\t{container_path}")

        content = "\n".join(lines) + "\n"
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, "w") as f:
            f.write(content)

        return (f"âœ… ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’ç”Ÿæˆ: '{out_path}'\n"
                f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(fastq_files)}\n"
                f"\nå†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:\n{content[:500]}")


def tool_edit_file(path: str, old_str: str, new_str: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ã‚’æ–‡å­—åˆ—ç½®æ›ã§ç·¨é›†ã™ã‚‹"""
    p = Path(path).expanduser()
    if not p.exists():
        return f"ã‚¨ãƒ©ãƒ¼: '{path}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
    if not p.is_file():
        return f"ã‚¨ãƒ©ãƒ¼: '{path}' ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
    suffix = p.suffix.lower()
    if suffix in [".gz", ".bz2", ".qza", ".qzv"]:
        return f"ã‚¨ãƒ©ãƒ¼: ãƒã‚¤ãƒŠãƒª/åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç·¨é›†ã§ãã¾ã›ã‚“ã€‚"
    try:
        with open(p, encoding="utf-8") as f:
            content = f.read()
        count = content.count(old_str)
        if count == 0:
            # ğŸ± éƒ¨åˆ†ä¸€è‡´ã®ãƒ’ãƒ³ãƒˆã‚’æç¤º
            snippet = old_str[:60].replace('\n', '\\n')
            return (f"ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã—ãŸæ–‡å­—åˆ—ãŒ '{p.name}' ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
                    f"æ¤œç´¢æ–‡å­—åˆ—ï¼ˆå…ˆé ­60å­—ï¼‰: {snippet}\n"
                    f"read_file ã§ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç¢ºèªã—ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
        if count > 1:
            return (f"ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã—ãŸæ–‡å­—åˆ—ãŒ {count} ç®‡æ‰€ã§è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚"
                    f"ã‚ˆã‚Šä¸€æ„ã«ç‰¹å®šã§ãã‚‹æ–‡å­—åˆ—ã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")
        if old_str == new_str:
            return "âš ï¸  old_str ã¨ new_str ãŒåŒä¸€ã§ã™ã€‚ç·¨é›†ã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        new_content = content.replace(old_str, new_str, 1)
        with open(p, "w", encoding="utf-8") as f:
            f.write(new_content)
        old_lines = old_str.count('\n') + 1
        new_lines = new_str.count('\n') + 1
        return f"âœ… '{p.name}' ã‚’ç·¨é›†ã—ã¾ã—ãŸï¼ˆ{old_lines} è¡Œ â†’ {new_lines} è¡Œï¼‰"
    except Exception as e:
        return f"âŒ ç·¨é›†ã‚¨ãƒ©ãƒ¼: {e}"


def tool_run_command(command: str, description: str, working_dir: str = None) -> str:
    """ã‚·ã‚§ãƒ«ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªä»˜ãï¼‰"""
    # ğŸ± working_dir æœªæŒ‡å®šã‹ã¤ã‚»ãƒƒã‚·ãƒ§ãƒ³å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãã“ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ã™ã‚‹
    if not working_dir and SESSION_OUTPUT_DIR:
        working_dir = SESSION_OUTPUT_DIR

    # ğŸ± working_dir ã‚’äº‹å‰æ¤œè¨¼ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç¢ºèªã‚’æ±‚ã‚ã‚‹å‰ã«ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™ï¼‰
    if working_dir:
        cwd = Path(working_dir).expanduser()
        if not cwd.exists():
            return f"âŒ ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {working_dir}"
        if not cwd.is_dir():
            return f"âŒ ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {working_dir}"
    else:
        cwd = None

    print(f"\n{c(ui('cmd_request'), YELLOW)}")
    print(f"   {ui('cmd_desc')}: {description}")
    print(f"   {ui('cmd_cmd')}:\n   {c(command, CYAN)}")

    # ğŸ± issue #31: SEQ2PIPE_AUTO_YES=1 ã®å ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆè‡ªå¾‹ãƒ¢ãƒ¼ãƒ‰ï¼‰
    if AUTO_YES:
        print(f"\n{c(ui('auto_approve'), DIM)}")
    else:
        print(f"\n{c(ui('cmd_confirm'), DIM)}", end=" > ")
        try:
            answer = input().strip().lower()
        except (EOFError, KeyboardInterrupt):
            return ui("cmd_cancelled_ki")

        if answer not in ["y", "yes", "ã¯ã„"]:
            return ui("cmd_cancelled")

    try:
        # ğŸ± QIIME2 conda bin ã‚’ PATH ã®å…ˆé ­ã«è¿½åŠ 
        run_env = os.environ.copy()
        if QIIME2_CONDA_BIN:
            run_env["PATH"] = QIIME2_CONDA_BIN + ":" + run_env.get("PATH", "")
        proc = subprocess.run(
            command, shell=True, capture_output=True, text=True,
            timeout=3600, cwd=cwd, env=run_env
        )
        output_parts = []
        if proc.stdout:
            output_parts.append(f"STDOUT:\n{proc.stdout[:3000]}")
        if proc.stderr:
            output_parts.append(f"STDERR:\n{proc.stderr[:1000]}")

        if proc.returncode == 0:
            return f"âœ… æˆåŠŸï¼ˆçµ‚äº†ã‚³ãƒ¼ãƒ‰ 0ï¼‰\n" + "\n".join(output_parts)
        else:
            return f"âš ï¸  çµ‚äº†ã‚³ãƒ¼ãƒ‰ {proc.returncode}\n" + "\n".join(output_parts)
    except subprocess.TimeoutExpired:
        # ğŸ± subprocess.run() ã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã«è‡ªå‹•ã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’ kill ã—ã¦ã‹ã‚‰å† raise ã™ã‚‹
        return "â±ï¸  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ1æ™‚é–“ã‚’è¶…ãˆã¾ã—ãŸï¼‰ã€‚ã‚³ãƒãƒ³ãƒ‰ã¯å¼·åˆ¶çµ‚äº†ã•ã‚Œã¾ã—ãŸã€‚"
    except Exception as e:
        return f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}"


def tool_set_plot_config(style: str = None, palette: str = None,
                          figsize_w: float = None, figsize_h: float = None,
                          dpi: int = None, font_size: int = None,
                          title_font_size: int = None,
                          fig_format: str = None) -> str:
    """ãƒ—ãƒ­ãƒƒãƒˆè¨­å®šã‚’å¤‰æ›´ã™ã‚‹"""
    changed = []
    if style is not None:
        PLOT_CONFIG["style"] = style
        changed.append(f"style: {style}")
    if palette is not None:
        PLOT_CONFIG["palette"] = palette
        changed.append(f"palette: {palette}")
    if figsize_w is not None or figsize_h is not None:
        w = figsize_w if figsize_w is not None else PLOT_CONFIG["figsize"][0]
        h = figsize_h if figsize_h is not None else PLOT_CONFIG["figsize"][1]
        PLOT_CONFIG["figsize"] = [w, h]
        changed.append(f"figsize: ({w}, {h})")
    if dpi is not None:
        PLOT_CONFIG["dpi"] = dpi
        changed.append(f"dpi: {dpi}")
    if font_size is not None:
        PLOT_CONFIG["font_size"] = font_size
        changed.append(f"font_size: {font_size}")
    if title_font_size is not None:
        PLOT_CONFIG["title_font_size"] = title_font_size
        changed.append(f"title_font_size: {title_font_size}")
    if fig_format is not None:
        fmt = fig_format.lower().lstrip(".")
        if fmt in ("pdf", "png", "svg"):
            PLOT_CONFIG["format"] = fmt
            changed.append(f"format: {fmt}")
        else:
            return f"âŒ ç„¡åŠ¹ãª format: '{fig_format}'ï¼ˆpdf / png / svg ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼‰"
    if changed:
        lines = "\n".join(f"  {item}" for item in changed)
        return f"âœ… ãƒ—ãƒ­ãƒƒãƒˆè¨­å®šã‚’æ›´æ–°ã—ã¾ã—ãŸ:\n{lines}"
    return "å¤‰æ›´ãªã—ï¼ˆæœ‰åŠ¹ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰"


def tool_execute_python(code: str, description: str, output_dir: str = "",
                         subfolder: str = "") -> str:
    """Pythonã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ è§£æãƒ»å¯è¦–åŒ–ã‚’è¡Œã†"""
    global SESSION_FIGURE_DIR

    # ğŸ± å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ±ºå®š
    if not output_dir:
        if not SESSION_FIGURE_DIR:
            ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            SESSION_FIGURE_DIR = str(Path.home() / "seq2pipe_results" / ts)
        output_dir = SESSION_FIGURE_DIR

    out_path = Path(output_dir)
    try:
        out_path.mkdir(parents=True, exist_ok=True)
    except PermissionError:
        return f"âŒ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã§ãã¾ã›ã‚“ï¼ˆæ¨©é™ä¸è¶³ï¼‰: {out_path}"
    except OSError as e:
        return f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {e}"

    # ğŸ± ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å¯¾å¿œï¼ˆè§£æç¨®åˆ¥ã”ã¨ã«å›³ã‚’æ•´ç†ï¼‰
    safe_sub = re.sub(r'[^\w]', '_', subfolder).strip('_') if subfolder else ""
    figures_dir = (out_path / "figures" / safe_sub) if safe_sub else (out_path / "figures")
    try:
        figures_dir.mkdir(parents=True, exist_ok=True)
    except PermissionError:
        return f"âŒ å›³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã§ãã¾ã›ã‚“ï¼ˆæ¨©é™ä¸è¶³ï¼‰: {figures_dir}"
    except OSError as e:
        return f"âŒ å›³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {e}"

    # ğŸ± ãƒ—ãƒªã‚¢ãƒ³ãƒ–ãƒ«: PLOT_CONFIG å¤‰æ•° + å…±é€šã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è‡ªå‹•æ³¨å…¥
    preamble = f"""import sys, os, warnings
warnings.filterwarnings('ignore')

# ğŸ± --- seq2pipe ãƒ“ãƒ«ãƒˆã‚¤ãƒ³å¤‰æ•° ---
FIGURE_DIR = {repr(str(figures_dir))}
OUTPUT_DIR = {repr(str(out_path))}
PLOT_STYLE = {repr(PLOT_CONFIG['style'])}
PLOT_PALETTE = {repr(PLOT_CONFIG['palette'])}
PLOT_FIGSIZE = tuple({PLOT_CONFIG['figsize']})
PLOT_DPI = {PLOT_CONFIG['dpi']}
FONT_SIZE = {PLOT_CONFIG['font_size']}
TITLE_FONT_SIZE = {PLOT_CONFIG['title_font_size']}
FIGURE_FORMAT = {repr(PLOT_CONFIG.get('format', 'pdf'))}

# ğŸ± --- å…±é€šã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    import numpy as np
    import pandas as pd
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    try:
        plt.style.use(PLOT_STYLE)
    except Exception:
        pass
    import seaborn as sns
    sns.set_palette(PLOT_PALETTE)
    matplotlib.rcParams['font.size'] = FONT_SIZE
    matplotlib.rcParams['axes.titlesize'] = TITLE_FONT_SIZE
    matplotlib.rcParams['figure.dpi'] = PLOT_DPI
except ImportError as _e:
    print("{ui('pkg_warning').replace('{}', '')}" + str(_e))
    print("{ui('pkg_hint')}")

# ğŸ± --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ¼ãƒ‰ ---
"""

    full_code = preamble + "\n" + code

    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False,
                                     encoding='utf-8') as f:
        f.write(full_code)
        tmp_path = f.name

    try:
        # ğŸ± å®Ÿè¡Œå‰ã®å›³ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
        existing_figs = set(figures_dir.glob("*.png")) | \
                        set(figures_dir.glob("*.pdf")) | \
                        set(figures_dir.glob("*.svg"))

        # ğŸ± QIIME2 conda Python ã‚’å„ªå…ˆä½¿ç”¨ï¼ˆnumpy/pandas/matplotlib ç­‰ãŒå…¥ã£ã¦ã„ã‚‹ï¼‰
        py_exec = QIIME2_PYTHON if Path(QIIME2_PYTHON).exists() else sys.executable
        proc = subprocess.run(
            [py_exec, tmp_path],
            capture_output=True, text=True,
            timeout=PYTHON_EXEC_TIMEOUT,  # ğŸ± issue #32: ç’°å¢ƒå¤‰æ•° SEQ2PIPE_PYTHON_TIMEOUT ã§ä¸Šæ›¸ãå¯
            cwd=str(out_path)
        )

        stdout = proc.stdout.strip()
        stderr = proc.stderr.strip()

        # ğŸ± æ–°è¦ç”Ÿæˆã•ã‚ŒãŸå›³ã‚’æ¤œå‡º
        new_figs = (set(figures_dir.glob("*.png")) |
                    set(figures_dir.glob("*.pdf")) |
                    set(figures_dir.glob("*.svg"))) - existing_figs
        new_figs = sorted(new_figs)

        # ğŸ± ANALYSIS_LOG ã«è¨˜éŒ²
        ANALYSIS_LOG.append({
            "step": len(ANALYSIS_LOG) + 1,
            "description": description,
            "subfolder": safe_sub,
            "figures": [str(f) for f in new_figs],
            "output_summary": stdout[:600] if stdout else "",
            "returncode": proc.returncode,
            "timestamp": datetime.datetime.now().isoformat(),
        })

        # ğŸ± çµæœãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        parts = []
        if proc.returncode == 0:
            parts.append(f"âœ… è§£æå®Œäº†: {description}")
        else:
            parts.append(f"âš ï¸  è§£æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {description}")
        if stdout:
            parts.append(f"\nğŸ“„ å‡ºåŠ›:\n{stdout[:2000]}")
        if stderr and proc.returncode != 0:
            parts.append(f"\n[STDERR]\n{stderr[:500]}")
        if new_figs:
            parts.append(f"\nğŸ“Š ç”Ÿæˆã•ã‚ŒãŸå›³ ({len(new_figs)} ä»¶):")
            for fig in new_figs:
                parts.append(f"   {fig}")
        else:
            parts.append("\nï¼ˆå›³ã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚savefig ã‚’å‘¼ã‚“ã§ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")

        return "\n".join(parts)

    except subprocess.TimeoutExpired:
        # ğŸ± subprocess.run() ã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã«è‡ªå‹•ã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’ kill ã—ã¦ã‹ã‚‰å† raise ã™ã‚‹
        return (f"â±ï¸  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{PYTHON_EXEC_TIMEOUT}ç§’ã‚’è¶…ãˆã¾ã—ãŸï¼‰ã€‚Pythonãƒ—ãƒ­ã‚»ã‚¹ã¯å¼·åˆ¶çµ‚äº†ã•ã‚Œã¾ã—ãŸã€‚\n"
                f"   ç’°å¢ƒå¤‰æ•° SEQ2PIPE_PYTHON_TIMEOUT ã«å¤§ãã„å€¤ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        return f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}"
    finally:
        Path(tmp_path).unlink(missing_ok=True)


def tool_log_analysis_step(description: str, subfolder: str = "",
                            figures: list = None, summary: str = "") -> str:
    """è§£æã‚¹ãƒ†ãƒƒãƒ—ã‚’ ANALYSIS_LOG ã«æ‰‹å‹•ç™»éŒ²ã™ã‚‹ï¼ˆrun_command çµŒç”±ã® QIIME2 æ“ä½œã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ï¼‰ã€‚
    issue #35: build_report_tex ã¯ ANALYSIS_LOG ã‚’å‚ç…§ã™ã‚‹ãŸã‚ã€run_command å®Ÿè¡Œå¾Œã«ã“ã®ãƒ„ãƒ¼ãƒ«ã§è¨˜éŒ²ã™ã‚‹ã€‚
    """
    safe_sub = re.sub(r'[^\w]', '_', subfolder).strip('_') if subfolder else ""
    # ğŸ± figures ãŒæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã§æ¸¡ã•ã‚ŒãŸå ´åˆã€Path ã¨ã—ã¦æ¤œè¨¼
    validated_figs = []
    if figures:
        for f in figures:
            p = Path(str(f)).expanduser()
            if p.exists():
                validated_figs.append(str(p))
            else:
                validated_figs.append(str(f))  # å­˜åœ¨ç¢ºèªã¯éå¿…é ˆï¼ˆãƒ‘ã‚¹è¨˜éŒ²ã®ã¿ï¼‰

    ANALYSIS_LOG.append({
        "step": len(ANALYSIS_LOG) + 1,
        "description": description,
        "subfolder": safe_sub,
        "figures": validated_figs,
        "output_summary": summary[:600] if summary else "",
        "returncode": 0,
        "timestamp": datetime.datetime.now().isoformat(),
    })
    return (f"âœ… ANALYSIS_LOG ã«ç™»éŒ²ã—ã¾ã—ãŸ (step {len(ANALYSIS_LOG)})\n"
            f"   èª¬æ˜: {description}\n"
            f"   å›³æ•°: {len(validated_figs)}\n"
            f"   åˆè¨ˆã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(ANALYSIS_LOG)}")


# ğŸ± ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ â†’ ã‚»ã‚¯ã‚·ãƒ§ãƒ³å ãƒãƒƒãƒ”ãƒ³ã‚°
_SECTION_JA = {
    "alpha_diversity":        "Î±å¤šæ§˜æ€§è§£æ",
    "beta_diversity":         "Î²å¤šæ§˜æ€§è§£æ",
    "taxonomy":               "åˆ†é¡çµ„æˆè§£æ",
    "differential_abundance": "å·®æ¬¡å­˜åœ¨é‡è§£æ",
    "machine_learning":       "æ©Ÿæ¢°å­¦ç¿’åˆ¤åˆ¥è§£æ",
}
_SECTION_EN = {
    "alpha_diversity":        "Alpha Diversity Analysis",
    "beta_diversity":         "Beta Diversity Analysis",
    "taxonomy":               "Taxonomic Composition Analysis",
    "differential_abundance": "Differential Abundance Analysis",
    "machine_learning":       "Machine Learning Classification",
}
_SUBFOLDER_ORDER = [
    "alpha_diversity", "beta_diversity", "taxonomy",
    "differential_abundance", "machine_learning", "",
]


def _tex_escape(s: str) -> str:
    """TeX ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ï¼ˆé †åºä¾å­˜ã«æ³¨æ„ï¼‰

    å‡¦ç†é †åºã®åŸå‰‡:
    1. \\ ã‚’ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã«é€€é¿ï¼ˆå¾Œç¶šãƒ«ãƒ¼ãƒ—ã§ {} ãŒå†ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚Œã‚‹ã®ã‚’é˜²ãï¼‰
    2. { } ã‚’ ^ ~ ã‚ˆã‚Šå…ˆã«å‡¦ç†ï¼ˆ^ â†’ \\^{} ã® {} ãŒå†ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚Œã‚‹ãƒã‚°ã‚’é˜²ãï¼‰
    3. ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ \\textbackslash{} ã«ç½®æ›ï¼ˆã‚¹ãƒ†ãƒƒãƒ—2 ã® {} ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚’å—ã‘ãªã„ï¼‰
    """
    _BS = "\x00BACKSLASH\x00"
    s = s.replace("\\", _BS)
    for ch, rep in [("&", r"\&"), ("%", r"\%"), ("#", r"\#"),
                    ("_", r"\_"), ("{", r"\{"), ("}", r"\}"),
                    ("$", r"\$"), ("^", r"\^{}"), ("~", r"\~{}")]:
        s = s.replace(ch, rep)
    s = s.replace(_BS, r"\textbackslash{}")
    return s


def _build_tex_content(lang_code: str, title_ja: str, title_en: str,
                        experiment_summary: str,
                        report_dir: Optional[Path] = None) -> str:
    """ANALYSIS_LOG ã‹ã‚‰ TeX ã‚½ãƒ¼ã‚¹ã‚’çµ„ã¿ç«‹ã¦ã‚‹"""
    from collections import defaultdict

    is_ja = (lang_code == "ja")
    title = title_ja if is_ja else title_en
    section_map = _SECTION_JA if is_ja else _SECTION_EN

    groups: dict = defaultdict(list)
    for entry in ANALYSIS_LOG:
        groups[entry.get("subfolder", "")].append(entry)

    total_figs = sum(len(e.get("figures", [])) for e in ANALYSIS_LOG)

    L = []  # lines

    # ğŸ± â”€â”€ ãƒ—ãƒªã‚¢ãƒ³ãƒ–ãƒ« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if is_ja:
        L += [
            r"\documentclass[a4paper,12pt]{article}",
            r"\usepackage{xeCJK}",
            r"\setCJKmainfont{Hiragino Mincho ProN}",
        ]
    else:
        L += [r"\documentclass[a4paper,12pt]{article}"]

    L += [
        r"\usepackage{graphicx}",
        r"\usepackage{booktabs}",
        r"\usepackage{longtable}",
        r"\usepackage{geometry}",
        r"\usepackage[hidelinks]{hyperref}",
        r"\geometry{margin=2.5cm}",
        f"\\title{{{_tex_escape(title)}}}",
        r"\author{seq2pipe}",
        r"\date{\today}",
        r"\begin{document}",
        r"\maketitle",
        r"\tableofcontents",
        r"\newpage",
    ]

    # ğŸ± â”€â”€ æ¦‚è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if is_ja:
        L += [
            r"\section{è§£ææ¦‚è¦}",
            r"æœ¬ãƒ¬ãƒãƒ¼ãƒˆã¯ seq2pipe ã®è‡ªå¾‹æ¢ç´¢ãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦å®Ÿè¡Œã•ã‚ŒãŸè§£æã®è¨˜éŒ²ã§ã™ã€‚",
            r"LLM ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå®Ÿé¨“ç³»ã®æƒ…å ±ã‚’ã‚‚ã¨ã«è¤‡æ•°ã®è§£ææ‰‹æ³•ã‚’è‡ªå‹•ã§é¸æŠãƒ»å®Ÿè¡Œã—ã€",
            r"çµ±è¨ˆçš„æœ‰æ„æ€§ã‚’è©•ä¾¡ã—ãªãŒã‚‰çµæœã‚’æ•´ç†ã—ã¾ã—ãŸã€‚",
            r"\vspace{1em}",
            r"\begin{tabular}{ll}",
            r"\toprule",
            f"ç·è§£æã‚¹ãƒ†ãƒƒãƒ—æ•° & {len(ANALYSIS_LOG)} \\\\",
            f"ç”Ÿæˆã•ã‚ŒãŸå›³ & {total_figs} ä»¶ \\\\",
            r"\bottomrule",
            r"\end{tabular}",
        ]
        if experiment_summary:
            L += [r"\vspace{1em}", r"\subsection{å®Ÿé¨“ç³»}", _tex_escape(experiment_summary)]
    else:
        L += [
            r"\section{Overview}",
            r"This report documents the analyses performed by seq2pipe's autonomous exploration mode.",
            r"The LLM agent automatically selected and executed multiple analysis methods",
            r"based on the experimental context, evaluating statistical significance at each step.",
            r"\vspace{1em}",
            r"\begin{tabular}{ll}",
            r"\toprule",
            f"Total analysis steps & {len(ANALYSIS_LOG)} \\\\",
            f"Figures generated & {total_figs} \\\\",
            r"\bottomrule",
            r"\end{tabular}",
        ]
        if experiment_summary:
            L += [r"\vspace{1em}", r"\subsection{Experimental Setup}",
                  _tex_escape(experiment_summary)]

    # ğŸ± â”€â”€ è§£æãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for sf in _SUBFOLDER_ORDER:
        if sf not in groups:
            continue
        entries = groups[sf]
        sec_name = section_map.get(sf, (_tex_escape(sf) if sf else
                                        ("ãã®ä»–ã®è§£æ" if is_ja else "Other Analyses")))
        L.append(f"\n\\section{{{sec_name}}}")

        for entry in entries:
            desc = _tex_escape(entry.get("description", ""))
            figs = entry.get("figures", [])
            out_summary = entry.get("output_summary", "")
            ok = entry.get("returncode", 0) == 0

            L.append(f"\n\\subsection{{{desc}}}")

            # ğŸ± çµ±è¨ˆå‡ºåŠ›ã®æŠœç²‹
            stat_lines = [line for line in out_summary.split("\n")
                          if any(kw in line.lower() for kw in
                                 ["p =", "p=", "p-value", "pvalue", "accuracy",
                                  "auc", "significant", "æœ‰æ„", "statistic",
                                  "f1", "precision", "recall", "r2", "rmse"])]
            if stat_lines:
                L += [r"\begin{verbatim}"] + stat_lines[:12] + [r"\end{verbatim}"]
            elif not ok:
                L.append(r"\textit{(ã“ã®è§£æã¯ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ)}"
                         if is_ja else
                         r"\textit{(This analysis did not complete due to an error)}")

            # ğŸ± å›³ã®æŒ¿å…¥
            for fig_path in figs:
                caption = desc
                # ğŸ± report_dir ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼ˆtectonic ã®ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹å¯¾ç­–ï¼‰
                if report_dir is not None:
                    try:
                        fig_include = os.path.relpath(fig_path, report_dir).replace("\\", "/")
                    except ValueError:
                        fig_include = fig_path  # Windowsãƒ‰ãƒ©ã‚¤ãƒ–è·¨ãç­‰ã§å¤±æ•—ã—ãŸå ´åˆã¯çµ¶å¯¾ãƒ‘ã‚¹
                else:
                    fig_include = fig_path
                L += [
                    r"\begin{figure}[htbp]",
                    r"\centering",
                    f"\\includegraphics[width=0.85\\textwidth]{{{fig_include}}}",
                    f"\\caption{{{caption}}}",
                    r"\end{figure}",
                ]

    # ğŸ± â”€â”€ è§£æãƒ­ã‚°è¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log_title = "è§£æãƒ­ã‚°" if is_ja else "Analysis Log"
    L += [
        f"\n\\section{{{log_title}}}",
        r"\begin{longtable}{r p{7cm} r r}",
        r"\toprule",
    ]
    if is_ja:
        L.append(r"Step & è§£æ & å›³æ•° & çŠ¶æ…‹ \\ \midrule \endhead")
    else:
        L.append(r"Step & Analysis & Figs & Status \\ \midrule \endhead")

    for entry in ANALYSIS_LOG:
        step = entry.get("step", "")
        desc = _tex_escape(entry.get("description", ""))
        n_figs = len(entry.get("figures", []))
        # ğŸ± âœ“/âœ— ã§ã¯ãªã ASCII æ–‡å­—ã‚’ä½¿ã†ï¼ˆãƒ•ã‚©ãƒ³ãƒˆä¾å­˜ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
        ok = r"\textbf{OK}" if entry.get("returncode", 0) == 0 else r"\textbf{NG}"
        L.append(f"{step} & {desc} & {n_figs} & {ok} \\\\")

    L += [r"\bottomrule", r"\end{longtable}", r"\end{document}"]

    return "\n".join(L)


def tool_build_report_tex(title_ja: str, title_en: str,
                            experiment_summary: str = "",
                            lang: str = "both") -> str:
    """ANALYSIS_LOG ã‹ã‚‰ TeX ã‚’è‡ªå‹•ç”Ÿæˆã—ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹"""
    if not ANALYSIS_LOG:
        return "âŒ ANALYSIS_LOG ãŒç©ºã§ã™ã€‚å…ˆã« execute_python ã§è§£æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"

    # ğŸ± å‡ºåŠ›å…ˆ
    if SESSION_FIGURE_DIR:
        report_dir = Path(SESSION_FIGURE_DIR) / "report"
    else:
        report_dir = Path.home() / "seq2pipe_results" / "report"
    report_dir.mkdir(parents=True, exist_ok=True)

    tectonic_bin = shutil.which("tectonic")
    results = []

    tasks = []
    if lang in ("ja", "both"):
        tasks.append(("report_ja.tex", "ja", "æ—¥æœ¬èª"))
    if lang in ("en", "both"):
        tasks.append(("report_en.tex", "en", "è‹±èª"))

    for filename, lc, label in tasks:
        tex_content = _build_tex_content(lc, title_ja, title_en, experiment_summary, report_dir)
        tex_path = report_dir / filename
        with open(tex_path, "w", encoding="utf-8") as f:
            f.write(tex_content)
        results.append(f"âœ… {label} TeX ã‚’ç”Ÿæˆ: {tex_path}")

        if tectonic_bin:
            try:
                proc = subprocess.run(
                    [tectonic_bin, str(tex_path)],
                    capture_output=True, text=True,
                    timeout=120, cwd=str(report_dir)
                )
                pdf_path = tex_path.with_suffix(".pdf")
                if proc.returncode == 0 and pdf_path.exists():
                    results.append(f"âœ… {label} PDF ã‚’ç”Ÿæˆ: {pdf_path}")
                else:
                    results.append(f"âš ï¸  {label} PDF ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¤±æ•—")
                    if proc.stderr:
                        results.append(f"   {proc.stderr[:300]}")
            except subprocess.TimeoutExpired:
                results.append(f"â±ï¸  {label} ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            except Exception as e:
                results.append(f"âŒ {label} ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            results.append("âš ï¸  tectonic ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚brew install tectonic ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")

    results.append(f"\nğŸ“ å‡ºåŠ›å…ˆ: {report_dir}")
    results.append(f"ğŸ“Š è¨˜éŒ²ã•ã‚ŒãŸè§£æã‚¹ãƒ†ãƒƒãƒ—: {len(ANALYSIS_LOG)}")
    results.append(f"ğŸ–¼ï¸  ç·å›³æ•°: {sum(len(e.get('figures', [])) for e in ANALYSIS_LOG)}")
    return "\n".join(results)


def tool_compile_report(content_ja: str, content_en: str, output_dir: str = "") -> str:
    """TeX ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ PDF ã‚’ç”Ÿæˆã™ã‚‹"""
    if not output_dir:
        if SESSION_FIGURE_DIR:
            output_dir = str(Path(SESSION_FIGURE_DIR) / "report")
        else:
            output_dir = str(Path.home() / "seq2pipe_results" / "report")

    out_path = Path(output_dir)
    out_path.mkdir(parents=True, exist_ok=True)

    results = []
    tectonic = shutil.which("tectonic")

    tasks = []
    if content_ja and content_ja.strip():
        tasks.append(("report_ja.tex", content_ja, "æ—¥æœ¬èª"))
    if content_en and content_en.strip():
        tasks.append(("report_en.tex", content_en, "è‹±èª"))

    if not tasks:
        return "âŒ content_ja ã¨ content_en ã®ä¸¡æ–¹ãŒç©ºã§ã™ã€‚"

    for filename, content, label in tasks:
        tex_path = out_path / filename
        with open(tex_path, 'w', encoding='utf-8') as f:
            f.write(content)
        results.append(f"âœ… {label} TeX ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ: {tex_path}")

        if tectonic:
            try:
                proc = subprocess.run(
                    [tectonic, str(tex_path)],
                    capture_output=True, text=True,
                    timeout=120, cwd=str(out_path)
                )
                pdf_path = tex_path.with_suffix('.pdf')
                if proc.returncode == 0 and pdf_path.exists():
                    results.append(f"âœ… {label} PDF ç”Ÿæˆå®Œäº†: {pdf_path}")
                else:
                    results.append(f"âš ï¸  {label} PDF ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«å•é¡Œ:")
                    if proc.stderr:
                        results.append(f"   {proc.stderr[:400]}")
            except subprocess.TimeoutExpired:
                results.append(f"â±ï¸  {label} ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            except Exception as e:
                results.append(f"âŒ {label} ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            results.append(f"âš ï¸  tectonic ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚brew install tectonic ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€æ‰‹å‹•ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ãã ã•ã„ã€‚")

    return "\n".join(results)


def tool_run_qiime2_pipeline(
    fastq_dir: str,
    paired_end: bool = True,
    trim_left_f: int = 17,
    trim_left_r: int = 21,
    trunc_len_f: int = 270,
    trunc_len_r: int = 220,
    metadata_path: str = "",
    classifier_path: str = "",
    n_threads: int = 4,
    sampling_depth: int = 5000,
    group_column: str = "",
) -> str:
    """
    æ¨™æº– QIIME2 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆâ†’DADA2â†’åˆ†é¡â†’å¤šæ§˜æ€§è§£æï¼‰ã‚’å…¨è‡ªå‹•å®Ÿè¡Œã™ã‚‹ã€‚
    å„ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§é †æ¬¡å®Ÿè¡Œã—ã€çµæœã‚’è¿”ã™ã€‚
    """
    out_dir = SESSION_OUTPUT_DIR if SESSION_OUTPUT_DIR else str(Path.home() / "seq2pipe_results" / "pipeline")
    Path(out_dir).mkdir(parents=True, exist_ok=True)

    run_env = os.environ.copy()
    if QIIME2_CONDA_BIN:
        run_env["PATH"] = QIIME2_CONDA_BIN + ":" + run_env.get("PATH", "")

    completed = []
    failed = []

    def _exec(cmd: str, step: str) -> tuple:
        """ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ã‚¹ãƒ†ãƒƒãƒ—çµæœã‚’è¿”ã™"""
        print(f"\n{c(f'[PIPELINE] {step}', CYAN + BOLD)}")
        print(f"{c(cmd, DIM)}")
        try:
            proc = subprocess.run(
                cmd, shell=True, capture_output=True, text=True,
                timeout=7200, cwd=out_dir, env=run_env
            )
            stdout = proc.stdout[:2000] if proc.stdout else ""
            stderr = proc.stderr[:1000] if proc.stderr else ""
            if proc.returncode == 0:
                print(f"{c('âœ… ' + step, GREEN)}")
                tool_log_analysis_step(description=step, subfolder="pipeline")
                completed.append(f"âœ… {step}")
                return True, stdout
            else:
                print(f"{c('âŒ ' + step, RED)}")
                print(stderr)
                failed.append(f"âŒ {step}")
                return False, stderr
        except subprocess.TimeoutExpired:
            failed.append(f"â±ï¸ {step}: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ2æ™‚é–“è¶…éï¼‰")
            return False, "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"
        except Exception as e:
            failed.append(f"âŒ {step}: {e}")
            return False, str(e)

    # â”€â”€ STEP 0: ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    manifest_path = str(Path(out_dir) / "manifest.tsv")
    manifest_result = tool_generate_manifest(
        fastq_dir=fastq_dir,
        output_path=manifest_path,
        paired_end=paired_end,
    )
    if "âŒ" in manifest_result:
        return f"âŒ ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ:\n{manifest_result}"
    completed.append("âœ… STEP 0: ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆç”Ÿæˆ")
    print(f"{c('âœ… STEP 0: ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆç”Ÿæˆ', GREEN)}")

    # â”€â”€ STEP 1: FASTQ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if paired_end:
        import_cmd = (
            "qiime tools import"
            " --type 'SampleData[PairedEndSequencesWithQuality]'"
            " --input-path manifest.tsv"
            " --output-path paired-end-demux.qza"
            " --input-format PairedEndFastqManifestPhred33V2"
        )
        demux_file = "paired-end-demux.qza"
    else:
        import_cmd = (
            "qiime tools import"
            " --type 'SampleData[SequencesWithQuality]'"
            " --input-path manifest.tsv"
            " --output-path single-end-demux.qza"
            " --input-format SingleEndFastqManifestPhred33V2"
        )
        demux_file = "single-end-demux.qza"

    ok, out = _exec(import_cmd, "STEP 1: FASTQ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
    if not ok:
        return f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—:\n{out}\n\nå®Œäº†æ¸ˆã¿:\n" + "\n".join(completed)

    # â”€â”€ STEP 2: ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ã‚µãƒãƒªãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _exec(
        f"qiime demux summarize --i-data {demux_file} --o-visualization demux-summary.qzv",
        "STEP 2: demux ã‚µãƒãƒªãƒ¼ï¼ˆã‚¯ã‚ªãƒªãƒ†ã‚£ç¢ºèªï¼‰"
    )

    # â”€â”€ STEP 3: DADA2 ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if paired_end:
        dada2_cmd = (
            f"qiime dada2 denoise-paired"
            f" --i-demultiplexed-seqs {demux_file}"
            f" --p-trim-left-f {trim_left_f}"
            f" --p-trim-left-r {trim_left_r}"
            f" --p-trunc-len-f {trunc_len_f}"
            f" --p-trunc-len-r {trunc_len_r}"
            f" --p-n-threads {n_threads}"
            f" --o-table table.qza"
            f" --o-representative-sequences rep-seqs.qza"
            f" --o-denoising-stats denoising-stats.qza"
        )
    else:
        dada2_cmd = (
            f"qiime dada2 denoise-single"
            f" --i-demultiplexed-seqs {demux_file}"
            f" --p-trim-left {trim_left_f}"
            f" --p-trunc-len {trunc_len_f}"
            f" --p-n-threads {n_threads}"
            f" --o-table table.qza"
            f" --o-representative-sequences rep-seqs.qza"
            f" --o-denoising-stats denoising-stats.qza"
        )

    ok, out = _exec(dada2_cmd, "STEP 3: DADA2 ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°")
    if not ok:
        return f"âŒ DADA2 å¤±æ•—:\n{out}\n\nå®Œäº†æ¸ˆã¿:\n" + "\n".join(completed)

    # â”€â”€ STEP 4: ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°çµ±è¨ˆã®è¦–è¦šåŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if metadata_path and Path(metadata_path).exists():
        _exec(
            f"qiime metadata tabulate"
            f" --m-input-file denoising-stats.qza"
            f" --o-visualization denoising-stats.qzv",
            "STEP 4: ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°çµ±è¨ˆã®ç¢ºèª"
        )

    # â”€â”€ STEP 5: ç³»çµ±ç™ºç”Ÿãƒ„ãƒªãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ok_tree, _ = _exec(
        "qiime phylogeny align-to-tree-mafft-fasttree"
        " --i-sequences rep-seqs.qza"
        " --o-alignment aligned-rep-seqs.qza"
        " --o-masked-alignment masked-aligned-rep-seqs.qza"
        " --o-tree unrooted-tree.qza"
        " --o-rooted-tree rooted-tree.qza",
        "STEP 5: ç³»çµ±ç™ºç”Ÿãƒ„ãƒªãƒ¼ç”Ÿæˆ"
    )

    # â”€â”€ STEP 6: åˆ†é¡å­¦çš„æ³¨é‡ˆï¼ˆSILVA138åˆ†é¡å™¨ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    has_taxonomy = False
    if classifier_path and Path(classifier_path).exists():
        ok_tax, _ = _exec(
            f"qiime feature-classifier classify-sklearn"
            f" --i-classifier {classifier_path}"
            f" --i-reads rep-seqs.qza"
            f" --p-n-jobs {n_threads}"
            f" --o-classification taxonomy.qza",
            "STEP 6: åˆ†é¡å­¦çš„æ³¨é‡ˆï¼ˆSILVA138ï¼‰"
        )
        has_taxonomy = ok_tax
        if has_taxonomy and metadata_path and Path(metadata_path).exists():
            _exec(
                "qiime taxa barplot"
                " --i-table table.qza"
                " --i-taxonomy taxonomy.qza"
                f" --m-metadata-file {metadata_path}"
                " --o-visualization taxa-bar-plots.qzv",
                "STEP 6b: ã‚¿ã‚¯ã‚µãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ"
            )
    else:
        completed.append("âš ï¸ STEP 6: åˆ†é¡å™¨ãŒæœªæŒ‡å®šã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆclassifier_path ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼‰")

    # â”€â”€ STEP 7: ã‚³ã‚¢å¤šæ§˜æ€§è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if metadata_path and Path(metadata_path).exists() and ok_tree:
        ok_div, _ = _exec(
            f"qiime diversity core-metrics-phylogenetic"
            f" --i-phylogeny rooted-tree.qza"
            f" --i-table table.qza"
            f" --p-sampling-depth {sampling_depth}"
            f" --m-metadata-file {metadata_path}"
            f" --output-dir core-metrics-results/",
            "STEP 7: Î±ãƒ»Î²å¤šæ§˜æ€§ï¼ˆcore-metrics-phylogeneticï¼‰"
        )
        if ok_div:
            for metric in ["faith_pd", "evenness", "shannon"]:
                _exec(
                    f"qiime diversity alpha-group-significance"
                    f" --i-alpha-diversity core-metrics-results/{metric}_vector.qza"
                    f" --m-metadata-file {metadata_path}"
                    f" --o-visualization core-metrics-results/{metric}-group-significance.qzv",
                    f"STEP 7b: Î±å¤šæ§˜æ€§ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒ ({metric})"
                )
            if group_column:
                _exec(
                    f"qiime diversity beta-group-significance"
                    f" --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza"
                    f" --m-metadata-file {metadata_path}"
                    f" --m-metadata-column {group_column}"
                    f" --o-visualization core-metrics-results/unweighted-unifrac-beta-significance.qzv",
                    "STEP 7c: Î²å¤šæ§˜æ€§ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒï¼ˆUniFracï¼‰"
                )

    # â”€â”€ STEP 8: QZA â†’ TSV/BIOM ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # QIIME2 artifacts ã‚’æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã¦ Python ã§ç›´æ¥è§£æã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
    export_dir = str(Path(out_dir) / "exported")
    Path(export_dir).mkdir(parents=True, exist_ok=True)

    # Feature table (BIOM â†’ TSV)
    _exec(
        f"qiime tools export --input-path table.qza --output-path {export_dir}/table/ && "
        f"biom convert -i {export_dir}/table/feature-table.biom "
        f"-o {export_dir}/feature-table.tsv --to-tsv",
        "STEP 8a: Feature table (ASV counts) ã‚’ TSV ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"
    )
    # Taxonomy
    if has_taxonomy:
        _exec(
            f"qiime tools export --input-path taxonomy.qza --output-path {export_dir}/taxonomy/",
            "STEP 8b: Taxonomy ã‚’ TSV ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"
        )
    # DADA2 denoising stats
    _exec(
        f"qiime tools export --input-path denoising-stats.qza --output-path {export_dir}/denoising_stats/",
        "STEP 8c: DADA2 denoising stats ã‚’ TSV ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"
    )
    # Representative sequences
    _exec(
        f"qiime tools export --input-path rep-seqs.qza --output-path {export_dir}/rep-seqs/",
        "STEP 8d: ä»£è¡¨é…åˆ— (rep-seqs) ã‚’ FASTA ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"
    )
    # Alpha diversity metrics
    for _metric in ["shannon_vector", "faith_pd_vector", "evenness_vector", "observed_features_vector"]:
        _qza = str(Path(out_dir) / "core-metrics-results" / f"{_metric}.qza")
        if Path(_qza).exists():
            _exec(
                f"qiime tools export --input-path core-metrics-results/{_metric}.qza "
                f"--output-path {export_dir}/alpha/{_metric}/",
                f"STEP 8e: {_metric} ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"
            )
    # Beta diversity distance matrices
    for _mat in ["unweighted_unifrac_distance_matrix", "weighted_unifrac_distance_matrix",
                 "bray_curtis_distance_matrix", "jaccard_distance_matrix"]:
        _qza = str(Path(out_dir) / "core-metrics-results" / f"{_mat}.qza")
        if Path(_qza).exists():
            _exec(
                f"qiime tools export --input-path core-metrics-results/{_mat}.qza "
                f"--output-path {export_dir}/beta/{_mat}/",
                f"STEP 8f: {_mat} ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"
            )

    # â”€â”€ STEP 9: Python ã«ã‚ˆã‚‹æœ¬æ ¼è§£æãƒ»å¯è¦–åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    fig_dir = SESSION_FIGURE_DIR if SESSION_FIGURE_DIR else str(Path(out_dir) / "figures")
    Path(fig_dir).mkdir(parents=True, exist_ok=True)

    _analysis_code = f"""
import io, os, sys, glob, zipfile
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# stdout ã‚’ UTF-8 ã«çµ±ä¸€ï¼ˆçµµæ–‡å­—ãƒ»æ—¥æœ¬èªã‚’å®‰å…¨ã«å‡ºåŠ›ã™ã‚‹ãŸã‚ï¼‰
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')

session_dir = Path({repr(out_dir)})
export_dir  = Path({repr(export_dir)})
fig_dir     = Path({repr(fig_dir)})
fig_dir.mkdir(parents=True, exist_ok=True)

dpi       = {PLOT_CONFIG.get('dpi', 300)}
font_size = {PLOT_CONFIG.get('font_size', 12)}
plt.rcParams.update({{
    'figure.dpi'       : dpi,
    'font.size'        : font_size,
    'axes.spines.top'  : False,
    'axes.spines.right': False,
    'savefig.dpi'      : dpi,
}})

import matplotlib.font_manager as _fm
_jp_candidates = ['Hiragino Sans', 'Hiragino Maru Gothic Pro', 'AppleGothic', 'IPAexGothic']
for _fc in _jp_candidates:
    if any(f.name == _fc for f in _fm.fontManager.ttflist):
        plt.rcParams['font.family'] = _fc
        break

warnings_list = []

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. Feature table (ASV counts) ã‚’èª­ã¿è¾¼ã‚€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
asv_table = None
table_tsv = export_dir / "feature-table.tsv"
if table_tsv.exists():
    asv_table = pd.read_csv(table_tsv, sep='\\t', skiprows=1, index_col=0)
    asv_table = asv_table.astype(float)
    print(f"âœ… ASV table: {{asv_table.shape[0]}} ASV x {{asv_table.shape[1]}} ã‚µãƒ³ãƒ—ãƒ«")
    asv_table.to_csv(fig_dir / "asv_counts.csv")
else:
    warnings_list.append("âš ï¸ feature-table.tsv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆtable.qza ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ç¢ºèªï¼‰")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. Taxonomy ã‚’èª­ã¿è¾¼ã‚€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
taxonomy = None
tax_tsv = export_dir / "taxonomy" / "taxonomy.tsv"
if tax_tsv.exists():
    taxonomy = pd.read_csv(tax_tsv, sep='\\t', index_col=0)
    print(f"âœ… Taxonomy: {{len(taxonomy)}} ASV ã®åˆ†é¡æƒ…å ±")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. å±ãƒ¬ãƒ™ãƒ«é›†è¨ˆãƒ»ç›¸å¯¾å­˜åœ¨é‡ãƒ»ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
genus_rel = None
if asv_table is not None and taxonomy is not None:
    def _parse_level(taxon_str, prefix):
        if not isinstance(taxon_str, str):
            return "Unclassified"
        for part in taxon_str.split(';'):
            part = part.strip()
            if part.startswith(prefix + '__'):
                val = part[len(prefix) + 2:].strip()
                return val if val else f"Unclassified {{prefix}}"
        return "Unclassified"

    taxonomy['Phylum'] = taxonomy['Taxon'].apply(lambda x: _parse_level(x, 'p'))
    taxonomy['Family'] = taxonomy['Taxon'].apply(lambda x: _parse_level(x, 'f'))
    taxonomy['Genus']  = taxonomy['Taxon'].apply(lambda x: _parse_level(x, 'g'))
    taxonomy.to_csv(fig_dir / "taxonomy_parsed.csv")

    merged = asv_table.join(taxonomy[['Phylum', 'Family', 'Genus']])

    # å±ãƒ¬ãƒ™ãƒ«é›†è¨ˆ
    sample_cols = asv_table.columns.tolist()
    genus_counts = merged.groupby('Genus')[sample_cols].sum()
    genus_counts.to_csv(fig_dir / "genus_counts.csv")

    # ç›¸å¯¾å­˜åœ¨é‡ (%)
    genus_rel = genus_counts.div(genus_counts.sum(axis=0), axis=1) * 100
    genus_rel.to_csv(fig_dir / "genus_relative_abundance.csv")
    print(f"âœ… å±ãƒ¬ãƒ™ãƒ«é›†è¨ˆ: {{genus_counts.shape[0]}} å±")

    # ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•ï¼ˆTop 10 + Otherï¼‰
    top_n = 10
    top_genera = genus_rel.mean(axis=1).sort_values(ascending=False).head(top_n).index.tolist()
    plot_df = genus_rel.loc[top_genera].copy()
    plot_df.loc['Other'] = genus_rel.drop(index=top_genera).sum(axis=0)
    plot_df = plot_df.T  # è¡Œ=ã‚µãƒ³ãƒ—ãƒ«, åˆ—=å±

    colors = list(plt.cm.tab20.colors[:top_n]) + [(0.75, 0.75, 0.75)]
    fig, ax = plt.subplots(figsize=(max(10, len(plot_df) * 0.9), 6))
    plot_df.plot(kind='bar', stacked=True, ax=ax, color=colors,
                 width=0.8, edgecolor='white', linewidth=0.3)
    ax.set_xlabel('Sample ID', fontsize=font_size)
    ax.set_ylabel('Relative abundance (%)', fontsize=font_size)
    ax.set_title('Genus-level composition (Top 10)', fontsize=font_size + 2, fontweight='bold')
    ax.tick_params(axis='x', rotation=45)
    ax.legend(title='Genus', bbox_to_anchor=(1.01, 1), loc='upper left', fontsize=font_size - 2)
    ax.set_ylim(0, 100)
    plt.tight_layout()
    plt.savefig(fig_dir / 'genus_composition_stacked.pdf', bbox_inches='tight')
    plt.close()
    print('âœ… å±ãƒ¬ãƒ™ãƒ«ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•: genus_composition_stacked.pdf')

    # é–€ãƒ¬ãƒ™ãƒ«ã‚‚é›†è¨ˆãƒ»ä¿å­˜
    phylum_counts = merged.groupby('Phylum')[sample_cols].sum()
    phylum_rel    = phylum_counts.div(phylum_counts.sum(axis=0), axis=1) * 100
    phylum_rel.to_csv(fig_dir / "phylum_relative_abundance.csv")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. DADA2 ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°çµ±è¨ˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
stats_files = list((export_dir / "denoising_stats").glob("*.tsv")) \
              if (export_dir / "denoising_stats").exists() else []
if not stats_files:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: QZA ã‹ã‚‰ç›´æ¥èª­ã‚€
    stats_qza = session_dir / "denoising-stats.qza"
    if stats_qza.exists():
        try:
            with zipfile.ZipFile(stats_qza, 'r') as z:
                for name in z.namelist():
                    if name.endswith('stats.tsv'):
                        with z.open(name) as f:
                            stats_files = [io.BytesIO(f.read())]
                        break
        except Exception as e:
            warnings_list.append(f"DADA2çµ±è¨ˆèª­ã¿è¾¼ã¿å¤±æ•—: {{e}}")

if stats_files:
    try:
        stats_df = pd.read_csv(stats_files[0], sep='\\t', index_col=0)
        req_cols = ['input', 'non-chimeric']
        if all(c in stats_df.columns for c in req_cols):
            stats_df.to_csv(fig_dir / "dada2_stats.csv")
            fig, axes = plt.subplots(1, 2, figsize=(12, 5))
            x = range(len(stats_df))
            axes[0].bar(x, stats_df['input'],        label='Input',       alpha=0.8, color='#4C72B0')
            axes[0].bar(x, stats_df.get('filtered', stats_df['non-chimeric']),
                                                      label='Filtered',    alpha=0.8, color='#DD8452')
            axes[0].bar(x, stats_df['non-chimeric'], label='Non-chimeric', alpha=0.8, color='#55A868')
            axes[0].set_xticks(list(x))
            axes[0].set_xticklabels(stats_df.index, rotation=45, ha='right')
            axes[0].set_xlabel('Sample ID')
            axes[0].set_ylabel('Read count')
            axes[0].set_title('DADA2: ãƒªãƒ¼ãƒ‰æ•°ã®å¤‰åŒ–', fontweight='bold')
            axes[0].legend()
            retention = stats_df['non-chimeric'] / stats_df['input'] * 100
            axes[1].bar(x, retention, color='#55A868', alpha=0.85)
            axes[1].set_xticks(list(x))
            axes[1].set_xticklabels(stats_df.index, rotation=45, ha='right')
            axes[1].set_xlabel('Sample ID')
            axes[1].set_ylabel('Retention (%)')
            axes[1].set_title('DADA2: ãƒªãƒ¼ãƒ‰ä¿æŒç‡', fontweight='bold')
            axes[1].set_ylim(0, 100)
            axes[1].axhline(70, ls='--', color='tomato', lw=1, label='70%åŸºæº–ç·š')
            axes[1].legend()
            plt.tight_layout()
            plt.savefig(fig_dir / 'dada2_stats.pdf', bbox_inches='tight')
            plt.close()
            print('âœ… DADA2çµ±è¨ˆã‚°ãƒ©ãƒ•: dada2_stats.pdf')
    except Exception as e:
        warnings_list.append(f'DADA2çµ±è¨ˆã‚°ãƒ©ãƒ•ç”Ÿæˆå¤±æ•—: {{e}}')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. Î±å¤šæ§˜æ€§ã®èª­ã¿è¾¼ã¿ãƒ»å¯è¦–åŒ–ãƒ»çµ±è¨ˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
alpha_dir = export_dir / "alpha"
alpha_data = {{}}
metric_labels = {{
    'shannon_vector'           : 'Shannon diversity index',
    'faith_pd_vector'          : "Faith's phylogenetic diversity",
    'evenness_vector'          : 'Pielou evenness',
    'observed_features_vector' : 'Observed features (ASVs)',
}}
if alpha_dir.exists():
    for metric_dir in sorted(alpha_dir.iterdir()):
        tsv_files = list(metric_dir.glob("*.tsv"))
        if tsv_files:
            try:
                df = pd.read_csv(tsv_files[0], sep='\\t', index_col=0)
                if len(df.columns) >= 1:
                    alpha_data[metric_dir.name] = df.iloc[:, 0]
                    print(f"âœ… Î±å¤šæ§˜æ€§ {{metric_dir.name}}: {{len(df)}} ã‚µãƒ³ãƒ—ãƒ«")
            except Exception as e:
                warnings_list.append(f"Î±å¤šæ§˜æ€§èª­ã¿è¾¼ã¿å¤±æ•— ({{metric_dir.name}}): {{e}}")

if alpha_data:
    alpha_df = pd.DataFrame(alpha_data)
    alpha_df.to_csv(fig_dir / "alpha_diversity.csv")

    n = len(alpha_df.columns)
    fig, axes = plt.subplots(1, n, figsize=(5 * n, 5), squeeze=False)
    for i, col in enumerate(alpha_df.columns):
        ax = axes[0][i]
        vals = alpha_df[col].dropna()
        ax.bar(range(len(vals)), vals.values, color='steelblue', alpha=0.85, edgecolor='white')
        ax.set_xticks(range(len(vals)))
        ax.set_xticklabels(vals.index, rotation=45, ha='right', fontsize=font_size - 2)
        ax.set_ylabel(metric_labels.get(col, col), fontsize=font_size)
        ax.set_title(metric_labels.get(col, col), fontsize=font_size + 1, fontweight='bold')
    plt.tight_layout()
    plt.savefig(fig_dir / 'alpha_diversity.pdf', bbox_inches='tight')
    plt.close()
    print('âœ… Î±å¤šæ§˜æ€§ã‚°ãƒ©ãƒ•: alpha_diversity.pdf')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. Î²å¤šæ§˜æ€§ PCoAï¼ˆè·é›¢è¡Œåˆ—ã‹ã‚‰ numpy ã§è¨ˆç®—ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
beta_dir = export_dir / "beta"
if beta_dir.exists():
    for matrix_dir in sorted(beta_dir.iterdir()):
        tsv_files = list(matrix_dir.glob("*.tsv"))
        if not tsv_files:
            continue
        try:
            dist_df = pd.read_csv(tsv_files[0], sep='\\t', index_col=0)
            n = len(dist_df)
            D = dist_df.values.astype(float)
            # Double centering (classical MDS / PCoA)
            J = np.eye(n) - np.ones((n, n)) / n
            B = -0.5 * J @ (D ** 2) @ J
            eigvals, eigvecs = np.linalg.eigh(B)
            idx = np.argsort(eigvals)[::-1]
            eigvals, eigvecs = eigvals[idx], eigvecs[:, idx]
            pos = eigvals > 1e-10
            coords = eigvecs[:, pos] * np.sqrt(eigvals[pos])
            var_exp = eigvals[pos] / eigvals[pos].sum() * 100

            n_pcs = min(3, coords.shape[1])
            pcoa_df = pd.DataFrame(
                coords[:, :n_pcs],
                index=dist_df.index,
                columns=[f'PC{{i+1}}' for i in range(n_pcs)]
            )
            pcoa_df.to_csv(fig_dir / f"pcoa_{{matrix_dir.name}}.csv")

            fig, ax = plt.subplots(figsize=(7, 6))
            sc = ax.scatter(pcoa_df['PC1'], pcoa_df['PC2'],
                            s=120, alpha=0.85, color='steelblue',
                            edgecolors='white', linewidths=0.6)
            for sid, row in pcoa_df.iterrows():
                ax.annotate(str(sid), (row['PC1'], row['PC2']),
                            textcoords='offset points', xytext=(6, 4),
                            fontsize=font_size - 3)
            ax.set_xlabel(f"PC1 ({{var_exp[0]:.1f}}%)", fontsize=font_size)
            ax.set_ylabel(f"PC2 ({{var_exp[1]:.1f}}%)" if len(var_exp) > 1 else "PC2", fontsize=font_size)
            title = matrix_dir.name.replace('_distance_matrix', '').replace('_', ' ').title()
            ax.set_title(f'PCoA â€“ {{title}}', fontsize=font_size + 1, fontweight='bold')
            plt.tight_layout()
            plt.savefig(fig_dir / f'pcoa_{{matrix_dir.name}}.pdf', bbox_inches='tight')
            plt.close()
            print(f'âœ… PCoA: pcoa_{{matrix_dir.name}}.pdf')
        except Exception as e:
            warnings_list.append(f'PCoAå¤±æ•— ({{matrix_dir.name}}): {{e}}')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å®Œäº†ã‚µãƒãƒªãƒ¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print('\\n' + '='*60)
print('âœ… Python è§£æãƒ»å¯è¦–åŒ– å®Œäº†')
print(f'ğŸ“ å‡ºåŠ›å…ˆ: {{fig_dir}}')
for f in sorted(fig_dir.glob('*.pdf')):
    print(f'  ğŸ“Š {{f.name}}')
for f in sorted(fig_dir.glob('*.csv')):
    print(f'  ğŸ“‹ {{f.name}}')
if warnings_list:
    print('\\nâš ï¸ è­¦å‘Š:')
    for w in warnings_list:
        print(f'  {{w}}')
"""
    print(f"\n{c('[PIPELINE] STEP 9: Python ã«ã‚ˆã‚‹ ASV è§£æãƒ»å¯è¦–åŒ–', CYAN + BOLD)}")
    viz_result = tool_execute_python(
        code=_analysis_code,
        description="QIIME2å‡ºåŠ›ï¼ˆASV counts / taxonomy / alpha / betaï¼‰ã‚’Pythonã§è§£æãƒ»å¯è¦–åŒ–",
        output_dir=fig_dir,
    )
    if "âœ…" in viz_result:
        completed.append("âœ… STEP 8-9: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ + Pythonè§£æï¼ˆå±çµ„æˆãƒ»Î±/Î²å¤šæ§˜æ€§ãƒ»PCoAï¼‰")
    else:
        failed.append(f"âš ï¸ Pythonè§£æ: {viz_result[:300]}")

    # â”€â”€ ã‚µãƒãƒªãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    sep = "â•" * 56
    summary_lines = [
        sep,
        "ğŸ  QIIME2 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ + å¯è¦–åŒ– å®Œäº†",
        sep,
        *completed,
    ]
    if failed:
        summary_lines += ["", "âš ï¸  å¤±æ•—ã—ãŸã‚¹ãƒ†ãƒƒãƒ—:", *failed]
    summary_lines += [
        "",
        f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {out_dir}",
        f"ğŸ–¼ï¸  å›³ã®ä¿å­˜å…ˆ: {fig_dir}",
        "",
        "â”â”â” æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— â”â”â”",
        "æ¬¡ã¯ build_report_tex ã‚’å‘¼ã³å‡ºã—ã¦ PDF ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚",
        "å¼•æ•°: title_ja, title_en, experiment_summary ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã€‚",
    ]
    return "\n".join(summary_lines)


def dispatch_tool(name: str, args: dict) -> str:
    """ãƒ„ãƒ¼ãƒ«åã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ãƒ„ãƒ¼ãƒ«é–¢æ•°ã‚’å‘¼ã³å‡ºã™"""
    try:
        if name == "inspect_directory":
            return tool_inspect_directory(**args)
        elif name == "read_file":
            return tool_read_file(**args)
        elif name == "check_system":
            return tool_check_system()
        elif name == "write_file":
            return tool_write_file(**args)
        elif name == "generate_manifest":
            return tool_generate_manifest(**args)
        elif name == "edit_file":
            return tool_edit_file(**args)
        elif name == "run_command":
            return tool_run_command(**args)
        elif name == "set_plot_config":
            return tool_set_plot_config(**args)
        elif name == "execute_python":
            return tool_execute_python(**args)
        elif name == "log_analysis_step":
            return tool_log_analysis_step(**args)
        elif name == "compile_report":
            # ğŸ± issue #36: éæ¨å¥¨ãƒ„ãƒ¼ãƒ« â€” build_report_tex ã‚’ä½¿ã†ã‚ˆã†èª˜å°
            return "âš ï¸  compile_report ã¯éæ¨å¥¨ã§ã™ã€‚ä»£ã‚ã‚Šã« build_report_tex ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        elif name == "build_report_tex":
            return tool_build_report_tex(**args)
        elif name == "run_qiime2_pipeline":
            return tool_run_qiime2_pipeline(**args)
        # ğŸ± ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ˆãæ··åŒã•ã‚Œã‚‹åˆ¥åã‚’ build_report_tex ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
        elif name in ("generate_report", "create_report", "make_report", "report"):
            _content = args.get("content_ja") or args.get("content") or args.get("experiment_summary", "")
            _content_en = args.get("content_en", _content)
            return tool_build_report_tex(content_ja=_content, content_en=_content_en)
        else:
            _valid = [
                "inspect_directory", "read_file", "check_system", "write_file",
                "generate_manifest", "edit_file", "run_command", "set_plot_config",
                "execute_python", "log_analysis_step", "build_report_tex",
            ]
            return (
                f"âŒ ä¸æ˜ãªãƒ„ãƒ¼ãƒ«: '{name}'\n"
                f"åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ï¼ˆæ­£ç¢ºãªåå‰ã‚’ä½¿ã†ã“ã¨ï¼‰:\n" +
                "\n".join(f"  - {t}" for t in _valid)
            )
    except TypeError as e:
        return f"âŒ ãƒ„ãƒ¼ãƒ«å¼•æ•°ã‚¨ãƒ©ãƒ¼ ({name}): {e}"
    except Exception as e:
        return f"âŒ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({name}): {e}"


# ğŸº ======================================================================
# ğŸ± Ollama API
# ğŸº ======================================================================

def call_ollama(messages: list, model: str, tools: list = None) -> dict:
    """Ollama /api/chat ã‚’å‘¼ã³å‡ºã™ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹ï¼‰"""
    body = {
        "model": model,
        "messages": messages,
        "stream": True,
    }
    if tools:
        body["tools"] = tools
        body["temperature"] = 0.3  # ãƒ„ãƒ¼ãƒ«å¼•æ•°JSONç”Ÿæˆã®å®‰å®šæ€§å‘ä¸Š

    data = json.dumps(body).encode("utf-8")
    req = urllib.request.Request(
        OLLAMA_URL,
        data=data,
        headers={"Content-Type": "application/json"},
        method="POST"
    )

    full_content = ""
    tool_calls = []
    thinking_content = ""
    _max_content_chars = 20000  # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢: 20KB è¶…ã§æ‰“ã¡åˆ‡ã‚Š
    _repeat_detector: list = []  # ç›´è¿‘ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¹°ã‚Šè¿”ã—æ¤œå‡ºç”¨

    try:
        with urllib.request.urlopen(req, timeout=OLLAMA_TIMEOUT) as resp:
            for raw_line in resp:
                line = raw_line.decode("utf-8").strip()
                if not line:
                    continue
                try:
                    chunk = json.loads(line)
                except json.JSONDecodeError:
                    continue

                msg = chunk.get("message", {})
                content = msg.get("content", "")

                # ğŸ± thinkingï¼ˆæ¨è«–ãƒ–ãƒ­ãƒƒã‚¯ã€qwen3ç­‰ï¼‰
                if msg.get("thinking"):
                    thinking_content += msg["thinking"]
                    continue

                # ğŸ± tool_calls ãŒå«ã¾ã‚Œã‚‹å ´åˆ
                if msg.get("tool_calls"):
                    tool_calls.extend(msg["tool_calls"])

                # ğŸ± ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                if content:
                    print(content, end="", flush=True)
                    full_content += content

                    # ç„¡é™ç¹°ã‚Šè¿”ã—æ¤œå‡º: ç›´è¿‘ 500 æ–‡å­—ãŒåŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¹°ã‚Šè¿”ã—ã¦ã„ãŸã‚‰æ‰“ã¡åˆ‡ã‚‹
                    if len(full_content) > 2000:
                        tail = full_content[-500:]
                        chunk_size = 50
                        chunks = [tail[i:i+chunk_size] for i in range(0, len(tail), chunk_size)]
                        if len(chunks) >= 4 and len(set(chunks[-4:])) == 1:
                            print("\n[âš ï¸  ç¹°ã‚Šè¿”ã—æ¤œå‡º â€” ç”Ÿæˆã‚’ä¸­æ–­]", flush=True)
                            full_content = full_content[:-500] + "\n[TRUNCATED: repetition detected]"
                            break

                    # æœ€å¤§æ–‡å­—æ•°è¶…éã§æ‰“ã¡åˆ‡ã‚Š
                    if len(full_content) > _max_content_chars:
                        print(f"\n[âš ï¸  å¿œç­”ãŒ {_max_content_chars} æ–‡å­—ã‚’è¶…ãˆãŸãŸã‚æ‰“ã¡åˆ‡ã‚Š]", flush=True)
                        break

                if chunk.get("done"):
                    break

        if full_content:
            print()  # æ”¹è¡Œ

        return {
            "content": full_content,
            "tool_calls": tool_calls,
            "thinking": thinking_content
        }

    except urllib.error.HTTPError as e:
        raise ConnectionError(
            f"Ollama HTTP ã‚¨ãƒ©ãƒ¼: {e.code} {e.reason}\n"
            f"è©³ç´°: {e}"
        )
    except urllib.error.URLError as e:
        # ğŸ± socket.timeout ã¯ URLError ã«åŒ…ã¾ã‚Œã¦å±ŠããŸã‚ã€reason ã§åˆ¤å®š
        if isinstance(e.reason, (socket.timeout, TimeoutError)):
            raise ConnectionError(
                f"Ollama ã¸ã®æ¥ç¶šãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆtimeout={OLLAMA_TIMEOUT}sï¼‰ã€‚\nè©³ç´°: {e}"
            )
        raise ConnectionError(
            f"Ollama ã«æ¥ç¶šã§ãã¾ã›ã‚“ï¼ˆ{OLLAMA_URL}ï¼‰ã€‚\n"
            f"'ollama serve' ã‚’åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\nè©³ç´°: {e}"
        )
    except (socket.timeout, TimeoutError) as e:
        # ğŸ± URLError ã«åŒ…ã¾ã‚Œãšã«ç›´æ¥ raise ã•ã‚Œã‚‹ç¨€ãªã‚±ãƒ¼ã‚¹
        raise ConnectionError(
            f"Ollama ã¸ã®æ¥ç¶šãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆtimeout={OLLAMA_TIMEOUT}sï¼‰ã€‚\nè©³ç´°: {e}"
        )


def check_python_deps() -> bool:
    """å¿…é ˆ Python ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒ QIIME2_PYTHON ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã‚‹ã‹ç¢ºèª"""
    # ğŸ± issue #34: scipy/sklearn/statsmodels/biom-format ã‚’è¿½åŠ 
    required_pkgs = [
        ("numpy", "numpy"),
        ("pandas", "pandas"),
        ("matplotlib", "matplotlib"),
        ("seaborn", "seaborn"),
        ("scipy", "scipy"),
        ("sklearn", "scikit-learn"),
        ("statsmodels", "statsmodels"),
        ("biom", "biom-format"),
    ]
    # ğŸ± QIIME2 conda Python ã‚’å„ªå…ˆä½¿ç”¨
    py_exec = QIIME2_PYTHON if Path(QIIME2_PYTHON).exists() else sys.executable
    if py_exec != sys.executable:
        print(f"   {c(ui('qiime2_python', py_exec), DIM)}")
    check_code = "; ".join(f"import {pkg}" for pkg, _ in required_pkgs)
    try:
        proc = subprocess.run(
            [py_exec, "-c", check_code],
            capture_output=True, text=True, timeout=10
        )
        if proc.returncode == 0:
            print(f"   {c(ui('deps_ok'), GREEN)}")
            return True
        else:
            # ğŸ± ImportError ã®å ´åˆ stderr ã‹ã‚‰ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åã‚’æŠ½å‡º
            missing = proc.stderr.strip().split("\n")[-1] if proc.stderr else "ä¸æ˜"
            print(f"   {c(ui('deps_warn', missing), YELLOW)}")
            print(f"   {ui('deps_hint')}")
            pip_pkgs = " ".join(pip for _, pip in required_pkgs)
            install_cmd = f"{py_exec} -m pip install {pip_pkgs}"
            print(f"   {ui('deps_hint2', c(install_cmd, CYAN))}")
            return False
    except Exception:
        return False


def check_ollama_running() -> bool:
    """Ollama ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª"""
    try:
        urllib.request.urlopen("http://localhost:11434/api/tags", timeout=3)
        return True
    except Exception:
        return False


def get_available_models() -> list:
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    try:
        with urllib.request.urlopen("http://localhost:11434/api/tags", timeout=5) as resp:
            data = json.loads(resp.read())
            return [m["name"] for m in data.get("models", [])]
    except Exception:
        return []


# ğŸº ======================================================================
# ğŸ± ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—
# ğŸº ======================================================================

def _extract_tool_calls_from_text(content: str) -> list:
    """
    ãƒ†ã‚­ã‚¹ãƒˆå†…ã® JSON ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æã™ã‚‹ã€‚
    qwen2.5-coder ç­‰ã€ãƒã‚¤ãƒ†ã‚£ãƒ– function calling ã‚’ä½¿ã‚ãšã«ãƒ†ã‚­ã‚¹ãƒˆå†…ã«
    JSON ã‚’åŸ‹ã‚è¾¼ã‚€ãƒ¢ãƒ‡ãƒ«å‘ã‘ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã€‚
    å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
      - ```json\n{"name": "...", "arguments": {...}}\n```
      - {"name": "...", "arguments": {...}}
      - [{"name": "...", "arguments": {...}}, ...]
    """
    found = []

    # 1. ```json ... ``` ã¾ãŸã¯ ``` ... ``` ãƒ–ãƒ­ãƒƒã‚¯ã‚’å„ªå…ˆæŠ½å‡º
    blocks = re.findall(r'```(?:json)?\s*([\[\{].*?[\]\}])\s*```', content, re.DOTALL)

    if not blocks:
        # 2. ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãªã—: "name" ã¨ "arguments" ã‚’ä¸¡æ–¹å«ã‚€ {} ã‚’æ¢ã™
        blocks = re.findall(
            r'(\{[^`<>]*?"name"\s*:\s*"[^"]+?"[^`<>]*?"arguments"\s*:\s*\{.*?\}[^`<>]*?\})',
            content, re.DOTALL
        )

    for raw in blocks:
        try:
            parsed = json.loads(raw.strip())
        except json.JSONDecodeError:
            continue

        items = parsed if isinstance(parsed, list) else [parsed]

        for item in items:
            if not isinstance(item, dict):
                continue
            name = item.get("name")
            args = item.get("arguments", {})
            if name and isinstance(args, dict):
                found.append({"function": {"name": name, "arguments": args}})

    return found


def run_agent_loop(messages: list, model: str, max_steps: int = None):
    # ğŸ± issue #33: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 30 â†’ MAX_AGENT_STEPS(100), ç’°å¢ƒå¤‰æ•° SEQ2PIPE_MAX_STEPS ã§ä¸Šæ›¸ãå¯
    if max_steps is None:
        max_steps = MAX_AGENT_STEPS
    """ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å«ã‚€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ"""
    steps = 0
    while True:
        if steps >= max_steps:
            print(f"\n{c(ui('agent_limit', max_steps), YELLOW)}")
            break
        steps += 1

        print(f"\n{c('ğŸ˜º AI', CYAN + BOLD)}: ", end="", flush=True)

        response = call_ollama(messages, model, tools=TOOLS)

        # ğŸ± content ã‚‚ tool_calls ã‚‚ç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å†è©¦è¡Œï¼ˆç©ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ä¼šè©±ã‚’æ±šæŸ“ã—ãªã„ï¼‰
        if not response["content"] and not response["tool_calls"]:
            print(f"\n{c(ui('empty_response'), YELLOW)}")
            continue

        # ğŸ± ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒã‚¤ãƒ†ã‚£ãƒ– tool_calls ãŒãªãã¦ã‚‚ãƒ†ã‚­ã‚¹ãƒˆå†…ã«JSON ãŒã‚ã‚Œã°è§£æ
        # ï¼ˆqwen2.5-coder ç­‰ã€é–¢æ•°å‘¼ã³å‡ºã—ã‚’ãƒ†ã‚­ã‚¹ãƒˆä¸­ã«åŸ‹ã‚è¾¼ã‚€ãƒ¢ãƒ‡ãƒ«å‘ã‘ï¼‰
        if not response["tool_calls"] and response["content"]:
            _fallback = _extract_tool_calls_from_text(response["content"])
            if _fallback:
                print(f"\n{c('[ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯] ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’æ¤œå‡ºã—ã¾ã—ãŸ', YELLOW)}")
                response["tool_calls"] = _fallback
                response["content"] = ""  # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚ºã«ç§»è¡Œã™ã‚‹ã®ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ã‚¯ãƒªã‚¢

        assistant_msg = {"role": "assistant", "content": response["content"]}

        # ğŸ± tool_calls ãŒã‚ã‚Œã°å®Ÿè¡Œ
        if response["tool_calls"]:
            tool_results = []
            for tc in response["tool_calls"]:
                fn = tc.get("function", {})
                tool_name = fn.get("name", "")
                tool_args = fn.get("arguments", {})
                if isinstance(tool_args, str):
                    try:
                        tool_args = json.loads(tool_args)
                    except json.JSONDecodeError:
                        tool_args = {}

                print(f"\n{c(ui('tool_exec', tool_name), MAGENTA)}")
                print(f"{c(json.dumps(tool_args, ensure_ascii=False, indent=2), DIM)}")

                result = dispatch_tool(tool_name, tool_args)

                print(f"\n{c(ui('tool_result'), GREEN)}")
                print(result)

                tool_results.append({
                    "role": "tool",
                    "content": result
                })

            # ğŸ± tool_calls ã‚’ assistant ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸è¦ â€” å¤–å´ã® if ã§ç¢ºèªæ¸ˆã¿ï¼‰
            assistant_msg["tool_calls"] = response["tool_calls"]

            messages.append(assistant_msg)
            messages.extend(tool_results)

            # ğŸ± ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œã€ç¶šã‘ã¦ AI ã«å¿œç­”ã•ã›ã‚‹
            continue
        else:
            # ğŸ± ãƒ„ãƒ¼ãƒ«ãªã— â†’ é€šå¸¸ã®å¿œç­”ã§çµ‚äº†
            messages.append(assistant_msg)
            break


# ğŸº ======================================================================
# ğŸ± ãƒãƒŠãƒ¼ãƒ»UI
# ğŸº ======================================================================

# ğŸ± ãƒãƒŠãƒ¼æ–‡å­—åˆ—ï¼ˆ"2" ã‚’æ­£ã—ã„ã‚·ãƒ³ã‚°ãƒ«æ–œã‚ã§ä¿®æ­£æ¸ˆã¿ï¼‰
BANNER_LINES = [
    " â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—",
    " â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â•šâ•â•â•â•â–ˆâ–ˆâ•—",
    " â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•”â•â•",
    " â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•",
    " â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—",
    " â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â–€â–€â•â• â•šâ•â•â•â•â•â•",
    " â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—",
    " â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•",
    " â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—",
    " â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â•",
    " â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—",
    " â•šâ•â•     â•šâ•â•â•šâ•â•     â•šâ•â•â•â•â•â•â•",
    "      sequence -> pipeline",
]

# ğŸ± ã‚·ã‚¢ãƒ³ç³»ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ256è‰²ï¼‰
_GRAD = [
    "\033[38;5;23m",   # dark teal
    "\033[38;5;30m",
    "\033[38;5;37m",
    "\033[38;5;44m",
    "\033[38;5;51m",   # bright cyan
    "\033[1;36m",      # bold cyan
    "\033[38;5;87m",
    "\033[38;5;123m",  # pale cyan
    "\033[38;5;87m",
    "\033[1;36m",
    "\033[38;5;51m",
    "\033[38;5;44m",
    "\033[38;5;37m",
]


def print_banner():
    """ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‹ã‚¹ãƒ‘ãƒ¼ã‚¯ãƒ«ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã§ãƒãƒŠãƒ¼ã‚’è¡¨ç¤º"""
    import time
    import random

    n = len(BANNER_LINES)
    is_tty = sys.stdout.isatty()

    if not is_tty:
        for line in BANNER_LINES:
            print(f"{CYAN}{BOLD}{line}{RESET}")
        return

    try:
        # ğŸ± Phase 1: æš—ã„ã‚·ã‚¢ãƒ³ã§ä¸€ç¬è¡¨ç¤º
        for line in BANNER_LINES:
            sys.stdout.write(f"\033[38;5;23m{line}\033[0m\n")
        sys.stdout.flush()
        time.sleep(0.04)

        # ğŸ± Phase 2: ã‚«ãƒ¼ã‚½ãƒ«ã‚’å…ˆé ­ã¸æˆ»ã™
        sys.stdout.write(f"\033[{n}A\r")
        sys.stdout.flush()

        # ğŸ± Phase 3: ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ¼ã§ä¸‹ã‚¹ã‚¤ãƒ¼ãƒ—
        for i, line in enumerate(BANNER_LINES):
            color = _GRAD[i % len(_GRAD)]
            sys.stdout.write(f"\033[2K{color}\033[1m{line}\033[0m\n")
            sys.stdout.flush()
            time.sleep(0.03)

        # ğŸ± Phase 4: ã‚¹ãƒ‘ãƒ¼ã‚¯ãƒ«ï¼ˆãƒ©ãƒ³ãƒ€ãƒ è¡ŒãŒç™½ãå…‰ã‚‹ Ã— 3æ³¢ï¼‰
        for _ in range(3):
            sparks = set(random.sample(range(n), k=min(4, n)))
            sys.stdout.write(f"\033[{n}A\r")
            for i, line in enumerate(BANNER_LINES):
                color = "\033[1;97m" if i in sparks else _GRAD[i % len(_GRAD)]
                sys.stdout.write(f"\033[2K{color}\033[1m{line}\033[0m\n")
            sys.stdout.flush()
            time.sleep(0.09)

        # ğŸ± Phase 5: ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«è½ã¡ç€ã
        sys.stdout.write(f"\033[{n}A\r")
        for i, line in enumerate(BANNER_LINES):
            color = _GRAD[i % len(_GRAD)]
            sys.stdout.write(f"\033[2K{color}\033[1m{line}\033[0m\n")
        sys.stdout.flush()
        print()

    except Exception:
        # ğŸ± ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—æ™‚ã¯é™çš„è¡¨ç¤ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        for line in BANNER_LINES:
            print(f"{CYAN}{BOLD}{line}{RESET}")

INITIAL_MESSAGE = """ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯ QIIME2 + Python ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ è§£æã‚’æ”¯æ´ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ« AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚

å¯¾å¿œã—ã¦ã„ã‚‹è§£æ:
  [QIIME2] ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â†’ DADA2 ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚° â†’ åˆ†é¡ â†’ å¤šæ§˜æ€§è§£æ â†’ å·®æ¬¡è§£æ
  [Python] çµ„æˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— / PCoA å›³ / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ¤åˆ¥ / ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ
  [ãƒ¬ãƒãƒ¼ãƒˆ] è§£æçµ‚äº†å¾Œã« TeX / PDF ãƒ¬ãƒãƒ¼ãƒˆã‚’æ—¥æœ¬èªãƒ»è‹±èªã§è‡ªå‹•ç”Ÿæˆ

å§‹ã‚ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã‚’æ•™ãˆã¦ãã ã•ã„:

  1. ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
     ä¾‹: /Users/yourname/microbiome-data/

  2. å®Ÿé¨“ç³»ã®èª¬æ˜
     ä¾‹: ãƒ’ãƒˆè…¸å†…ç´°èŒã€16S V3-V4 é ˜åŸŸï¼ˆ341F/806Rï¼‰ã€Illumina MiSeq ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ 2Ã—250bp
         ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ« 5 ã‚µãƒ³ãƒ—ãƒ« vs å‡¦ç†ç¾¤ 5 ã‚µãƒ³ãƒ—ãƒ«

  3. è¡Œã„ãŸã„è§£æ
     ä¾‹: åˆ†é¡çµ„æˆã®å¯è¦–åŒ– / Î±ãƒ»Î² å¤šæ§˜æ€§è§£æ / ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®å·®æ¬¡è§£æ / æ©Ÿæ¢°å­¦ç¿’åˆ¤åˆ¥

  4. å›³ã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆçœç•¥å¯ï¼‰
     ä¾‹: ç™½èƒŒæ™¯ãƒ»è‰²ã¯é’ç³» / ãƒ€ãƒ¼ã‚¯ç³» / è«–æ–‡å‘ã‘é«˜è§£åƒåº¦ï¼ˆ300 DPIï¼‰

ä¸€åº¦ã«ã¾ã¨ã‚ã¦æ•™ãˆã¦ã‚‚ã‚‰ã†ã¨ã€ã‚ˆã‚Šçš„ç¢ºãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚
"""

INITIAL_MESSAGE_EN = """Hello! I am a local AI agent specialized in QIIME2 + Python downstream microbiome analysis.

Supported analyses:
  [QIIME2] Import â†’ DADA2 denoising â†’ Taxonomy classification â†’ Diversity analysis â†’ Differential analysis
  [Python] Composition heatmap / PCoA plot / Random forest classification / Network analysis
  [Report] Auto-generate TeX / PDF reports in Japanese or English after analysis

To get started, please provide:

  1. Path to your data directory
     e.g. /Users/yourname/microbiome-data/

  2. Description of your experiment
     e.g. Human gut microbiome, 16S V3-V4 (341F/806R), Illumina MiSeq paired-end 2Ã—250bp
          5 control samples vs 5 treatment samples

  3. Analyses you want to perform
     e.g. Taxonomy composition visualization / Alpha & beta diversity / Differential analysis / ML classification

  4. Figure style (optional)
     e.g. White background, blue palette / Dark theme / High-resolution for publication (300 DPI)

Providing all information at once helps generate a more accurate pipeline.
"""


def select_language() -> str:
    """èµ·å‹•æ™‚ã«æ“ä½œè¨€èªã‚’é¸æŠã™ã‚‹ï¼ˆJA / ENï¼‰ã€‚é¸æŠçµæœã‚’è¿”ã— LANG ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    global LANG
    print(f"\n{CYAN}{BOLD}  Select language / è¨€èªã‚’é¸æŠã—ã¦ãã ã•ã„{RESET}")
    print(f"  {BOLD}[1]{RESET} æ—¥æœ¬èª (Japanese)")
    print(f"  {BOLD}[2]{RESET} English")
    while True:
        try:
            choice = input(f"\n  {BOLD}>{RESET} ").strip()
        except (EOFError, KeyboardInterrupt):
            print(f"\n\n{c(ui('goodbye'), CYAN)}")
            sys.exit(0)
        choice_lower = choice.lower()
        if choice_lower in ("1", "ja", "japanese"):
            LANG = "ja"
            break
        elif choice_lower in ("2", "en", "english"):
            LANG = "en"
            break
        else:
            print(f"  {YELLOW}Please enter 1 or 2 / 1 ã‹ 2 ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„{RESET}")
    print()
    return LANG


def select_model(available_models: list) -> str:
    """ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ"""
    # ğŸ± ç’°å¢ƒå¤‰æ•° QIIME2_AI_MODEL ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æœ€å„ªå…ˆ
    if DEFAULT_MODEL in available_models:
        return DEFAULT_MODEL

    preferred = ["qwen2.5-coder:7b", "qwen2.5-coder:3b", "qwen3:8b",
                 "llama3.2:3b", "llama3.1:8b", "mistral:7b", "codellama:7b"]

    for p in preferred:
        if p in available_models:
            return p
        # ğŸ± ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä¸€è‡´
        for m in available_models:
            if m.startswith(p.split(":")[0]):
                return m

    if available_models:
        return available_models[0]
    return DEFAULT_MODEL


# ğŸº ======================================================================
# ğŸ± ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
# ğŸº ======================================================================

def main():
    # ğŸ± Windows 10+ ã§ ANSI ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚³ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
    if sys.platform == "win32":
        os.system("")

    print_banner()

    # ğŸ± ã‚»ãƒƒã‚·ãƒ§ãƒ³ã”ã¨ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆåŒä¸€ãƒ—ãƒ­ã‚»ã‚¹ã§è¤‡æ•°å›å‘¼ã°ã‚ŒãŸå ´åˆã®æ··å…¥é˜²æ­¢ï¼‰
    global ANALYSIS_LOG, SESSION_OUTPUT_DIR, SESSION_FIGURE_DIR, LANG
    ANALYSIS_LOG = []
    SESSION_OUTPUT_DIR = ""
    SESSION_FIGURE_DIR = ""
    LANG = "ja"  # ğŸ± select_language() ã§ä¸Šæ›¸ãã•ã‚Œã‚‹

    # ğŸ± è¨€èªé¸æŠ
    select_language()

    # ğŸ± Python ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç¢ºèªï¼ˆå¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œã€è­¦å‘Šã®ã¿ï¼‰
    check_python_deps()

    # ğŸ± Ollama èµ·å‹•ç¢ºèª
    if not check_ollama_running():
        print(f"{c(ui('ollama_error'), RED)}")
        print(f"   {ui('ollama_hint')}")
        print(f"   {c('ollama serve', CYAN)}")
        print(f"\n   {ui('ollama_hint2')}")
        print(f"   {c('./setup.sh', CYAN)}")
        sys.exit(1)

    # ğŸ± ãƒ¢ãƒ‡ãƒ«é¸æŠ
    available = get_available_models()
    if not available:
        print(f"{c(ui('no_model'), YELLOW)}")
        print(f"   {ui('no_model_hint', c('ollama pull qwen2.5-coder:7b', CYAN))}")
        print(f"   {ui('no_model_hint2', c('ollama pull llama3.2:3b', CYAN))}")
        sys.exit(1)

    model = select_model(available)
    print(f"{c(ui('model_selected', model), GREEN)}")
    print(f"{c(ui('hint_exit'), DIM)}\n")

    # ğŸ± ã‚»ãƒƒã‚·ãƒ§ãƒ³å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ·å‹•æ™‚ã«ä½œæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
    _ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    _session_root = Path.home() / "seq2pipe_results" / _ts
    _session_root.mkdir(parents=True, exist_ok=True)
    SESSION_OUTPUT_DIR = str(_session_root)
    SESSION_FIGURE_DIR = str(_session_root / "figures")
    Path(SESSION_FIGURE_DIR).mkdir(parents=True, exist_ok=True)
    print(f"{c(ui('session_dir', SESSION_OUTPUT_DIR), GREEN)}")
    print(f"{c(ui('session_dir_hint'), DIM)}\n")

    # ğŸ± è¨€èªã«å¿œã˜ãŸã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨åˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é¸æŠ
    if LANG == "en":
        # ğŸ± SYSTEM_PROMPT ã®æœ«å°¾ã«è¿½è¨˜ã™ã‚‹ã“ã¨ã§ã€Œæœ€å¾Œã®æŒ‡ç¤ºã€ã¨ã—ã¦æ©Ÿèƒ½ã•ã›ã‚‹
        # ğŸ± ï¼ˆLLM ã¯å¾Œæ–¹ã®æŒ‡ç¤ºã‚’å„ªå…ˆã™ã‚‹å‚¾å‘ãŒã‚ã‚‹ãŸã‚ã€å…ˆé ­è¿½è¨˜ã‚ˆã‚Šç¢ºå®Ÿï¼‰
        lang_suffix = (
            "\n\nâ”â”â” LANGUAGE OVERRIDE (highest priority) â”â”â”\n"
            "The user has selected English as the interface language.\n"
            "You MUST respond in English for ALL subsequent messages.\n"
            "This includes explanations, shell scripts, Python code comments, and reports.\n"
            "Do NOT use Japanese in any output."
        )
        initial_msg = INITIAL_MESSAGE_EN
        # ğŸ± è‹±èª: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ³¨å…¥
        session_suffix = (
            f"\n\nâ”â”â” SESSION OUTPUT DIRECTORY â”â”â”\n"
            f"All outputs for this session (QIIME2 artifacts .qza/.qzv, scripts, reports, figures) "
            f"MUST be saved under: {SESSION_OUTPUT_DIR}\n"
            f"  - QIIME2 artifacts: {SESSION_OUTPUT_DIR}/<filename>.qza\n"
            f"  - Figures: {SESSION_FIGURE_DIR}/<filename>.pdf\n"
            f"  - Reports: {SESSION_OUTPUT_DIR}/report/\n"
            f"run_command tool automatically runs in this directory, so relative paths work.\n"
            f"Use relative paths in QIIME2 commands (e.g. --output-path table.qza)."
        )
    else:
        lang_suffix = ""
        initial_msg = INITIAL_MESSAGE
        # ğŸ± æ—¥æœ¬èª: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ³¨å…¥
        session_suffix = (
            f"\n\nâ”â”â” ã‚»ãƒƒã‚·ãƒ§ãƒ³å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª â”â”â”\n"
            f"ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã™ã¹ã¦ã®å‡ºåŠ›ï¼ˆQIIME2 ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ .qza/.qzvã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€ãƒ¬ãƒãƒ¼ãƒˆã€å›³ï¼‰ã¯\n"
            f"ä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã—ã¦ãã ã•ã„: {SESSION_OUTPUT_DIR}\n"
            f"  - QIIME2 ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ: {SESSION_OUTPUT_DIR}/<ãƒ•ã‚¡ã‚¤ãƒ«å>.qza\n"
            f"  - å›³ãƒ»ã‚°ãƒ©ãƒ•: {SESSION_FIGURE_DIR}/<ãƒ•ã‚¡ã‚¤ãƒ«å>.pdf\n"
            f"  - ãƒ¬ãƒãƒ¼ãƒˆ: {SESSION_OUTPUT_DIR}/report/\n"
            f"run_command ãƒ„ãƒ¼ãƒ«ã¯è‡ªå‹•çš„ã«ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§å®Ÿè¡Œã•ã‚Œã¾ã™ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ãŒä½¿ãˆã¾ã™ï¼‰ã€‚\n"
            f"QIIME2 ã‚³ãƒãƒ³ãƒ‰ã§ã¯ç›¸å¯¾ãƒ‘ã‚¹ã‚’ä½¿ã£ã¦ãã ã•ã„ï¼ˆä¾‹: --output-path table.qzaï¼‰ã€‚"
        )

    # ğŸ± ä¼šè©±å±¥æ­´ã‚’åˆæœŸåŒ–
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT + lang_suffix + session_suffix},
        {"role": "assistant", "content": initial_msg}
    ]

    print(f"{c('ğŸ˜º AI', CYAN + BOLD)}: {initial_msg}")

    # ğŸ± ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
    while True:
        try:
            user_input = input(f"\n{c(ui('prompt'), BOLD + GREEN)} > ").strip()
        except (EOFError, KeyboardInterrupt):
            print(f"\n\n{c(ui('goodbye'), CYAN)}")
            break

        if not user_input:
            continue

        if user_input.lower() in ["quit", "exit", "çµ‚äº†", "q"]:
            print(f"\n{c(ui('goodbye'), CYAN)}")
            break

        messages.append({"role": "user", "content": user_input})

        try:
            run_agent_loop(messages, model)
        except ConnectionError as e:
            print(f"\n{c(str(e), RED)}")
            break
        except Exception as e:
            print(f"\n{c(ui('runtime_error', e), RED)}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
