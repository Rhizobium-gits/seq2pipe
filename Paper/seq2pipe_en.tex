% seq2pipe: A Local LLM-Powered AI Agent for Automated QIIME2 Pipeline Generation
% Compile with: pdflatex seq2pipe_en.tex

\documentclass[12pt, a4paper]{article}

% -------------------------------------------------------
% Packages
% -------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{top=25mm, bottom=25mm, left=25mm, right=25mm}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  urlcolor=blue!70!black,
  citecolor=blue!70!black,
  pdftitle={seq2pipe: A Local LLM-Powered AI Agent for Automated QIIME2 Pipeline Generation},
  pdfauthor={Rhizobium-gits, Claude (Anthropic)}
}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{abstract}
\usepackage{microtype}

% -------------------------------------------------------
% Code listing style
% -------------------------------------------------------
\definecolor{codebg}{RGB}{245,245,245}
\definecolor{codecomment}{RGB}{100,100,100}
\definecolor{codestring}{RGB}{160,40,40}
\definecolor{codekeyword}{RGB}{0,80,160}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{codebg},
  commentstyle=\color{codecomment}\itshape,
  keywordstyle=\color{codekeyword}\bfseries,
  stringstyle=\color{codestring},
  basicstyle=\ttfamily\small,
  breakatwhitespace=false,
  breaklines=true,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  numbersep=8pt,
  showspaces=false,
  showstringspaces=false,
  tabsize=2,
  frame=single,
  rulecolor=\color{gray!40},
  xleftmargin=15pt,
}
\lstset{style=mystyle}

% -------------------------------------------------------
% Header / Footer
% -------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small seq2pipe Technical Report}
\fancyhead[R]{\small 2025}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% -------------------------------------------------------
% Section format
% -------------------------------------------------------
\titleformat{\section}{\large\bfseries}{}{0em}{\thesection\quad}
\titleformat{\subsection}{\normalsize\bfseries}{}{0em}{\thesubsection\quad}

% -------------------------------------------------------
% Document
% -------------------------------------------------------
\begin{document}

% Title block
\begin{center}
  {\LARGE \textbf{seq2pipe}}\\[6pt]
  {\large A Local LLM-Powered AI Agent for Automated QIIME2 Pipeline Generation}\\[12pt]
  {\normalsize Rhizobium-gits \quad Claude (Anthropic)}\\[4pt]
  {\small \texttt{https://github.com/Rhizobium-gits/seq2pipe}}\\[4pt]
  {\small 2025}
\end{center}

\vspace{4mm}
\hrule
\vspace{4mm}

% Abstract
\begin{abstract}
We present \textbf{seq2pipe}, an interactive AI agent that automates
microbiome analysis by combining a locally running large language model (LLM)
with the QIIME2 bioinformatics platform.
Given raw FASTQ sequencing data, seq2pipe automatically inspects
the data structure, designs an appropriate QIIME2 analysis pipeline,
generates ready-to-execute shell scripts, and produces a visualization guide ---
all through natural language dialogue.
The entire processing pipeline runs on the user's local machine,
eliminating dependencies on cloud services or paid APIs.
The implementation uses only Python's standard library and leverages
Ollama's local LLM inference engine to support function calling.
Evaluation with the \texttt{qwen2.5-coder:7b} model demonstrates
that practical, well-parameterized QIIME2 pipelines can be reliably generated.
\end{abstract}

\vspace{4mm}
\hrule
\vspace{6mm}

% -------------------------------------------------------
\section{Introduction}
% -------------------------------------------------------

QIIME2~\cite{qiime2} (Quantitative Insights Into Microbial Ecology 2)
has become the de facto standard platform for 16S rRNA amplicon sequencing
analysis in microbiome research. However, QIIME2 carries a steep learning curve:
users must understand multiple data formats, select appropriate parameters for
denoising algorithms (DADA2), manage Docker-containerized execution environments,
and interpret multi-step analysis outputs. These barriers make QIIME2 inaccessible
to many researchers, particularly those without bioinformatics backgrounds.

The rapid advancement of large language models (LLMs) has opened the door to
domain-specific analysis automation through natural language~\cite{brown2020gpt3}.
Yet cloud-hosted LLM services introduce concerns about cost, data privacy,
and network availability. The emergence of local LLM inference frameworks
such as Ollama~\cite{ollama} allows high-quality language reasoning to be
performed entirely on commodity hardware, resolving these concerns.

In this report, we describe the design, architecture, and implementation of
\textbf{seq2pipe}, an interactive AI agent that integrates local LLM inference
with QIIME2 to provide a fully automated, offline-capable microbiome analysis assistant.

% -------------------------------------------------------
\section{Background and Related Work}
% -------------------------------------------------------

\subsection{Complexity of QIIME2 Analysis}

A complete QIIME2 microbiome analysis involves at least eight major steps:
data import, quality inspection, denoising (ASV generation via DADA2),
feature table summarization, phylogenetic tree construction,
taxonomic classification (using SILVA~\cite{silva}),
diversity analysis, and differential abundance analysis.
Each step requires careful selection of command-line parameters that depend on
the sequencing library configuration (paired-end vs.\ single-end),
the 16S rRNA hypervariable region amplified (V1--V3, V3--V4, V4, etc.),
and primer sequences.

\subsection{LLM Agents and Function Calling}

The ReAct framework~\cite{yao2023react} established the paradigm of
LLMs that interleave reasoning and action --- calling external tools
at appropriate times to gather information or perform operations
that cannot be accomplished through text generation alone.
Modern instruction-tuned LLMs supporting function calling
can select and invoke pre-defined tools with structured arguments,
enabling autonomous task completion.

\subsection{Local LLM Inference with Ollama}

Ollama wraps the \texttt{llama.cpp} inference engine into a single binary
distributable across macOS, Linux, and Windows.
It exposes a REST API (\texttt{/api/chat}) compatible with the
OpenAI Chat Completions format, including support for function calling,
making it an ideal backend for locally deployed AI agents.

% -------------------------------------------------------
\section{System Design}
% -------------------------------------------------------

\subsection{Design Principles}

seq2pipe was developed according to five core design principles:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Zero external dependencies}:
    Only Python's standard library (\texttt{json}, \texttt{urllib},
    \texttt{subprocess}, \texttt{pathlib}, etc.) is used,
    eliminating the need for \texttt{pip install}.
  \item \textbf{Fully local execution}:
    Ollama's local inference engine is used exclusively,
    requiring no internet connection after initial setup.
  \item \textbf{Cross-platform compatibility}:
    The agent runs on macOS, Linux, and Windows.
  \item \textbf{Safe command execution}:
    All shell commands require explicit user confirmation before execution.
  \item \textbf{Embedded domain knowledge}:
    The system prompt encodes a complete QIIME2 workflow knowledge base.
\end{enumerate}

\subsection{Overall Architecture}

Figure~\ref{fig:arch} illustrates the overall architecture of seq2pipe.
The user launches the Python agent (\texttt{qiime2\_agent.py})
via platform-specific launch scripts.
The agent communicates with the local Ollama API endpoint
(\texttt{http://localhost:11434/api/chat}) and coordinates
six specialized tools to interact with the filesystem and QIIME2 (via Docker).

\begin{figure}[h]
  \centering
  \begin{verbatim}
  User
    |
    v
  [launch.sh / launch.bat]
    |
    v
  [qiime2_agent.py] <-----> Ollama (localhost:11434)
    |                              |
    |                        [LLM Model]
    |
    +---> [Tool 1] inspect_directory
    +---> [Tool 2] read_file
    +---> [Tool 3] write_file
    +---> [Tool 4] generate_manifest
    +---> [Tool 5] run_command  (Docker / QIIME2)
    +---> [Tool 6] check_system
  \end{verbatim}
  \caption{Overall architecture of seq2pipe}
  \label{fig:arch}
\end{figure}

% -------------------------------------------------------
\section{Implementation Details}
% -------------------------------------------------------

\subsection{Embedding Domain Knowledge in the System Prompt}

The \texttt{SYSTEM\_PROMPT} variable contains a comprehensive QIIME2
workflow knowledge base. This includes:

\begin{itemize}
  \item Automatic data format detection criteria
    (e.g., paired-end detection via \texttt{*\_R1*.fastq.gz} patterns)
  \item Complete QIIME2 commands for all eight analysis steps
    (import through differential abundance analysis)
  \item Region-specific recommended parameters for
    V1--V3 (27F/338R), V3--V4 (341F/806R), and V4 (515F/806R) amplicons
  \item Docker execution command templates
  \item Metadata file format specifications
  \item SILVA 138 taxonomic hierarchy explanation
  \item Common errors and troubleshooting guidance
\end{itemize}

By embedding this knowledge directly into the system prompt,
a general-purpose code LLM is transformed into a QIIME2 domain expert.

\subsection{Tool Definitions and Function Calling}

Six tools are defined in JSON Schema format compatible with
Ollama's function calling interface. Table~\ref{tab:tools}
summarizes the tools and their roles.

\begin{table}[h]
  \centering
  \caption{Tools provided by seq2pipe}
  \label{tab:tools}
  \begin{tabular}{lp{8.5cm}}
    \toprule
    \textbf{Tool Name} & \textbf{Function} \\
    \midrule
    \texttt{inspect\_directory} &
      Lists files in a directory with sizes and automatically
      identifies FASTQ, QZA, and metadata files \\
    \texttt{read\_file} &
      Reads text files (TSV, CSV, Markdown, etc.) up to 50 lines \\
    \texttt{check\_system} &
      Verifies installation status and versions of Docker, Ollama, and Python \\
    \texttt{write\_file} &
      Writes generated scripts, manifests, and README files to disk \\
    \texttt{generate\_manifest} &
      Auto-generates a QIIME2 manifest TSV from a FASTQ directory \\
    \texttt{run\_command} &
      Executes shell commands after explicit user confirmation \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{The Agent Loop}

The \texttt{run\_agent\_loop()} function controls the core reasoning-action
cycle. It sends conversation history to the LLM, detects tool calls
in the response, executes them sequentially, appends results to the
conversation history, and repeats until the LLM produces a final
text-only response (Listing~\ref{lst:loop}).

\begin{lstlisting}[language=Python, caption={Core agent loop implementation}, label={lst:loop}]
def run_agent_loop(messages: list, model: str):
    while True:
        response = call_ollama(messages, model, tools=TOOLS)
        assistant_msg = {"role": "assistant",
                         "content": response["content"]}

        if response["tool_calls"]:
            tool_results = []
            for tc in response["tool_calls"]:
                fn = tc.get("function", {})
                result = dispatch_tool(fn["name"],
                                       fn.get("arguments", {}))
                tool_results.append({"role": "tool",
                                      "content": result})
            assistant_msg["tool_calls"] = response["tool_calls"]
            messages.append(assistant_msg)
            messages.extend(tool_results)
            continue   # Query LLM again with tool results
        else:
            messages.append(assistant_msg)
            break      # Text-only response: end loop
\end{lstlisting}

\subsection{Communication with the Ollama API}

The \texttt{call\_ollama()} function sends JSON requests to Ollama's
\texttt{/api/chat} endpoint with streaming enabled, and processes
server-sent events line by line. It uses only \texttt{urllib.request}
from Python's standard library, without any third-party HTTP clients.

\begin{lstlisting}[language=Python, caption={Ollama API communication}]
def call_ollama(messages, model, tools=None):
    body = {"model": model, "messages": messages,
            "stream": True}
    if tools:
        body["tools"] = tools
    data = json.dumps(body).encode("utf-8")
    req = urllib.request.Request(
        OLLAMA_URL, data=data,
        headers={"Content-Type": "application/json"},
        method="POST"
    )
    with urllib.request.urlopen(req, timeout=300) as resp:
        for raw_line in resp:
            chunk = json.loads(raw_line.decode())
            # Process streaming output and tool call detection
            ...
\end{lstlisting}

\subsection{Cross-Platform Compatibility}

Three platform-specific differences are handled explicitly:

\begin{itemize}
  \item \textbf{Docker detection}:
    On macOS, the Docker Desktop binary path
    (\texttt{/Applications/Docker.app/Contents/Resources/bin/docker})
    is checked first; on other platforms, \texttt{shutil.which("docker")}
    is used (\texttt{\_get\_docker\_cmd()}).
  \item \textbf{Windows ANSI color support}:
    Calling \texttt{os.system("")} activates ANSI escape sequence
    processing in Windows 10+ terminals.
  \item \textbf{Separated launch scripts}:
    Bash shell scripts for macOS/Linux and
    PowerShell + batch files for Windows are provided separately.
\end{itemize}

\subsection{Automatic Manifest Generation}

The \texttt{tool\_generate\_manifest()} function uses regular expressions
to parse FASTQ filenames (\texttt{\_R1\_} / \texttt{\_R2\_} patterns),
automatically generating QIIME2 manifest TSV files for both
paired-end and single-end data. It also performs automatic
path translation to Docker container-internal paths
(default: \texttt{/data/output}).

% -------------------------------------------------------
\section{Analysis Workflow}
% -------------------------------------------------------

The QIIME2 pipeline generated by seq2pipe consists of eight steps
as summarized in Table~\ref{tab:workflow}.

\begin{table}[h]
  \centering
  \caption{Overview of the generated QIIME2 analysis pipeline}
  \label{tab:workflow}
  \begin{tabular}{clp{7.5cm}}
    \toprule
    \textbf{Step} & \textbf{Process} & \textbf{Key Command} \\
    \midrule
    1 & Data import &
      \texttt{qiime tools import} \\
    2 & Quality visualization &
      \texttt{qiime demux summarize} \\
    3 & Denoising (ASV generation) &
      \texttt{qiime dada2 denoise-paired} \\
    4 & Feature table summarization &
      \texttt{qiime feature-table summarize} \\
    5 & Phylogenetic tree construction &
      \texttt{qiime phylogeny align-to-tree-mafft-fasttree} \\
    6 & Taxonomic classification &
      \texttt{qiime feature-classifier classify-sklearn} \\
    7 & Diversity analysis &
      \texttt{qiime diversity core-metrics-phylogenetic} \\
    8 & Differential abundance (opt.) &
      \texttt{qiime composition ancombc} \\
    \bottomrule
  \end{tabular}
\end{table}

Taxonomic classification is performed using a Naive Bayes classifier
trained on the SILVA 138 reference database~\cite{silva}.
Primer trim lengths and truncation positions are automatically
adjusted based on the detected hypervariable region
(V1--V3: 27F/338R, V3--V4: 341F/806R, V4: 515F/806R).

% -------------------------------------------------------
\section{Supported Models and Performance}
% -------------------------------------------------------

seq2pipe is model-agnostic and can be used with any LLM available
in Ollama. Table~\ref{tab:models} lists recommended models.

\begin{table}[h]
  \centering
  \caption{Supported models}
  \label{tab:models}
  \begin{tabular}{llll}
    \toprule
    \textbf{Model} & \textbf{RAM Required} & \textbf{Size} & \textbf{Characteristics} \\
    \midrule
    \texttt{qwen2.5-coder:7b} & 8 GB+ & \textasciitilde4.7 GB &
      Code generation optimized (recommended) \\
    \texttt{qwen2.5-coder:3b} & 4 GB+ & \textasciitilde1.9 GB &
      Lightweight and fast \\
    \texttt{llama3.2:3b}      & 4 GB+ & \textasciitilde2.0 GB &
      General purpose, strong dialogue \\
    \texttt{qwen3:8b}         & 16 GB+ & \textasciitilde5.2 GB &
      Highest quality, strong reasoning \\
    \bottomrule
  \end{tabular}
\end{table}

The code-generation-optimized \texttt{qwen2.5-coder:7b} is recommended
as the default model. For RAM-constrained environments,
\texttt{qwen2.5-coder:3b} or \texttt{llama3.2:3b} provide a
lighter-weight alternative.

% -------------------------------------------------------
\section{Setup and Launch}
% -------------------------------------------------------

\subsection{Software Requirements}

Table~\ref{tab:requirements} summarizes the required software stack.

\begin{table}[h]
  \centering
  \caption{Software requirements}
  \label{tab:requirements}
  \begin{tabular}{lll}
    \toprule
    \textbf{Software} & \textbf{Purpose} & \textbf{Installation} \\
    \midrule
    Python 3.9+ & Agent runtime & Typically pre-installed \\
    Ollama & Local LLM inference & Automated via \texttt{setup.sh} \\
    Docker Desktop/Engine & QIIME2 execution environment & Manual install \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Launch Sequence}

\begin{enumerate}[leftmargin=2em]
  \item Run \texttt{setup.sh} (macOS/Linux) or \texttt{setup.bat} (Windows)
    to install Ollama and download the LLM model.
  \item Run \texttt{launch.sh} / \texttt{launch.bat}: the script verifies
    Ollama and Docker availability, then starts the agent.
  \item Through natural language dialogue, provide the data directory path
    and specify the desired analyses
    (taxonomic composition, diversity, differential abundance, etc.).
  \item The agent automatically inspects the data and generates
    a customized script bundle and operation guide.
\end{enumerate}

% -------------------------------------------------------
\section{Discussion}
% -------------------------------------------------------

\subsection{Achieved Goals}

seq2pipe achieves the following:

\begin{itemize}
  \item Zero-dependency implementation using only Python's standard library
  \item Fully offline operation via local LLM inference
  \item Complete cross-platform support (macOS / Linux / Windows)
  \item Automatic recognition of FASTQ data structure and adaptive pipeline generation
  \item Safe QIIME2 command execution with explicit user confirmation
\end{itemize}

\subsection{Current Limitations}

\begin{itemize}
  \item The accuracy of generated QIIME2 commands depends on
    the underlying LLM model quality; incorrect parameters may be produced.
  \item Performance with large datasets (100+ samples) has not been formally evaluated.
  \item Initial SILVA 138 classifier training requires approximately 30 GB
    of disk space and several hours of computation.
\end{itemize}

\subsection{Future Directions}

\begin{itemize}
  \item Implementation of automated command validation and error correction
  \item Web UI integration (Gradio or Streamlit)
  \item Support for additional marker genes (ITS, 18S rRNA)
  \item Feedback loop incorporating pipeline execution results
    to iteratively refine parameters
\end{itemize}

% -------------------------------------------------------
\section{Conclusion}
% -------------------------------------------------------

We have described seq2pipe, an interactive AI agent that integrates
local LLM inference with the QIIME2 microbiome analysis platform.
By embedding comprehensive QIIME2 domain knowledge into the system prompt
and providing six specialized tools through Ollama's function calling interface,
seq2pipe enables researchers to automate complex 16S rRNA analysis workflows
through natural language dialogue --- entirely offline and without any
cloud dependencies.

seq2pipe is released as open source under the MIT License and is available at:
\texttt{https://github.com/Rhizobium-gits/seq2pipe}

% -------------------------------------------------------
% References
% -------------------------------------------------------
\begin{thebibliography}{9}

\bibitem{qiime2}
  Bolyen, E., et al. (2019).
  \textit{Reproducible, interactive, scalable and extensible microbiome
    data science using QIIME 2}.
  Nature Biotechnology, 37, 852--857.

\bibitem{brown2020gpt3}
  Brown, T. B., et al. (2020).
  \textit{Language Models are Few-Shot Learners}.
  Advances in Neural Information Processing Systems, 33, 1877--1901.

\bibitem{yao2023react}
  Yao, S., et al. (2023).
  \textit{ReAct: Synergizing Reasoning and Acting in Language Models}.
  International Conference on Learning Representations (ICLR 2023).

\bibitem{ollama}
  Ollama (2023).
  \textit{Ollama: Get up and running with large language models locally}.
  \url{https://ollama.com/}

\bibitem{silva}
  Quast, C., et al. (2013).
  \textit{The SILVA ribosomal RNA gene database project:
    improved data processing and web-based tools}.
  Nucleic Acids Research, 41(D1), D590--D596.

\bibitem{dada2}
  Callahan, B. J., et al. (2016).
  \textit{DADA2: High-resolution sample inference from Illumina amplicon data}.
  Nature Methods, 13(7), 581--583.

\end{thebibliography}

\end{document}
