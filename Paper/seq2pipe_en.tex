% seq2pipe: A Local LLM-Powered AI Agent for Automated QIIME2 Pipeline Generation
% Compile with: pdflatex seq2pipe_en.tex  or  tectonic seq2pipe_en.tex

\documentclass[12pt, a4paper]{article}

% -------------------------------------------------------
% Packages
% -------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{top=25mm, bottom=25mm, left=25mm, right=25mm}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  urlcolor=blue!70!black,
  citecolor=blue!70!black,
  pdftitle={seq2pipe: A Local LLM-Powered AI Agent for Automated QIIME2 Pipeline Generation},
  pdfauthor={Rhizobium-gits, Claude (Anthropic)}
}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{abstract}
\usepackage{microtype}

% -------------------------------------------------------
% Code listing style
% -------------------------------------------------------
\definecolor{codebg}{RGB}{245,245,245}
\definecolor{codecomment}{RGB}{100,100,100}
\definecolor{codestring}{RGB}{160,40,40}
\definecolor{codekeyword}{RGB}{0,80,160}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{codebg},
  commentstyle=\color{codecomment}\itshape,
  keywordstyle=\color{codekeyword}\bfseries,
  stringstyle=\color{codestring},
  basicstyle=\ttfamily\small,
  breakatwhitespace=false,
  breaklines=true,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  numbersep=8pt,
  showspaces=false,
  showstringspaces=false,
  tabsize=2,
  frame=single,
  rulecolor=\color{gray!40},
  xleftmargin=15pt,
}
\lstset{style=mystyle}

% -------------------------------------------------------
% Header / Footer
% -------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small seq2pipe Technical Report}
\fancyhead[R]{\small 2026}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% -------------------------------------------------------
% Section format
% -------------------------------------------------------
\titleformat{\section}{\large\bfseries}{}{0em}{\thesection\quad}
\titleformat{\subsection}{\normalsize\bfseries}{}{0em}{\thesubsection\quad}

% -------------------------------------------------------
% Document
% -------------------------------------------------------
\begin{document}

% Title block
\begin{center}
  {\LARGE \textbf{seq2pipe}}\\[6pt]
  {\large A Local LLM-Powered AI Agent for Automated QIIME2 Pipeline Generation}\\[12pt]
  {\normalsize Rhizobium-gits \quad Claude (Anthropic)}\\[4pt]
  {\small \texttt{https://github.com/Rhizobium-gits/seq2pipe}}\\[4pt]
  {\small 2026}
\end{center}

\vspace{4mm}
\hrule
\vspace{4mm}

% Abstract
\begin{abstract}
We present \textbf{seq2pipe}, an interactive AI agent that automates end-to-end
microbiome analysis by combining a locally running large language model (LLM)
with the QIIME2 bioinformatics platform.
Given raw FASTQ sequencing data, seq2pipe automatically inspects
the data structure, designs an appropriate QIIME2 analysis pipeline,
generates ready-to-execute shell scripts, performs Python-based downstream
analysis (diversity, taxonomy, differential abundance, machine learning),
saves all figures as PDF, and produces bilingual (Japanese/English)
TeX/PDF reports --- all through natural language dialogue.
The entire workflow runs on the user's local machine, eliminating
dependencies on cloud services or paid APIs.
The implementation uses only Python's standard library and leverages
Ollama's local LLM inference engine to orchestrate 11 specialized tools
via function calling.
A startup language selector (Japanese/English), automatic Python dependency
checking, and comprehensive error handling make seq2pipe immediately
usable by researchers without bioinformatics backgrounds.
\end{abstract}

\vspace{4mm}
\hrule
\vspace{6mm}

% -------------------------------------------------------
\section{Introduction}
% -------------------------------------------------------

QIIME2~\cite{qiime2} (Quantitative Insights Into Microbial Ecology 2)
has become the de facto standard platform for 16S rRNA amplicon sequencing
analysis in microbiome research. However, QIIME2 carries a steep learning curve:
users must understand multiple data formats, select appropriate parameters for
denoising algorithms (DADA2), manage Docker-containerized execution environments,
and interpret multi-step analysis outputs. These barriers make QIIME2 inaccessible
to many researchers, particularly those without bioinformatics backgrounds.
Furthermore, visualization of QIIME2 outputs (\texttt{.qzv} artifacts) typically
requires an online viewer (\texttt{view.qiime2.org}), creating an additional
dependency that is unavailable in offline or restricted-network environments.

The rapid advancement of large language models (LLMs) has opened the door to
domain-specific analysis automation through natural language~\cite{brown2020gpt3}.
Yet cloud-hosted LLM services introduce concerns about cost, data privacy,
and network availability. The emergence of local LLM inference frameworks
such as Ollama~\cite{ollama} allows high-quality language reasoning to be
performed entirely on commodity hardware, resolving these concerns.

In this report, we describe the design, architecture, and implementation of
\textbf{seq2pipe}, an interactive AI agent that integrates local LLM inference
with QIIME2 to provide a fully automated, offline-capable microbiome analysis
assistant. seq2pipe goes well beyond pipeline generation: it performs Python-based
downstream statistical analysis, saves publication-quality figures, and
automatically generates bilingual research reports --- all within a single
conversational interface.

% -------------------------------------------------------
\section{Background and Related Work}
% -------------------------------------------------------

\subsection{Complexity of QIIME2 Analysis}

A complete QIIME2 microbiome analysis involves at least eight major steps:
data import, quality inspection, denoising (ASV generation via DADA2),
feature table summarization, phylogenetic tree construction,
taxonomic classification (using SILVA~\cite{silva}),
diversity analysis, and differential abundance analysis.
Each step requires careful selection of command-line parameters that depend on
the sequencing library configuration (paired-end vs.\ single-end),
the 16S rRNA hypervariable region amplified (V1--V3, V3--V4, V4, etc.),
and primer sequences.

\subsection{LLM Agents and Function Calling}

The ReAct framework~\cite{yao2023react} established the paradigm of
LLMs that interleave reasoning and action --- calling external tools
at appropriate times to gather information or perform operations
that cannot be accomplished through text generation alone.
Modern instruction-tuned LLMs supporting function calling
can select and invoke pre-defined tools with structured arguments,
enabling autonomous task completion.

\subsection{Local LLM Inference with Ollama}

Ollama wraps the \texttt{llama.cpp} inference engine into a single binary
distributable across macOS, Linux, and Windows.
It exposes a REST API (\texttt{/api/chat}) compatible with the
OpenAI Chat Completions format, including support for function calling,
making it an ideal backend for locally deployed AI agents.

% -------------------------------------------------------
\section{System Design}
% -------------------------------------------------------

\subsection{Design Principles}

seq2pipe was developed according to seven core design principles:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Zero external dependencies}:
    Only Python's standard library (\texttt{json}, \texttt{urllib},
    \texttt{subprocess}, \texttt{pathlib}, \texttt{socket}, etc.) is used,
    eliminating the need for \texttt{pip install}.
  \item \textbf{Fully local execution}:
    Ollama's local inference engine is used exclusively,
    requiring no internet connection after initial setup.
  \item \textbf{Cross-platform compatibility}:
    The agent runs on macOS, Linux, and Windows.
  \item \textbf{Safe command execution}:
    All shell commands require explicit user confirmation before execution.
  \item \textbf{Embedded domain knowledge}:
    The system prompt encodes a complete QIIME2 workflow knowledge base.
  \item \textbf{Multilingual UI}:
    The user selects Japanese or English at startup; all AI responses
    and auto-generated reports follow this preference.
  \item \textbf{Robust error handling}:
    Connection errors, timeouts, empty responses, and filesystem errors
    are all caught and reported with user-friendly messages.
\end{enumerate}

\subsection{Overall Architecture}

Figure~\ref{fig:arch} illustrates the overall architecture of seq2pipe.
The user launches the Python agent (\texttt{qiime2\_agent.py})
via platform-specific launch scripts.
The agent communicates with the local Ollama API endpoint
(\texttt{http://localhost:11434/api/chat}) and coordinates
11 specialized tools to interact with the filesystem, QIIME2 (via Docker),
and Python analysis libraries.

\begin{figure}[h]
  \centering
  \begin{verbatim}
  User (language selection: Japanese / English)
    |
    v
  [launch.sh / launch.bat]
    |
    v
  [qiime2_agent.py] <-----> Ollama (localhost:11434)
    |                              |
    |                        [LLM Model]
    |
    +---> [Tool  1] inspect_directory  (data structure survey)
    +---> [Tool  2] read_file          (read text files)
    +---> [Tool  3] write_file         (write scripts / manifests)
    +---> [Tool  4] edit_file          (partial file editing)
    +---> [Tool  5] generate_manifest  (QIIME2 manifest generation)
    +---> [Tool  6] run_command        (Docker / QIIME2 execution)
    +---> [Tool  7] check_system       (environment check)
    +---> [Tool  8] set_plot_config    (figure style / DPI / format)
    +---> [Tool  9] execute_python     (Python analysis & visualization)
    +---> [Tool 10] build_report_tex   (TeX/PDF report generation)
    +---> [Tool 11] compile_report     (LLM-written TeX compilation)
  \end{verbatim}
  \caption{Overall architecture of seq2pipe}
  \label{fig:arch}
\end{figure}

% -------------------------------------------------------
\section{Implementation Details}
% -------------------------------------------------------

\subsection{Startup Sequence and Multilingual UI}

On launch, \texttt{select\_language()} prompts the user to choose
Japanese (\texttt{ja}) or English (\texttt{en}).
The selection is stored in the \texttt{LANG} global variable, and the
\texttt{ui()} function returns all interface text in the chosen language.
Auto-generated reports follow the same language setting.

\texttt{check\_python\_deps()} verifies that numpy, pandas, matplotlib,
and seaborn are available at startup via \texttt{subprocess},
guiding the user to install any missing packages before analysis begins.

\subsection{Embedding Domain Knowledge in the System Prompt}

The \texttt{SYSTEM\_PROMPT} variable contains a comprehensive QIIME2
workflow knowledge base. This includes:

\begin{itemize}
  \item Automatic data format detection criteria
    (e.g., paired-end detection via \texttt{*\_R1*.fastq.gz} patterns)
  \item Complete QIIME2 commands for all eight analysis steps
    (import through differential abundance analysis)
  \item Region-specific recommended parameters for
    V1--V3 (27F/338R), V3--V4 (341F/806R), and V4 (515F/806R) amplicons
  \item Docker execution command templates
  \item Metadata file format specifications
  \item SILVA 138 taxonomic hierarchy explanation
  \item Common errors and troubleshooting guidance
  \item Python downstream analysis and autonomous exploration mode guidelines
\end{itemize}

By embedding this knowledge directly into the system prompt,
a general-purpose code LLM is transformed into a QIIME2 domain expert.

\subsection{Tool Definitions and Function Calling}

Eleven tools are defined in JSON Schema format compatible with
Ollama's function calling interface. Table~\ref{tab:tools}
summarizes the tools and their roles.

\begin{table}[h]
  \centering
  \caption{Tools provided by seq2pipe}
  \label{tab:tools}
  \begin{tabular}{lp{8cm}}
    \toprule
    \textbf{Tool Name} & \textbf{Function} \\
    \midrule
    \texttt{inspect\_directory} &
      Lists files with sizes and automatically identifies
      FASTQ, QZA, and metadata files \\
    \texttt{read\_file} &
      Reads text files (TSV, CSV, Markdown, etc.) up to 50 lines \\
    \texttt{write\_file} &
      Writes generated scripts, manifests, and README files to disk \\
    \texttt{edit\_file} &
      Replaces a unique string in an existing file (old\_str $\to$ new\_str) \\
    \texttt{generate\_manifest} &
      Auto-generates a QIIME2 manifest TSV from a FASTQ directory \\
    \texttt{run\_command} &
      Executes shell commands after explicit user confirmation \\
    \texttt{check\_system} &
      Verifies Docker, Ollama, Python, and available disk space \\
    \texttt{set\_plot\_config} &
      Configures matplotlib style, palette, DPI, font size, and output format \\
    \texttt{execute\_python} &
      Executes Python code for analysis/visualization, saves figures
      to \texttt{FIGURE\_DIR}, and appends steps to \texttt{ANALYSIS\_LOG} \\
    \texttt{build\_report\_tex} &
      Programmatically builds TeX from \texttt{ANALYSIS\_LOG} and compiles
      bilingual Japanese/English PDF reports via tectonic \\
    \texttt{compile\_report} &
      Receives LLM-written TeX source and compiles it to PDF (legacy mode) \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{The Agent Loop}

The \texttt{run\_agent\_loop()} function controls the core reasoning-action
cycle. It sends conversation history to the LLM, detects tool calls
in the response, executes them sequentially, appends results to the
conversation history, and repeats until the LLM produces a final
text-only response. An empty-response guard retries automatically
if the LLM returns neither content nor tool calls (Listing~\ref{lst:loop}).

\begin{lstlisting}[language=Python, caption={Core agent loop implementation}, label={lst:loop}]
def run_agent_loop(messages: list, model: str):
    while True:
        response = call_ollama(messages, model, tools=TOOLS)

        # Guard: retry on empty response
        if not response["content"] and not response["tool_calls"]:
            print("Empty response from AI. Retrying...")
            continue

        assistant_msg = {"role": "assistant",
                         "content": response["content"]}

        if response["tool_calls"]:
            tool_results = []
            for tc in response["tool_calls"]:
                fn = tc.get("function", {})
                result = dispatch_tool(fn["name"],
                                       fn.get("arguments", {}))
                tool_results.append({"role": "tool",
                                      "content": result})
            assistant_msg["tool_calls"] = response["tool_calls"]
            messages.append(assistant_msg)
            messages.extend(tool_results)
            continue   # Query LLM again with tool results
        else:
            messages.append(assistant_msg)
            break      # Text-only response: end loop
\end{lstlisting}

\subsection{Communication with the Ollama API and Error Handling}

The \texttt{call\_ollama()} function sends JSON requests to Ollama's
\texttt{/api/chat} endpoint with streaming enabled, and processes
server-sent events line by line using only \texttt{urllib.request}
from Python's standard library.

Comprehensive error handling covers:
\begin{itemize}
  \item \texttt{urllib.error.HTTPError}: Ollama server HTTP errors (4xx/5xx)
  \item \texttt{urllib.error.URLError + socket.timeout}:
    distinguishes timeouts (300 s) from connection-refused errors
    (Ollama not running)
  \item \texttt{socket.timeout / TimeoutError}:
    socket-level timeout fallback
\end{itemize}

\subsection{Python Downstream Analysis and Automatic Report Generation}

\texttt{tool\_execute\_python()} executes Python code via
\texttt{subprocess.run()} with a timeout of 300 seconds.
Before execution, a preamble is prepended that injects
\texttt{FIGURE\_DIR}, \texttt{PLOT\_CONFIG}, and \texttt{FIGURE\_FORMAT}
variables, allowing the LLM to generate analysis code without
knowing the exact file paths.
Each execution appends a structured entry (step name, figure paths,
statistical results) to the global \texttt{ANALYSIS\_LOG}.

\texttt{tool\_build\_report\_tex()} reads \texttt{ANALYSIS\_LOG}
and programmatically generates complete TeX source --- no LLM
involvement required, making it fast and deterministic.
Separate Japanese (XeLaTeX + xeCJK) and English (pdflatex) documents
are produced and compiled to PDF via tectonic.

\subsection{Automatic Manifest Generation}

The \texttt{tool\_generate\_manifest()} function uses regular expressions
(\texttt{re.sub(count=1)}) to parse FASTQ filenames,
automatically generating QIIME2 manifest TSV files for both
paired-end and single-end data. R2 file lookup is implemented
with a dictionary for O(1) performance, scaling to large datasets.
If no R1/R2 pairs are found, the function returns an error
without writing an empty manifest. Partial matching rates below 80\%
trigger an enhanced warning showing the mismatch percentage.
Path translation to Docker container-internal paths
(default: \texttt{/data/output}) is performed automatically.

\subsection{Cross-Platform Compatibility}

Three platform-specific differences are handled explicitly:

\begin{itemize}
  \item \textbf{Docker detection}:
    On macOS, the Docker Desktop binary path
    (\texttt{/Applications/Docker.app/.../docker})
    is checked first; on other platforms, \texttt{shutil.which("docker")}
    is used.
  \item \textbf{Windows ANSI color support}:
    Calling \texttt{os.system("")} activates ANSI escape sequence
    processing in Windows 10+ terminals.
  \item \textbf{Separated launch scripts}:
    Bash shell scripts for macOS/Linux and
    PowerShell + batch files for Windows are provided separately.
\end{itemize}

% -------------------------------------------------------
\section{Analysis Workflow}
% -------------------------------------------------------

The QIIME2 pipeline generated by seq2pipe consists of eight steps
as summarized in Table~\ref{tab:workflow}.

\begin{table}[h]
  \centering
  \caption{Overview of the generated QIIME2 analysis pipeline}
  \label{tab:workflow}
  \begin{tabular}{clp{7.5cm}}
    \toprule
    \textbf{Step} & \textbf{Process} & \textbf{Key Command} \\
    \midrule
    1 & Data import &
      \texttt{qiime tools import} \\
    2 & Quality visualization &
      \texttt{qiime demux summarize} \\
    3 & Denoising (ASV generation) &
      \texttt{qiime dada2 denoise-paired} \\
    4 & Feature table summarization &
      \texttt{qiime feature-table summarize} \\
    5 & Phylogenetic tree construction &
      \texttt{qiime phylogeny align-to-tree-mafft-fasttree} \\
    6 & Taxonomic classification &
      \texttt{qiime feature-classifier classify-sklearn} \\
    7 & Diversity analysis &
      \texttt{qiime diversity core-metrics-phylogenetic} \\
    8 & Differential abundance (opt.) &
      \texttt{qiime composition ancombc} \\
    \bottomrule
  \end{tabular}
\end{table}

Taxonomic classification is performed using a Naive Bayes classifier
trained on the SILVA 138 reference database~\cite{silva}.
Primer trim lengths and truncation positions are automatically
adjusted based on the detected hypervariable region
(V1--V3: 27F/338R, V3--V4: 341F/806R, V4: 515F/806R).

Following QIIME2 analysis, the autonomous exploration mode executes
five phases of Python downstream analysis:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Phase 1 (alpha\_diversity)}:
    Shannon / Simpson / Chao1 indices with statistical tests
  \item \textbf{Phase 2 (beta\_diversity)}:
    Bray-Curtis PCoA with PERMANOVA
  \item \textbf{Phase 3 (taxonomy)}:
    Phylum/genus-level stacked bar charts and heatmaps
  \item \textbf{Phase 4 (differential\_abundance)}:
    All-ASV Mann-Whitney U with Benjamini-Hochberg correction and volcano plot
  \item \textbf{Phase 5 (machine\_learning)}:
    Random Forest 5-fold CV with feature importance visualization
\end{enumerate}

Upon completion, \texttt{build\_report\_tex} automatically generates
bilingual TeX/PDF reports from \texttt{ANALYSIS\_LOG}.

% -------------------------------------------------------
\section{Supported Models and Performance}
% -------------------------------------------------------

seq2pipe is model-agnostic and can be used with any LLM available
in Ollama. Table~\ref{tab:models} lists recommended models.

\begin{table}[h]
  \centering
  \caption{Supported models}
  \label{tab:models}
  \begin{tabular}{llll}
    \toprule
    \textbf{Model} & \textbf{RAM Required} & \textbf{Size} & \textbf{Characteristics} \\
    \midrule
    \texttt{qwen2.5-coder:7b} & 8 GB+ & \textasciitilde4.7 GB &
      Code generation optimized (recommended) \\
    \texttt{qwen2.5-coder:3b} & 4 GB+ & \textasciitilde1.9 GB &
      Lightweight and fast \\
    \texttt{llama3.2:3b}      & 4 GB+ & \textasciitilde2.0 GB &
      General purpose, strong dialogue \\
    \texttt{qwen3:8b}         & 16 GB+ & \textasciitilde5.2 GB &
      Highest quality, strong reasoning \\
    \bottomrule
  \end{tabular}
\end{table}

The code-generation-optimized \texttt{qwen2.5-coder:7b} is recommended
as the default model. For RAM-constrained environments (e.g., 8 GB MacBook Air),
\texttt{qwen2.5-coder:3b} or \texttt{llama3.2:3b} provide a
lighter-weight alternative with minimal capability trade-off
for structured QIIME2 command generation.

% -------------------------------------------------------
\section{Setup and Launch}
% -------------------------------------------------------

\subsection{Software Requirements}

Table~\ref{tab:requirements} summarizes the required software stack.

\begin{table}[h]
  \centering
  \caption{Software requirements}
  \label{tab:requirements}
  \begin{tabular}{lll}
    \toprule
    \textbf{Software} & \textbf{Purpose} & \textbf{Installation} \\
    \midrule
    Python 3.9+ & Agent runtime & Typically pre-installed \\
    Ollama & Local LLM inference & Automated via \texttt{setup.sh} \\
    Docker Desktop/Engine & QIIME2 execution environment & Manual install \\
    numpy, pandas, etc. & Python downstream analysis &
      Automated via \texttt{setup.sh} \\
    tectonic (optional) & PDF report compilation &
      \texttt{brew install tectonic} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Launch Sequence}

\begin{enumerate}[leftmargin=2em]
  \item Run \texttt{setup.sh} (macOS/Linux) or \texttt{setup.bat} (Windows)
    to install Ollama, download the LLM model, and install Python packages.
  \item Run \texttt{launch.sh} / \texttt{launch.bat}: the script verifies
    Ollama and Docker availability, then starts the agent.
  \item Select Japanese or English at the language prompt.
  \item Through natural language dialogue, provide the data directory path
    and specify the desired analyses
    (taxonomic composition, diversity, differential abundance, etc.).
  \item The agent automatically inspects the data, generates a customized
    script bundle, runs Python analyses, and produces a PDF report.
\end{enumerate}

% -------------------------------------------------------
\section{Discussion}
% -------------------------------------------------------

\subsection{Achieved Goals}

seq2pipe achieves the following:

\begin{itemize}
  \item Zero-dependency implementation using only Python's standard library
  \item Fully offline operation via local LLM inference
  \item Complete cross-platform support (macOS / Linux / Windows)
  \item Automatic recognition of FASTQ data structure and adaptive pipeline generation
  \item Safe QIIME2 command execution with explicit user confirmation
  \item Python-based 5-phase downstream analysis (diversity, taxonomy,
    differential abundance, machine learning)
  \item Automatic bilingual (Japanese/English) TeX/PDF report generation
    from \texttt{ANALYSIS\_LOG} without LLM involvement
  \item Startup language selection UI (Japanese / English)
  \item Robust handling of connection errors, timeouts, empty responses,
    and filesystem errors
\end{itemize}

\subsection{Current Limitations}

\begin{itemize}
  \item The accuracy of generated QIIME2 commands depends on
    the underlying LLM model quality; incorrect parameters may be produced.
  \item Performance with large datasets (100+ samples) has not been formally evaluated.
  \item Initial SILVA 138 classifier training requires approximately 30 GB
    of disk space and several hours of computation.
  \item PDF report compilation requires tectonic to be installed separately.
\end{itemize}

\subsection{Future Directions}

\begin{itemize}
  \item Implementation of automated command validation and error correction
  \item Web UI integration (Gradio or Streamlit)
  \item Support for additional marker genes (ITS, 18S rRNA)
  \item Feedback loop incorporating pipeline execution results
    to iteratively refine parameters
  \item Parallel tool execution for faster autonomous exploration
\end{itemize}

% -------------------------------------------------------
\section{Conclusion}
% -------------------------------------------------------

We have described seq2pipe, an interactive AI agent that integrates
local LLM inference with the QIIME2 microbiome analysis platform.
By embedding comprehensive QIIME2 domain knowledge into the system prompt
and providing 11 specialized tools through Ollama's function calling interface,
seq2pipe enables researchers to automate an entire 16S rRNA analysis workflow ---
from raw FASTQ data through statistical analysis, publication-quality figures,
and bilingual research reports --- entirely offline and without any
cloud dependencies.

seq2pipe is released as open source under the MIT License and is available at:
\texttt{https://github.com/Rhizobium-gits/seq2pipe}

% -------------------------------------------------------
% References
% -------------------------------------------------------
\begin{thebibliography}{9}

\bibitem{qiime2}
  Bolyen, E., et al. (2019).
  \textit{Reproducible, interactive, scalable and extensible microbiome
    data science using QIIME 2}.
  Nature Biotechnology, 37, 852--857.

\bibitem{brown2020gpt3}
  Brown, T. B., et al. (2020).
  \textit{Language Models are Few-Shot Learners}.
  Advances in Neural Information Processing Systems, 33, 1877--1901.

\bibitem{yao2023react}
  Yao, S., et al. (2023).
  \textit{ReAct: Synergizing Reasoning and Acting in Language Models}.
  International Conference on Learning Representations (ICLR 2023).

\bibitem{ollama}
  Ollama (2023).
  \textit{Ollama: Get up and running with large language models locally}.
  \url{https://ollama.com/}

\bibitem{silva}
  Quast, C., et al. (2013).
  \textit{The SILVA ribosomal RNA gene database project:
    improved data processing and web-based tools}.
  Nucleic Acids Research, 41(D1), D590--D596.

\bibitem{dada2}
  Callahan, B. J., et al. (2016).
  \textit{DADA2: High-resolution sample inference from Illumina amplicon data}.
  Nature Methods, 13(7), 581--583.

\end{thebibliography}

\end{document}
