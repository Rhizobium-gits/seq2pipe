% seq2pipe: A Local LLM-Powered AI Agent for Automated QIIME2 Pipeline Generation
% Compile with: pdflatex seq2pipe_en.tex  or  tectonic seq2pipe_en.tex

\documentclass[12pt, a4paper]{article}

% -------------------------------------------------------
% Packages
% -------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{top=25mm, bottom=25mm, left=25mm, right=25mm}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  urlcolor=blue!70!black,
  citecolor=blue!70!black,
  pdftitle={seq2pipe: A Local LLM-Powered AI Agent for Automated QIIME2 Pipeline Generation},
  pdfauthor={Rhizobium-gits, Claude (Anthropic)}
}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{abstract}
\usepackage{microtype}

% -------------------------------------------------------
% Code listing style
% -------------------------------------------------------
\definecolor{codebg}{RGB}{245,245,245}
\definecolor{codecomment}{RGB}{100,100,100}
\definecolor{codestring}{RGB}{160,40,40}
\definecolor{codekeyword}{RGB}{0,80,160}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{codebg},
  commentstyle=\color{codecomment}\itshape,
  keywordstyle=\color{codekeyword}\bfseries,
  stringstyle=\color{codestring},
  basicstyle=\ttfamily\small,
  breakatwhitespace=false,
  breaklines=true,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  numbersep=8pt,
  showspaces=false,
  showstringspaces=false,
  tabsize=2,
  frame=single,
  rulecolor=\color{gray!40},
  xleftmargin=15pt,
}
\lstset{style=mystyle}

% -------------------------------------------------------
% Header / Footer
% -------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small seq2pipe Technical Report}
\fancyhead[R]{\small February 2026}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% -------------------------------------------------------
% Section format
% -------------------------------------------------------
\titleformat{\section}{\large\bfseries}{}{0em}{\thesection\quad}
\titleformat{\subsection}{\normalsize\bfseries}{}{0em}{\thesubsection\quad}

% -------------------------------------------------------
% Document
% -------------------------------------------------------
\begin{document}

% Title block
\begin{center}
  {\LARGE \textbf{seq2pipe}}\\[6pt]
  {\large A Local LLM-Powered AI Agent for Automated QIIME2 Pipeline Generation}\\[12pt]
  {\normalsize Rhizobium-gits \quad Claude (Anthropic)}\\[4pt]
  {\small \texttt{https://github.com/Rhizobium-gits/seq2pipe}}\\[4pt]
  {\small February 23, 2026}
\end{center}

\vspace{4mm}
\hrule
\vspace{4mm}

% Abstract
\begin{abstract}
We present \textbf{seq2pipe}, an interactive AI agent that automates end-to-end
microbiome analysis by combining a locally running large language model (LLM)
with the QIIME2 bioinformatics platform.
Given raw FASTQ sequencing data, seq2pipe automatically inspects
the data structure, designs an appropriate QIIME2 analysis pipeline,
generates ready-to-execute shell scripts, and then invokes a second
\textbf{tool-calling code-generation agent} (\texttt{code\_agent.py})
that reads actual exported file contents before writing Python code ---
eliminating format-mismatch errors common in blind one-shot generation.
The code agent follows a ``NEVER GIVE UP'' policy: on any Python error
it rewrites and retries until \texttt{EXIT CODE: 0}.
In autonomous mode it automatically produces 14 publication-quality figures
across 5 analysis phases (quality control, alpha diversity, beta diversity
including CLR-PCA and NMDS, taxonomic composition, and sample correlation).
The entire workflow runs on the user's local machine, eliminating
dependencies on cloud services or paid APIs.
A startup language selector (Japanese/English), automatic Python dependency
checking, and comprehensive error handling make seq2pipe immediately
usable by researchers without bioinformatics backgrounds.
\end{abstract}

\vspace{4mm}
\hrule
\vspace{6mm}

% -------------------------------------------------------
\section{Introduction}
% -------------------------------------------------------

QIIME2~\cite{qiime2} (Quantitative Insights Into Microbial Ecology 2)
has become the de facto standard platform for 16S rRNA amplicon sequencing
analysis in microbiome research. However, QIIME2 carries a steep learning curve:
users must understand multiple data formats, select appropriate parameters for
denoising algorithms (DADA2), manage Docker-containerized execution environments,
and interpret multi-step analysis outputs. These barriers make QIIME2 inaccessible
to many researchers, particularly those without bioinformatics backgrounds.
Furthermore, visualization of QIIME2 outputs (\texttt{.qzv} artifacts) typically
requires an online viewer (\texttt{view.qiime2.org}), creating an additional
dependency that is unavailable in offline or restricted-network environments.

The rapid advancement of large language models (LLMs) has opened the door to
domain-specific analysis automation through natural language~\cite{brown2020gpt3}.
Yet cloud-hosted LLM services introduce concerns about cost, data privacy,
and network availability. The emergence of local LLM inference frameworks
such as Ollama~\cite{ollama} allows high-quality language reasoning to be
performed entirely on commodity hardware, resolving these concerns.

In this report, we describe the design, architecture, and implementation of
\textbf{seq2pipe}, an interactive AI agent that integrates local LLM inference
with QIIME2 to provide a fully automated, offline-capable microbiome analysis
assistant. seq2pipe goes well beyond pipeline generation: it performs Python-based
downstream statistical analysis, saves publication-quality figures, and
automatically generates bilingual research reports --- all within a single
conversational interface.

% -------------------------------------------------------
\section{Background and Related Work}
% -------------------------------------------------------

\subsection{Complexity of QIIME2 Analysis}

A complete QIIME2 microbiome analysis involves at least eight major steps:
data import, quality inspection, denoising (ASV generation via DADA2),
feature table summarization, phylogenetic tree construction,
taxonomic classification (using SILVA~\cite{silva}),
diversity analysis, and differential abundance analysis.
Each step requires careful selection of command-line parameters that depend on
the sequencing library configuration (paired-end vs.\ single-end),
the 16S rRNA hypervariable region amplified (V1--V3, V3--V4, V4, etc.),
and primer sequences.

\subsection{LLM Agents and Function Calling}

The ReAct framework~\cite{yao2023react} established the paradigm of
LLMs that interleave reasoning and action --- calling external tools
at appropriate times to gather information or perform operations
that cannot be accomplished through text generation alone.
Modern instruction-tuned LLMs supporting function calling
can select and invoke pre-defined tools with structured arguments,
enabling autonomous task completion.

\subsection{Local LLM Inference with Ollama}

Ollama wraps the \texttt{llama.cpp} inference engine into a single binary
distributable across macOS, Linux, and Windows.
It exposes a REST API (\texttt{/api/chat}) compatible with the
OpenAI Chat Completions format, including support for function calling,
making it an ideal backend for locally deployed AI agents.

% -------------------------------------------------------
\section{System Design}
% -------------------------------------------------------

\subsection{Design Principles}

seq2pipe was developed according to seven core design principles:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Zero external dependencies}:
    Only Python's standard library (\texttt{json}, \texttt{urllib},
    \texttt{subprocess}, \texttt{pathlib}, \texttt{socket}, etc.) is used,
    eliminating the need for \texttt{pip install}.
  \item \textbf{Fully local execution}:
    Ollama's local inference engine is used exclusively,
    requiring no internet connection after initial setup.
  \item \textbf{Cross-platform compatibility}:
    The agent runs on macOS, Linux, and Windows.
  \item \textbf{Safe command execution}:
    All shell commands require explicit user confirmation before execution.
  \item \textbf{Embedded domain knowledge}:
    The system prompt encodes a complete QIIME2 workflow knowledge base.
  \item \textbf{Multilingual UI}:
    The user selects Japanese or English at startup; all AI responses
    and auto-generated reports follow this preference.
  \item \textbf{Robust error handling}:
    Connection errors, timeouts, empty responses, and filesystem errors
    are all caught and reported with user-friendly messages.
\end{enumerate}

\subsection{Overall Architecture}

Figure~\ref{fig:arch} illustrates the three-layer architecture of seq2pipe.
The user launches \texttt{cli.py} via platform-specific launch scripts.
\texttt{cli.py} presents a rainbow ASCII banner and prompts the user to
select between two operating modes.
In either mode, \texttt{pipeline\_runner.py} first executes the full
QIIME2 pipeline and exports the results, then
\texttt{code\_agent.py} --- a vibe-local-style tool-calling agent ---
performs Python downstream analysis.

\begin{figure}[h]
  \centering
  \begin{verbatim}
  User
    |
    v
  [launch.sh / launch.bat] -> [cli.py]  (rainbow banner / mode selection)
                                 |
          +-----------------------+------------------------+
          |                       |                        |
          v                       v                        v
  Mode 1: Analysis mode   [pipeline_runner.py]    Mode 2: Autonomous
  (user specifies prompt)  QIIME2 pipeline +        (--auto flag)
                           result export
          |                       |                        |
          v                       v                        v
  [code_agent.py] <--------------+-------------------> [code_agent.py]
    Tool-calling code generation agent (vibe-local style)
    |
    +---> Ollama (localhost:11434) <-- Local LLM
    |       TOOL FIRST: read files first, then write code
    |       NEVER GIVE UP: rewrite + retry until EXIT CODE: 0
    |
    +-- list_files      (enumerate exported directory)
    +-- read_file       (read file contents -> LLM sees column names)
    +-- write_file      (atomic write via mkstemp+replace)
    +-- run_python      (execute with QIIME2 conda Python)
    `-- install_package (detect ModuleNotFoundError -> pip + approval)

  Autonomous task (Mode 2): 5 phases * 14 figures
    Phase 0: Quality check (denoising statistics)
    Phase 1: Alpha diversity (Shannon/Chao1/Simpson/Faith's PD + stats)
    Phase 2: Beta diversity (PCoA + CLR-PCA + NMDS + rarefaction curves)
    Phase 3: Taxonomy (phylum stacked bar + genus heatmap + CLR bar)
    Phase 4: Sample correlation matrix + hierarchical clustering

  [qiime2_agent.py]  QIIME2 pipeline generation (11 tools, inside pipeline_runner.py)
    +-- inspect_directory / read_file / write_file / edit_file
    +-- generate_manifest / run_command / check_system
    +-- set_plot_config / execute_python / build_report_tex / log_analysis_step
  \end{verbatim}
  \caption{Three-layer architecture of seq2pipe}
  \label{fig:arch}
\end{figure}

% -------------------------------------------------------
\section{Implementation Details}
% -------------------------------------------------------

\subsection{Startup Sequence and Multilingual UI}

On launch, \texttt{select\_language()} prompts the user to choose
Japanese (\texttt{ja}) or English (\texttt{en}).
The selection is stored in the \texttt{LANG} global variable, and the
\texttt{ui()} function returns all interface text in the chosen language.
Auto-generated reports follow the same language setting.

\texttt{check\_python\_deps()} verifies that numpy, pandas, matplotlib,
and seaborn are available at startup via \texttt{subprocess},
guiding the user to install any missing packages before analysis begins.

\subsection{Embedding Domain Knowledge in the System Prompt}

The \texttt{SYSTEM\_PROMPT} variable contains a comprehensive QIIME2
workflow knowledge base. This includes:

\begin{itemize}
  \item Automatic data format detection criteria
    (e.g., paired-end detection via \texttt{*\_R1*.fastq.gz} patterns)
  \item Complete QIIME2 commands for all eight analysis steps
    (import through differential abundance analysis)
  \item Region-specific recommended parameters for
    V1--V3 (27F/338R), V3--V4 (341F/806R), and V4 (515F/806R) amplicons
  \item Docker execution command templates
  \item Metadata file format specifications
  \item SILVA 138 taxonomic hierarchy explanation
  \item Common errors and troubleshooting guidance
  \item Python downstream analysis and autonomous exploration mode guidelines
\end{itemize}

By embedding this knowledge directly into the system prompt,
a general-purpose code LLM is transformed into a QIIME2 domain expert.

\subsection{Tool Definitions and Function Calling}

seq2pipe uses two separate tool sets.
\texttt{qiime2\_agent.py} defines 11 tools (Table~\ref{tab:tools_qiime2})
for QIIME2 pipeline orchestration.
\texttt{code\_agent.py} defines 5 tools (Table~\ref{tab:tools_code})
for Python code generation following the vibe-local paradigm:
the LLM \emph{reads the actual file contents before writing code},
eliminating format-mismatch errors.

\begin{table}[h]
  \centering
  \caption{Tools in \texttt{qiime2\_agent.py} (QIIME2 pipeline orchestration)}
  \label{tab:tools_qiime2}
  \begin{tabular}{lp{7.5cm}}
    \toprule
    \textbf{Tool Name} & \textbf{Function} \\
    \midrule
    \texttt{inspect\_directory} &
      Lists files with sizes; auto-identifies FASTQ, QZA, metadata \\
    \texttt{read\_file} &
      Reads text files (TSV, CSV, Markdown, etc.) up to 50 lines \\
    \texttt{write\_file} &
      Writes scripts, manifests, and README files \\
    \texttt{edit\_file} &
      Replaces a unique string in an existing file \\
    \texttt{generate\_manifest} &
      Auto-generates QIIME2 manifest TSV from a FASTQ directory \\
    \texttt{run\_command} &
      Executes shell commands after user confirmation;
      auto-detects QIIME2 conda bin, supports \texttt{SEQ2PIPE\_AUTO\_YES} \\
    \texttt{check\_system} &
      Verifies Docker, Ollama, Python, and disk space \\
    \texttt{set\_plot\_config} &
      Configures matplotlib style, palette, DPI, and output format \\
    \texttt{execute\_python} &
      Executes Python analysis code; saves figures to \texttt{FIGURE\_DIR};
      logs steps to \texttt{ANALYSIS\_LOG} \\
    \texttt{build\_report\_tex} &
      Builds TeX/PDF reports from \texttt{ANALYSIS\_LOG} via tectonic \\
    \texttt{log\_analysis\_step} &
      Manually registers QIIME2 steps into \texttt{ANALYSIS\_LOG} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{Tools in \texttt{code\_agent.py} (vibe-local style code generation)}
  \label{tab:tools_code}
  \begin{tabular}{lp{8cm}}
    \toprule
    \textbf{Tool Name} & \textbf{Function} \\
    \midrule
    \texttt{list\_files} &
      Lists all files in the exported results directory with their sizes \\
    \texttt{read\_file} &
      Returns the full content of a file (TSV, CSV, Python, etc.) to the LLM,
      allowing it to inspect column names and data format \emph{before} writing code \\
    \texttt{write\_file} &
      Writes a Python script atomically (via \texttt{mkstemp}+\texttt{replace})
      to prevent partial writes \\
    \texttt{run\_python} &
      Executes the written script using the QIIME2 conda Python interpreter;
      returns stdout, stderr, and exit code; detects new figure files \\
    \texttt{install\_package} &
      Detects \texttt{ModuleNotFoundError}, prompts user for approval,
      then runs \texttt{pip install} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{The Code Agent Loop (vibe-local Style)}

\texttt{run\_coding\_agent()} in \texttt{code\_agent.py} implements
a ``read-first, code-second'' tool-calling loop.
The LLM is instructed to \textbf{always call a tool immediately}
(TOOL FIRST principle) and \textbf{never give up on errors}
(NEVER GIVE UP principle).
The typical sequence per analysis task is:

\begin{enumerate}[leftmargin=2em]
  \item \texttt{list\_files} --- discover available exported files
  \item \texttt{read\_file} --- read a target file to inspect
    column names and data format
  \item \texttt{write\_file} --- generate a Python script that uses
    the exact column names seen in step 2
  \item \texttt{run\_python} --- execute the script;
    if exit code $\neq 0$, the LLM reads the traceback and loops
    back to \texttt{write\_file} to fix the error
\end{enumerate}

Listing~\ref{lst:loop} shows the core loop structure.

\begin{lstlisting}[language=Python, caption={Core tool-calling loop in run\_coding\_agent()}, label={lst:loop}]
def run_coding_agent(export_files, user_prompt, ...):
    messages = [{"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user",   "content": task}]
    steps = 0
    while steps < max_steps:
        response = call_ollama(messages, model, tools=_TOOL_DEFS)
        tool_calls = response.get("tool_calls", [])

        if not tool_calls:
            break  # LLM says it's done

        for tc in tool_calls:
            fn   = tc["function"]
            name = fn["name"]
            args = fn["arguments"]  # dict or JSON string
            if isinstance(args, str):
                args = json.loads(args)

            result, new_figs = _exec_tool(name, args, ...)
            figures.extend(new_figs)
            messages.append({"role": "tool",
                              "name": name,
                              "content": result[:4000]})
        steps += 1

    return CodeExecutionResult(success=bool(figures), figures=figures)
\end{lstlisting}

\subsection{Robustness Enhancements for Small LLMs}

Smaller models (7B parameters and below) often do not emit structured
\texttt{tool\_calls} objects via the Ollama API; instead, they embed
JSON tool invocations as plain text in the response body.
seq2pipe handles this through a four-layer fallback system implemented
in \texttt{run\_coding\_agent()}:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Text-based tool call parser (\texttt{\_parse\_text\_tool\_calls})}:
    When \texttt{tool\_calls} is empty, the response body is searched
    for JSON objects using five heuristic patterns:
    (a) \texttt{```json} code blocks,
    (b) the entire response as a JSON object,
    (c) inline regex scan for \texttt{"name": "..."} patterns,
    (d) name-less JSON objects inferred as \texttt{write\_file}/\texttt{list\_files}
        by key inspection, and
    (e) a lenient regex-based broken-JSON extractor.

  \item \textbf{Auto-inject \texttt{run\_python}}:
    Immediately after \texttt{write\_file} successfully writes a
    \texttt{.py} file, \texttt{run\_python} is automatically executed
    without waiting for the LLM to call it --- avoiding the common
    failure mode where small models write a script but forget to run it.

  \item \textbf{Step-6 fallback to 1-shot generation}:
    A \texttt{\_run\_python\_count} counter tracks how many times
    \texttt{run\_python} has been called (both via tool calls and
    auto-inject). If no execution has occurred after 5 steps,
    \texttt{run\_coding\_agent()} falls back to \texttt{run\_code\_agent()},
    which performs 1-shot code generation with up to 3 error-correction
    retries --- ensuring figure output even when the tool-calling loop stalls.

  \item \textbf{Repetition detector in \texttt{call\_ollama()}}:
    A sliding-window check truncates generation when the same 50-character
    chunk appears four consecutive times, or when total output exceeds
    20\,000 characters --- preventing infinite loops caused by degenerate
    model outputs.
\end{enumerate}

Together, these mechanisms guarantee that at least one analysis figure
is produced regardless of model size or Ollama version.

\subsection{The QIIME2 Pipeline Agent Loop}

\texttt{run\_agent\_loop()} in \texttt{qiime2\_agent.py} controls
the QIIME2 pipeline generation cycle.
It sends conversation history to the LLM, detects tool calls,
executes them sequentially, appends results, and repeats until
the LLM produces a text-only response.
An empty-response guard retries automatically if the LLM returns
neither content nor tool calls.

\subsection{Communication with the Ollama API and Error Handling}

The \texttt{call\_ollama()} function sends JSON requests to Ollama's
\texttt{/api/chat} endpoint with streaming enabled, and processes
server-sent events line by line using only \texttt{urllib.request}
from Python's standard library.

Comprehensive error handling covers:
\begin{itemize}
  \item \texttt{urllib.error.HTTPError}: Ollama server HTTP errors (4xx/5xx)
  \item \texttt{urllib.error.URLError + socket.timeout}:
    distinguishes timeouts (300 s) from connection-refused errors
    (Ollama not running)
  \item \texttt{socket.timeout / TimeoutError}:
    socket-level timeout fallback
\end{itemize}

\subsection{Python Downstream Analysis and Automatic Report Generation}

\texttt{tool\_execute\_python()} executes Python code via
\texttt{subprocess.run()} with a timeout of 300 seconds.
Before execution, a preamble is prepended that injects
\texttt{FIGURE\_DIR}, \texttt{PLOT\_CONFIG}, and \texttt{FIGURE\_FORMAT}
variables, allowing the LLM to generate analysis code without
knowing the exact file paths.
Each execution appends a structured entry (step name, figure paths,
statistical results) to the global \texttt{ANALYSIS\_LOG}.

\texttt{tool\_build\_report\_tex()} reads \texttt{ANALYSIS\_LOG}
and programmatically generates complete TeX source --- no LLM
involvement required, making it fast and deterministic.
Separate Japanese (XeLaTeX + xeCJK) and English (pdflatex) documents
are produced and compiled to PDF via tectonic.

\subsection{Automatic Manifest Generation}

The \texttt{tool\_generate\_manifest()} function uses regular expressions
(\texttt{re.sub(count=1)}) to parse FASTQ filenames,
automatically generating QIIME2 manifest TSV files for both
paired-end and single-end data. R2 file lookup is implemented
with a dictionary for O(1) performance, scaling to large datasets.
If no R1/R2 pairs are found, the function returns an error
without writing an empty manifest. Partial matching rates below 80\%
trigger an enhanced warning showing the mismatch percentage.
Path translation to Docker container-internal paths
(default: \texttt{/data/output}) is performed automatically.

\subsection{Cross-Platform Compatibility}

Three platform-specific differences are handled explicitly:

\begin{itemize}
  \item \textbf{Docker detection}:
    On macOS, the Docker Desktop binary path
    (\texttt{/Applications/Docker.app/.../docker})
    is checked first; on other platforms, \texttt{shutil.which("docker")}
    is used.
  \item \textbf{Windows ANSI color support}:
    Calling \texttt{os.system("")} activates ANSI escape sequence
    processing in Windows 10+ terminals.
  \item \textbf{Separated launch scripts}:
    Bash shell scripts for macOS/Linux and
    PowerShell + batch files for Windows are provided separately.
\end{itemize}

% -------------------------------------------------------
\section{Analysis Workflow}
% -------------------------------------------------------

The QIIME2 pipeline generated by seq2pipe consists of eight steps
as summarized in Table~\ref{tab:workflow}.

\begin{table}[h]
  \centering
  \caption{Overview of the generated QIIME2 analysis pipeline}
  \label{tab:workflow}
  \begin{tabular}{clp{7.5cm}}
    \toprule
    \textbf{Step} & \textbf{Process} & \textbf{Key Command} \\
    \midrule
    1 & Data import &
      \texttt{qiime tools import} \\
    2 & Quality visualization &
      \texttt{qiime demux summarize} \\
    3 & Denoising (ASV generation) &
      \texttt{qiime dada2 denoise-paired} \\
    4 & Feature table summarization &
      \texttt{qiime feature-table summarize} \\
    5 & Phylogenetic tree construction &
      \texttt{qiime phylogeny align-to-tree-mafft-fasttree} \\
    6 & Taxonomic classification &
      \texttt{qiime feature-classifier classify-sklearn} \\
    7 & Diversity analysis &
      \texttt{qiime diversity core-metrics-phylogenetic} \\
    8 & Differential abundance (opt.) &
      \texttt{qiime composition ancombc} \\
    \bottomrule
  \end{tabular}
\end{table}

Taxonomic classification is performed using a Naive Bayes classifier
trained on the SILVA 138 reference database~\cite{silva}.
Primer trim lengths and truncation positions are automatically
adjusted based on the detected hypervariable region
(V1--V3: 27F/338R, V3--V4: 341F/806R, V4: 515F/806R).

Following QIIME2 analysis, the autonomous agent mode (\texttt{--auto})
executes five phases of Python downstream analysis, producing
\textbf{14 publication-quality figures}.
Unlike simple script injection, the code agent first calls
\texttt{read\_file} on each target TSV to confirm exact column names,
then writes and executes analysis code.

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Phase 0 (quality)}:
    Denoising statistics --- input, filtered, denoised, non-chimeric read counts
  \item \textbf{Phase 1 (alpha diversity)}:
    Shannon / Chao1 / Simpson / Faith's PD with
    Mann-Whitney U and Kruskal-Wallis tests
  \item \textbf{Phase 2 (beta diversity)}:
    Bray-Curtis PCoA, UniFrac PCoA,
    CLR-transformed PCA (principal component analysis on compositional data),
    NMDS (non-metric multidimensional scaling),
    and rarefaction curves across all available beta-diversity TSV files
  \item \textbf{Phase 3 (taxonomy)}:
    Phylum-level stacked bar chart (relative abundance),
    genus-level heatmap with hierarchical clustering,
    and CLR-transformed phylum bar chart
  \item \textbf{Phase 4 (sample correlation)}:
    Pairwise sample correlation matrix with hierarchical clustering heatmap
\end{enumerate}

Upon completion, \texttt{build\_report\_tex} automatically generates
bilingual TeX/PDF reports from \texttt{ANALYSIS\_LOG}.

% -------------------------------------------------------
\section{Supported Models and Performance}
% -------------------------------------------------------

seq2pipe is model-agnostic and can be used with any LLM available
in Ollama. Table~\ref{tab:models} lists recommended models.

\begin{table}[h]
  \centering
  \caption{Supported models}
  \label{tab:models}
  \begin{tabular}{llll}
    \toprule
    \textbf{Model} & \textbf{RAM Required} & \textbf{Size} & \textbf{Characteristics} \\
    \midrule
    \texttt{qwen2.5-coder:7b} & 8 GB+ & \textasciitilde4.7 GB &
      Code generation optimized (recommended) \\
    \texttt{qwen2.5-coder:3b} & 4 GB+ & \textasciitilde1.9 GB &
      Lightweight and fast \\
    \texttt{llama3.2:3b}      & 4 GB+ & \textasciitilde2.0 GB &
      General purpose, strong dialogue \\
    \texttt{qwen3:8b}         & 16 GB+ & \textasciitilde5.2 GB &
      Highest quality, strong reasoning \\
    \bottomrule
  \end{tabular}
\end{table}

The code-generation-optimized \texttt{qwen2.5-coder:7b} is recommended
as the default model. For RAM-constrained environments (e.g., 8 GB MacBook Air),
\texttt{qwen2.5-coder:3b} or \texttt{llama3.2:3b} provide a
lighter-weight alternative with minimal capability trade-off
for structured QIIME2 command generation.

% -------------------------------------------------------
\section{Setup and Launch}
% -------------------------------------------------------

\subsection{Software Requirements}

Table~\ref{tab:requirements} summarizes the required software stack.

\begin{table}[h]
  \centering
  \caption{Software requirements}
  \label{tab:requirements}
  \begin{tabular}{lll}
    \toprule
    \textbf{Software} & \textbf{Purpose} & \textbf{Installation} \\
    \midrule
    Python 3.9+ & Agent runtime & Typically pre-installed \\
    Ollama & Local LLM inference & Automated via \texttt{setup.sh} \\
    Docker Desktop/Engine & QIIME2 execution environment & Manual install \\
    numpy, pandas, etc. & Python downstream analysis &
      Automated via \texttt{setup.sh} \\
    tectonic (optional) & PDF report compilation &
      \texttt{brew install tectonic} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Launch Sequence}

\begin{enumerate}[leftmargin=2em]
  \item Run \texttt{setup.sh} (macOS/Linux) or \texttt{setup.bat} (Windows)
    to install Ollama, download the LLM model, and install Python packages.
  \item Run \texttt{launch.sh} / \texttt{launch.bat}: the script verifies
    Ollama and Docker availability, then starts the agent.
  \item Select Japanese or English at the language prompt.
  \item Through natural language dialogue, provide the data directory path
    and specify the desired analyses
    (taxonomic composition, diversity, differential abundance, etc.).
  \item The agent automatically inspects the data, generates a customized
    script bundle, runs Python analyses, and produces a PDF report.
\end{enumerate}

% -------------------------------------------------------
\section{Example Results}
% -------------------------------------------------------

To demonstrate seq2pipe's end-to-end capability, we analysed
10 human stool samples (TEST01--TEST10, freeze-dried, Illumina MiSeq
paired-end V3--V4) without manual intervention.
The full pipeline ran in a single command using \texttt{\_run\_pipeline.py},
which invoked QIIME2 (DADA2 denoising + core-metrics-phylogenetic) and
then handed the exported data to \texttt{chat\_agent.InteractiveSession}
for downstream visualisation.
Six figures were generated automatically and compiled into a PDF report.
Figures are stored in the \texttt{Figure/} directory of this repository.

\subsection{Alpha Diversity}

Four alpha diversity metrics (Shannon entropy, Faith's PD, Pielou's
evenness, and observed features) were computed per sample and displayed
as a 2$\times$2 panel with strip-plot overlay (Figure~\ref{fig:alpha}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.92\textwidth]{../Figure/alpha_diversity.png}
  \caption{Alpha diversity of 10 human stool samples.
    Each panel shows a different metric computed from the DADA2-denoised
    feature table. Strip plots overlay individual sample values on the
    aggregate distribution.}
  \label{fig:alpha}
\end{figure}

\subsection{Beta Diversity}

Beta diversity was assessed using three dissimilarity measures.
Bray--Curtis PCoA (Figure~\ref{fig:bc}) and Unweighted UniFrac PCoA
(Figure~\ref{fig:unifrac}) were computed via scikit-learn MDS
(\texttt{n\_components=2, dissimilarity=`precomputed'}).
Jaccard distances were visualised as a clustered heatmap
(Figure~\ref{fig:jaccard}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{../Figure/bray_curtis_pcoa.png}
  \caption{Bray--Curtis PCoA. Samples are coloured by ID (tab10 palette)
    and labelled directly on the plot. MDS coordinates were computed from
    the QIIME2-exported distance matrix.}
  \label{fig:bc}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{../Figure/unifrac_pcoa.png}
  \caption{Unweighted UniFrac PCoA. Phylogenetic distances computed by
    QIIME2's core-metrics-phylogenetic pipeline are projected onto
    two MDS dimensions.}
  \label{fig:unifrac}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{../Figure/jaccard_heatmap.png}
  \caption{Jaccard distance heatmap with hierarchical clustering (Ward
    linkage, seaborn \texttt{clustermap}). Both axes show the 10 samples;
    colour intensity indicates pairwise dissimilarity.}
  \label{fig:jaccard}
\end{figure}

\subsection{DADA2 Denoising Statistics}

The progression of reads through each DADA2 stage
(input $\rightarrow$ filtered $\rightarrow$ denoised $\rightarrow$
merged $\rightarrow$ non-chimeric) is shown in Figure~\ref{fig:dada2}.
The grouped bar chart was generated directly from
\texttt{denoising\_stats/stats.tsv} without any manual intervention.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.92\textwidth]{../Figure/dada2_stats.png}
  \caption{DADA2 denoising statistics for 10 stool samples.
    Each group of bars represents one sample; bar colours distinguish
    the five processing stages. The figure reveals consistent filtering
    efficiency across samples.}
  \label{fig:dada2}
\end{figure}

\subsection{Shannon Diversity Violin Plot}

Figure~\ref{fig:shannon} shows a horizontal violin plot of Shannon
diversity, with samples sorted by their Shannon value and a vertical
mean line superimposed.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{../Figure/shannon_violin.png}
  \caption{Shannon alpha diversity per sample as a horizontal violin plot,
    sorted in ascending order. The vertical line marks the mean Shannon
    value across all samples.}
  \label{fig:shannon}
\end{figure}

All six figures and accompanying interpretive text were incorporated
into a Japanese PDF report (\texttt{report\_ja.pdf}) automatically by
\texttt{InteractiveSession.generate\_report()}, which calls the LLM to
write a figure caption for each plot and compiles the result with
\texttt{tectonic}.

% -------------------------------------------------------
\section{Discussion}
% -------------------------------------------------------

\subsection{Achieved Goals}

seq2pipe achieves the following:

\begin{itemize}
  \item Zero-dependency implementation using only Python's standard library
  \item Fully offline operation via local LLM inference
  \item Complete cross-platform support (macOS / Linux / Windows)
  \item Automatic recognition of FASTQ data structure and adaptive pipeline generation
  \item Safe QIIME2 command execution with explicit user confirmation
  \item \textbf{Vibe-local style tool-calling code agent} that reads actual file
    contents before writing code, eliminating format-mismatch errors that plagued
    blind one-shot generation approaches
  \item \textbf{Automated error correction}: when \texttt{run\_python} fails,
    the agent reads the traceback, rewrites the script, and retries until
    \texttt{EXIT CODE: 0} (NEVER GIVE UP policy)
  \item \textbf{5-phase autonomous analysis producing 14 figures}:
    quality control, alpha diversity (4 metrics), beta diversity
    (PCoA + CLR-PCA + NMDS + rarefaction curves), taxonomy (3 chart types),
    and sample correlation
  \item \textbf{ModuleNotFoundError auto-recovery}: detects missing packages,
    prompts user for approval, and runs \texttt{pip install}
  \item \textbf{Small-LLM robustness}: four-layer fallback system
    (\texttt{\_parse\_text\_tool\_calls}, auto-inject \texttt{run\_python},
    step-6 1-shot fallback, repetition detector) ensures reliable figure
    generation even with 7B parameter models
  \item Two operation modes: directed (Mode 1 — natural language prompt)
    and fully autonomous (Mode 2 — \texttt{--auto} flag)
  \item Automatic bilingual (Japanese/English) TeX/PDF report generation
    from \texttt{ANALYSIS\_LOG} without LLM involvement
  \item Startup language selection UI (Japanese / English)
  \item Robust handling of connection errors, timeouts, empty responses,
    and filesystem errors
\end{itemize}

\subsection{Current Limitations}

\begin{itemize}
  \item The accuracy of generated QIIME2 commands depends on
    the underlying LLM model quality; incorrect parameters may be produced.
  \item Performance with large datasets (100+ samples) has not been formally evaluated.
  \item Initial SILVA 138 classifier training requires approximately 30 GB
    of disk space and several hours of computation.
  \item PDF report compilation requires tectonic to be installed separately.
  \item The QIIME2 pipeline agent (\texttt{qiime2\_agent.py}) does not yet
    apply the read-first pattern; integrating it with \texttt{code\_agent.py}'s
    approach is a future goal.
\end{itemize}

\subsection{Future Directions}

\begin{itemize}
  \item Web UI integration (Gradio or Streamlit) --- a Streamlit prototype
    (\texttt{app.py}) is already planned
  \item Support for additional marker genes (ITS, 18S rRNA)
  \item Feedback loop incorporating pipeline execution results
    to iteratively refine parameters
  \item Parallel tool execution for faster autonomous exploration
  \item Integration of statistical result annotation directly into figures
  \item Extending the autonomous task list with differential abundance
    (volcano plots) and machine learning (Random Forest) analyses
\end{itemize}

% -------------------------------------------------------
\section{Conclusion}
% -------------------------------------------------------

We have described seq2pipe, an interactive AI agent that integrates
local LLM inference with the QIIME2 microbiome analysis platform.
The system combines two complementary LLM agents:
\texttt{qiime2\_agent.py} orchestrates 11 specialized tools for
QIIME2 pipeline generation, while \texttt{code\_agent.py} --- a
vibe-local-style tool-calling agent with 5 tools --- generates
accurate Python analysis code by reading actual file contents
before writing code and automatically correcting errors until success.
Together they enable researchers to automate an entire 16S rRNA analysis workflow ---
from raw FASTQ data through 14 publication-quality figures across 5 analysis phases,
and bilingual research reports --- entirely offline and without any cloud dependencies.

seq2pipe is released as open source under the MIT License and is available at:
\texttt{https://github.com/Rhizobium-gits/seq2pipe}

% -------------------------------------------------------
% References
% -------------------------------------------------------
\begin{thebibliography}{9}

\bibitem{qiime2}
  Bolyen, E., et al. (2019).
  \textit{Reproducible, interactive, scalable and extensible microbiome
    data science using QIIME 2}.
  Nature Biotechnology, 37, 852--857.

\bibitem{brown2020gpt3}
  Brown, T. B., et al. (2020).
  \textit{Language Models are Few-Shot Learners}.
  Advances in Neural Information Processing Systems, 33, 1877--1901.

\bibitem{yao2023react}
  Yao, S., et al. (2023).
  \textit{ReAct: Synergizing Reasoning and Acting in Language Models}.
  International Conference on Learning Representations (ICLR 2023).

\bibitem{ollama}
  Ollama (2023).
  \textit{Ollama: Get up and running with large language models locally}.
  \url{https://ollama.com/}

\bibitem{silva}
  Quast, C., et al. (2013).
  \textit{The SILVA ribosomal RNA gene database project:
    improved data processing and web-based tools}.
  Nucleic Acids Research, 41(D1), D590--D596.

\bibitem{dada2}
  Callahan, B. J., et al. (2016).
  \textit{DADA2: High-resolution sample inference from Illumina amplicon data}.
  Nature Methods, 13(7), 581--583.

\end{thebibliography}

\end{document}
