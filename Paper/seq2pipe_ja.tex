% seq2pipe: ローカルLLMによるQIIME2パイプライン自動生成AIエージェント
% XeLaTeX + xeCJK でコンパイル
% tectonic seq2pipe_ja.tex  または  xelatex seq2pipe_ja.tex

\documentclass[12pt, a4paper]{article}

% -------------------------------------------------------
% パッケージ
% -------------------------------------------------------
\usepackage{fontspec}
\usepackage{xeCJK}
\setCJKmainfont{Hiragino Mincho ProN}   % macOS 標準日本語フォント
\setCJKsansfont{Hiragino Sans}
\setCJKmonofont{Hiragino Sans}

\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{top=25mm, bottom=25mm, left=25mm, right=25mm}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  urlcolor=blue!70!black,
  citecolor=blue!70!black,
  pdftitle={seq2pipe: ローカルLLMによるQIIME2パイプライン自動生成AIエージェント},
  pdfauthor={Rhizobium-gits, Claude (Anthropic)}
}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{abstract}

% -------------------------------------------------------
% コードブロックのスタイル
% -------------------------------------------------------
\definecolor{codebg}{RGB}{245,245,245}
\definecolor{codecomment}{RGB}{100,100,100}
\definecolor{codestring}{RGB}{160,40,40}
\definecolor{codekeyword}{RGB}{0,80,160}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{codebg},
  commentstyle=\color{codecomment}\itshape,
  keywordstyle=\color{codekeyword}\bfseries,
  stringstyle=\color{codestring},
  basicstyle=\ttfamily\small,
  breakatwhitespace=false,
  breaklines=true,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  numbersep=8pt,
  showspaces=false,
  showstringspaces=false,
  tabsize=2,
  frame=single,
  rulecolor=\color{gray!40},
  xleftmargin=15pt,
}
\lstset{style=mystyle}

% -------------------------------------------------------
% ヘッダー・フッター
% -------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small seq2pipe 技術レポート}
\fancyhead[R]{\small 2026年2月}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% -------------------------------------------------------
% セクション書式
% -------------------------------------------------------
\titleformat{\section}{\large\bfseries}{}{0em}{\thesection\quad}
\titleformat{\subsection}{\normalsize\bfseries}{}{0em}{\thesubsection\quad}

% -------------------------------------------------------
% ドキュメント開始
% -------------------------------------------------------
\begin{document}

% タイトル
\begin{center}
  {\LARGE \textbf{seq2pipe}}\\[6pt]
  {\large ローカル LLM による QIIME2 パイプライン自動生成 AI エージェント}\\[12pt]
  {\normalsize Rhizobium-gits \quad Claude (Anthropic)}\\[4pt]
  {\small \texttt{https://github.com/Rhizobium-gits/seq2pipe}}\\[4pt]
  {\small 2026 年 2 月 23 日}
\end{center}

\vspace{4mm}
\hrule
\vspace{4mm}

% アブストラクト
\begin{abstract}
本稿では、ローカル大規模言語モデル（LLM）を用いたマイクロバイオーム解析自動化エージェント
\textbf{seq2pipe} の設計と実装について報告する。
seq2pipe はユーザーが保有する生 FASTQ データを入力として受け取り、
データ構造の自動認識・QIIME2 解析パイプラインの設計・実行可能シェルスクリプトの生成を行い、
続いて \textbf{ツール呼び出し型コード生成エージェント}（\texttt{code\_agent.py}）が
実際のエクスポートファイルの内容を先に読んでからコードを生成するため、
フォーマットの不一致によるエラーを根本的に排除する。
コードエージェントは「NEVER GIVE UP」方針に従い、
エラーが発生するたびにコードを修正して \texttt{EXIT CODE: 0} になるまで実行を繰り返す。
自律モードでは 5 フェーズ・14 種類の図を全自動で生成する
（クオリティ確認、α多様性、β多様性、分類組成、サンプル相関）。
処理はすべてユーザーのローカル環境で完結し、
クラウドサービスや有料 API への依存を排除する。
\end{abstract}

\vspace{4mm}
\hrule
\vspace{6mm}

% -------------------------------------------------------
\section{はじめに}
% -------------------------------------------------------

マイクロバイオーム研究において QIIME2~\cite{qiime2} は
16S rRNA アンプリコンシーケンシングデータの標準的な解析プラットフォームとして
広く普及している。しかし、QIIME2 の学習コストは高く、
データ形式の理解・適切なパラメータ選択・Docker を介した実行環境の構築など、
初心者にとっての障壁が大きい。
さらに QIIME2 の出力（\texttt{.qzv} アーティファクト）の可視化には
\texttt{view.qiime2.org} への依存があり、オフライン環境では解析結果の確認も困難である。

近年、大規模言語モデル（LLM）の急速な発展により、
自然言語によるコード生成やドメイン特化型の解析支援が実用化されつつある~\cite{brown2020gpt3}。
一方で、従来のクラウド型 LLM サービスはコスト・プライバシー・ネットワーク依存性という問題を持つ。
Ollama~\cite{ollama} をはじめとするローカル LLM 実行フレームワークの登場により、
これらの課題をクリアしながら高度な言語処理を実現することが可能となった。

本研究では、これらの技術を統合した対話型 AI エージェント \textbf{seq2pipe} を開発し、
その設計原則・アーキテクチャ・実装詳細を報告する。
seq2pipe は QIIME2 解析の自動化に留まらず、Python による下流解析・統計検定・
機械学習・可視化、そして日本語・英語両対応の自動レポート生成まで
一貫したパイプラインを提供する点で、従来ツールを大きく超える。

% -------------------------------------------------------
\section{背景と関連研究}
% -------------------------------------------------------

\subsection{QIIME2 における解析の複雑性}

QIIME2 は、データのインポートからデノイジング（DADA2）・系統樹構築・分類学的解析・
多様性解析・差次解析まで、多段階の処理を必要とする。
各ステップで適切なコマンドとパラメータを選択するには
シーケンシングのライブラリ構成（ペアエンド/シングルエンド）・
増幅領域（V1--V3, V3--V4, V4 など）・プライマー配列の知識が不可欠である。

\subsection{LLM エージェントの台頭}

ReAct フレームワーク~\cite{yao2023react} に代表されるように、
LLM が思考と行動を交互に実行することで複雑なタスクを自律的に処理する
「エージェント」パターンが確立されている。
関数呼び出し（Function Calling）機能を持つ LLM は、
外部ツールを適切なタイミングで呼び出しながら、
ユーザーの意図を達成することができる。

\subsection{ローカル LLM の実用化}

Ollama は \texttt{llama.cpp} を基盤とし、macOS・Linux・Windows 上で
量子化された LLM モデルをシングルバイナリで実行できる。
OpenAI の Chat Completions API と互換性のある
REST API（\texttt{/api/chat}）を提供しており、
関数呼び出しを含む高度な推論が可能である。

% -------------------------------------------------------
\section{システム設計}
% -------------------------------------------------------

\subsection{設計原則}

seq2pipe は以下の設計原則に従って開発した。

\begin{enumerate}[leftmargin=2em]
  \item \textbf{外部依存ゼロ}: Python 標準ライブラリ（\texttt{json}, \texttt{urllib},
    \texttt{subprocess}, \texttt{pathlib}, \texttt{socket} 等）のみを使用し、
    \texttt{pip install} を不要とする。
  \item \textbf{ローカル完結}: Ollama のローカル推論エンジンを使用し、
    インターネット接続を不要とする（初期セットアップを除く）。
  \item \textbf{クロスプラットフォーム}: macOS・Linux・Windows のすべてで動作する。
  \item \textbf{安全なコマンド実行}: コマンド実行前にユーザーへの確認を必須とする。
  \item \textbf{QIIME2 ドメイン知識の内包}: システムプロンプトに QIIME2 の
    完全なワークフロー知識を埋め込む。
  \item \textbf{多言語 UI}: 起動時に日本語 / 英語を選択し、
    AI 応答・自動生成レポートを統一する。
  \item \textbf{堅牢なエラー処理}: 接続エラー・タイムアウト・空レスポンス・
    ファイルシステムエラーをすべて適切にキャッチし、ユーザーに分かりやすいメッセージを返す。
\end{enumerate}

\subsection{全体アーキテクチャ}

図~\ref{fig:arch} に seq2pipe の 3 層アーキテクチャを示す。
ユーザーは起動スクリプトを通じてエントリーポイント（\texttt{cli.py}）を起動する。
\texttt{cli.py} は虹色 ASCII バナーを表示し、
2 つの操作モード（指定解析モード / 自律エージェントモード）を選択させる。
いずれのモードでも \texttt{pipeline\_runner.py} が QIIME2 パイプラインを実行して
結果をエクスポートし、続いて \texttt{code\_agent.py} が
vibe-local 方式のツール呼び出し型コード生成を行う。

\begin{figure}[h]
  \centering
  \begin{verbatim}
  ユーザー
    |
    v
  [launch.sh / launch.bat] -> [cli.py]  (バナー・モード選択)
                                  |
         +------------------------+---------------------------+
         |                        |                           |
         v                        v                           v
  モード 1: 指定解析      [pipeline_runner.py]        モード 2: 自律
  （自然言語でプロンプト）  QIIME2 実行 + エクスポート   （--auto フラグ）
         |                        |                           |
         v                        v                           v
  [code_agent.py] <--------------+--------------------> [code_agent.py]
    ツール呼び出し型コード生成エージェント（vibe-local 方式）
    |
    +---> Ollama (localhost:11434)  <-- ローカル LLM
    |       TOOL FIRST: 先にファイルを読んでからコードを書く
    |       NEVER GIVE UP: エラー → write_file 修正 → run_python 再実行
    |
    +-- list_files      (エクスポートファイル一覧)
    +-- read_file       (ファイル内容 → LLM が列名・形式を確認)
    +-- write_file      (atomic write で Python スクリプト生成)
    +-- run_python      (QIIME2 conda Python で実行 → exit code 確認)
    `-- install_package (ModuleNotFoundError 検出 → pip + ユーザー承認)

  自動解析タスク（モード 2）: 5 フェーズ・14 種類の図
    Phase 0: クオリティ確認（デノイジング統計）
    Phase 1: α多様性（Shannon/Chao1/Simpson/Faith's PD + 統計検定）
    Phase 2: β多様性（PCoA + CLR-PCA + NMDS + レアファクション曲線）
    Phase 3: 分類組成（phylum bar + genus heatmap + CLR bar）
    Phase 4: サンプル相関（相関行列 + 階層クラスタリング）

  [qiime2_agent.py]  QIIME2 パイプライン生成（11 ツール、pipeline_runner.py 内部）
    +-- inspect_directory / read_file / write_file / edit_file
    +-- generate_manifest / run_command / check_system
    +-- set_plot_config / execute_python / build_report_tex / log_analysis_step
  \end{verbatim}
  \caption{seq2pipe の 3 層アーキテクチャ}
  \label{fig:arch}
\end{figure}

% -------------------------------------------------------
\section{実装詳細}
% -------------------------------------------------------

\subsection{起動シーケンスと多言語 UI}

エージェントの起動時に \texttt{select\_language()} が呼ばれ、
ユーザーは日本語（\texttt{ja}）または英語（\texttt{en}）を選択する。
選択は \texttt{LANG} グローバル変数に保存され、
\texttt{ui()} 関数が全 UI テキストを選択言語で返す。
自動生成されるレポートも同じ言語設定を使用する。

また \texttt{check\_python\_deps()} が起動時に numpy・pandas・matplotlib・seaborn
の存在を \texttt{subprocess} 経由で確認し、不足する場合はインストールコマンドを案内する。

\subsection{システムプロンプトへのドメイン知識埋め込み}

\texttt{SYSTEM\_PROMPT} 変数に QIIME2 の完全なワークフロー知識を記述した。
これには以下の情報が含まれる。

\begin{itemize}
  \item データ形式の自動判定基準
    （\texttt{*\_R1*.fastq.gz} パターンによるペアエンド検出など）
  \item 全 8 ステップの QIIME2 コマンド（インポート〜差次解析）
  \item 増幅領域別推奨パラメータ（V1--V3, V3--V4, V4）
  \item Docker 実行テンプレート
  \item メタデータファイル形式仕様
  \item SILVA 138 分類階層の説明
  \item 一般的なエラーとトラブルシューティング
  \item Python 下流解析・自律探索モードの実行指針
\end{itemize}

この手法により、汎用 LLM モデルが QIIME2 専門家として振る舞うことを可能にした。

\subsection{ツール定義と関数呼び出し}

seq2pipe は 2 種類のツールセットを持つ。
\texttt{qiime2\_agent.py} が QIIME2 パイプライン生成用の 11 ツール（表~\ref{tab:tools_qiime2}）を提供し、
\texttt{code\_agent.py} が vibe-local 方式の Python コード生成用 5 ツール（表~\ref{tab:tools_code}）を提供する。
コードエージェントは、LLM がコードを書く前に必ず \texttt{read\_file} でファイルの
列名・形式を確認するよう指示されており、盲目的なコード生成によるフォーマットエラーを排除する。

\begin{table}[h]
  \centering
  \caption{\texttt{qiime2\_agent.py} のツール一覧（QIIME2 パイプライン生成）}
  \label{tab:tools_qiime2}
  \begin{tabular}{lp{7cm}}
    \toprule
    \textbf{ツール名} & \textbf{機能} \\
    \midrule
    \texttt{inspect\_directory} &
      ファイル一覧・サイズ取得、FASTQ/QZA/メタデータを自動判定 \\
    \texttt{read\_file} &
      テキストファイルを最大 50 行読み込む \\
    \texttt{write\_file} &
      スクリプト・マニフェスト・README を書き出す \\
    \texttt{edit\_file} &
      既存ファイルの文字列を置換する \\
    \texttt{generate\_manifest} &
      FASTQ ディレクトリから QIIME2 マニフェスト TSV を自動生成 \\
    \texttt{run\_command} &
      ユーザー確認後にシェルコマンドを実行；
      conda 環境を自動検出・\texttt{SEQ2PIPE\_AUTO\_YES} 対応 \\
    \texttt{check\_system} &
      Docker・Ollama・Python・ディスク容量を確認 \\
    \texttt{set\_plot\_config} &
      matplotlib スタイル・カラー・DPI・フォント・保存形式を設定 \\
    \texttt{execute\_python} &
      Python コードを実行し図を保存、\texttt{ANALYSIS\_LOG} に記録 \\
    \texttt{build\_report\_tex} &
      \texttt{ANALYSIS\_LOG} から TeX 構築・tectonic でコンパイル \\
    \texttt{log\_analysis\_step} &
      QIIME2 操作を \texttt{ANALYSIS\_LOG} に手動登録 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{\texttt{code\_agent.py} のツール一覧（vibe-local 方式コード生成）}
  \label{tab:tools_code}
  \begin{tabular}{lp{7.5cm}}
    \toprule
    \textbf{ツール名} & \textbf{機能} \\
    \midrule
    \texttt{list\_files} &
      エクスポートディレクトリのファイル一覧とサイズを返す \\
    \texttt{read\_file} &
      ファイルの全内容を LLM に渡す。
      コード生成前に列名・データ形式を確認させることで精度を向上させる \\
    \texttt{write\_file} &
      \texttt{mkstemp}+\texttt{replace} による atomic write で
      Python スクリプトを安全に生成する \\
    \texttt{run\_python} &
      QIIME2 conda Python で実行；stdout・stderr・exit code を返す；
      新規生成図ファイルを自動検出する \\
    \texttt{install\_package} &
      \texttt{ModuleNotFoundError} を検出してユーザーに承認を求め、
      承認されれば \texttt{pip install} を実行する \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{コードエージェントループの実装（vibe-local 方式）}

\texttt{code\_agent.py} の \texttt{run\_coding\_agent()} は
「先に読む、後で書く」原則に基づくツール呼び出しループを実装する。
LLM には \textbf{TOOL FIRST}（即座にツールを呼び出す）と
\textbf{NEVER GIVE UP}（エラーが出ても修正して再実行）を指示しており、
典型的な 1 解析タスクの流れは以下の通りである。

\begin{enumerate}[leftmargin=2em]
  \item \texttt{list\_files} でエクスポートファイルを把握
  \item \texttt{read\_file} で対象ファイルの列名・データ形式を確認
  \item \texttt{write\_file} で実際の列名を使った Python スクリプトを生成
  \item \texttt{run\_python} で実行し、exit code $\neq 0$ ならトレースバックを読んで
    \texttt{write\_file} で修正し再実行
\end{enumerate}

リスト~\ref{lst:loop} にループの中核部分を示す。

\begin{lstlisting}[language=Python, caption={run\_coding\_agent() のツール呼び出しループ}, label={lst:loop}]
def run_coding_agent(export_files, user_prompt, ...):
    messages = [{"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user",   "content": task}]
    steps = 0
    while steps < max_steps:
        response = call_ollama(messages, model, tools=_TOOL_DEFS)
        tool_calls = response.get("tool_calls", [])

        if not tool_calls:
            break  # LLM が完了と判断

        for tc in tool_calls:
            fn   = tc["function"]
            name = fn["name"]
            args = fn["arguments"]   # dict or JSON 文字列
            if isinstance(args, str):
                args = json.loads(args)

            result, new_figs = _exec_tool(name, args, ...)
            figures.extend(new_figs)
            messages.append({"role": "tool",
                              "name": name,
                              "content": result[:4000]})
        steps += 1

    return CodeExecutionResult(success=bool(figures), figures=figures)
\end{lstlisting}

\subsection{QIIME2 パイプラインエージェントループ}

\texttt{qiime2\_agent.py} の \texttt{run\_agent\_loop()} は
QIIME2 パイプライン生成の推論・行動サイクルを制御する。
LLM の応答にツール呼び出しが含まれる場合は順次実行して結果を会話履歴に追加し、
テキスト応答のみになるまで繰り返す。
空レスポンスのガード処理も実装している。

\subsection{Ollama API との通信と堅牢なエラー処理}

\texttt{call\_ollama()} 関数は Ollama の \texttt{/api/chat} エンドポイントに
JSON リクエストを送信し、ストリーミングレスポンスを行単位で受信する。
Python の \texttt{urllib.request} モジュールのみを使用しており、
\texttt{requests} 等の外部パッケージに依存しない。

エラー処理として以下を実装した。
\begin{itemize}
  \item \texttt{urllib.error.HTTPError}: Ollama サーバーの HTTP エラー（4xx/5xx）
  \item \texttt{urllib.error.URLError + socket.timeout}: タイムアウト（600 秒、
    環境変数 \texttt{SEQ2PIPE\_PYTHON\_TIMEOUT} で変更可）と
    サーバー未起動（\texttt{Connection refused}）を区別して表示
  \item \texttt{socket.timeout / TimeoutError}: ソケットレベルのタイムアウト
\end{itemize}

\subsection{Python 下流解析と自動レポート生成}

\texttt{tool\_execute\_python()} は Python コードを
\texttt{subprocess.run()} 経由で実行する。
実行前に \texttt{FIGURE\_DIR}・\texttt{PLOT\_CONFIG}・
\texttt{FIGURE\_FORMAT} 変数を自動注入するプリアンブルを付加するため、
LLM はファイルパスを意識せずに解析コードを生成できる。
実行結果のステップ・図パス・統計結果は \texttt{ANALYSIS\_LOG} に自動蓄積される。

\texttt{tool\_build\_report\_tex()} は \texttt{ANALYSIS\_LOG} の内容から
TeX ソースを Python で完全生成する（LLM を使用しない）。
日本語版（XeLaTeX + xeCJK）と英語版（pdflatex）を別々に生成し、
tectonic でコンパイルして PDF を出力する。

\subsection{マニフェスト自動生成}

\texttt{tool\_generate\_manifest()} は FASTQ ファイルの命名規則
（\texttt{\_R1\_} / \texttt{\_R2\_} パターン）を正規表現（\texttt{re.sub(count=1)}）で解析し、
ペアエンド・シングルエンド双方の QIIME2 マニフェスト TSV を自動生成する。
R2 ファイルの探索は辞書ルックアップ（O(1)）で実装し大規模データセットにも対応する。
Docker コンテナ内パス（デフォルト: \texttt{/data/output}）への変換も自動で行う。
ペアが 1 組も見つからない場合はファイルを書かずにエラーを返す。

\subsection{クロスプラットフォーム対応}

macOS・Linux・Windows の差異を吸収するために以下の対策を実装した。

\begin{itemize}
  \item \textbf{Docker 検出}:
    macOS では Docker Desktop の固定パス
    (\texttt{/Applications/Docker.app/.../docker}) を優先し、
    その他の OS では \texttt{shutil.which("docker")} を使用する。
  \item \textbf{Windows ANSI カラー}:
    \texttt{os.system("")} を呼ぶことで Windows 10 以降における
    ANSI エスケープシーケンスを有効化する。
  \item \textbf{起動スクリプトの分離}:
    macOS/Linux 用（Bash）と Windows 用（PowerShell + バッチ）を別々に提供する。
\end{itemize}

% -------------------------------------------------------
\section{解析ワークフロー}
% -------------------------------------------------------

seq2pipe が生成する QIIME2 パイプラインは表~\ref{tab:workflow} の 8 ステップで構成される。

\begin{table}[h]
  \centering
  \caption{生成される QIIME2 解析パイプラインの概要}
  \label{tab:workflow}
  \begin{tabular}{clp{7cm}}
    \toprule
    \textbf{STEP} & \textbf{処理} & \textbf{主要コマンド} \\
    \midrule
    1 & データインポート &
      \texttt{qiime tools import} \\
    2 & クオリティ確認 &
      \texttt{qiime demux summarize} \\
    3 & デノイジング（ASV 生成） &
      \texttt{qiime dada2 denoise-paired} \\
    4 & フィーチャーテーブル確認 &
      \texttt{qiime feature-table summarize} \\
    5 & 系統樹構築 &
      \texttt{qiime phylogeny align-to-tree-mafft-fasttree} \\
    6 & 分類学的解析 &
      \texttt{qiime feature-classifier classify-sklearn} \\
    7 & 多様性解析 &
      \texttt{qiime diversity core-metrics-phylogenetic} \\
    8 & 差次解析（オプション） &
      \texttt{qiime composition ancombc} \\
    \bottomrule
  \end{tabular}
\end{table}

SILVA 138 参照データベース~\cite{silva} を用いた Naive Bayes 分類器により、
16S rRNA 増幅領域の分類学的同定を行う。
増幅領域（V1--V3: 27F/338R, V3--V4: 341F/806R, V4: 515F/806R）に応じて
プライマートリム長・トランケーション長が自動的に調整される。

QIIME2 解析後、自律エージェントモード（\texttt{--auto}）では以下の
5 フェーズ・\textbf{14 種類の図} を全自動で生成する。
コードエージェントは各ファイルを \texttt{read\_file} で先に読んで
列名・形式を確認してからコードを生成するため、フォーマットエラーが起きにくい。

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Phase 0 (quality)}: デノイジング統計（入力 / フィルタリング / デノイジング / 非キメラ）
  \item \textbf{Phase 1 (alpha)}: Shannon / Chao1 / Simpson / Faith's PD の計算・可視化・
    Mann-Whitney U + Kruskal-Wallis 検定
  \item \textbf{Phase 2 (beta)}: Bray-Curtis PCoA・UniFrac PCoA・
    CLR 変換 PCA（組成データ向け主成分分析）・
    NMDS（非計量多次元尺度構成法）・
    全 β 多様性 TSV ファイルを使ったレアファクション曲線
  \item \textbf{Phase 3 (taxonomy)}: 門レベル stacked bar・属レベル heatmap・CLR 変換 phylum bar
  \item \textbf{Phase 4 (correlation)}: サンプル間相関行列 + 階層クラスタリング heatmap
\end{enumerate}

全フェーズ完了後、\texttt{build\_report\_tex} が \texttt{ANALYSIS\_LOG} から
日英 TeX/PDF レポートを自動生成する。

% -------------------------------------------------------
\section{対応モデルと性能比較}
% -------------------------------------------------------

seq2pipe は Ollama で動作する任意の LLM と組み合わせて使用できる。
表~\ref{tab:models} に推奨モデルとその特性を示す。

\begin{table}[h]
  \centering
  \caption{対応モデル一覧}
  \label{tab:models}
  \begin{tabular}{llll}
    \toprule
    \textbf{モデル} & \textbf{必要 RAM} & \textbf{モデルサイズ} & \textbf{特徴} \\
    \midrule
    \texttt{qwen2.5-coder:7b} & 8 GB 以上 & 約 4.7 GB &
      コード生成特化（推奨） \\
    \texttt{qwen2.5-coder:3b} & 4 GB 以上 & 約 1.9 GB &
      軽量・高速 \\
    \texttt{llama3.2:3b}      & 4 GB 以上 & 約 2.0 GB &
      汎用・対話能力高め \\
    \texttt{qwen3:8b}         & 16 GB 以上 & 約 5.2 GB &
      最高品質・推論能力高い \\
    \bottomrule
  \end{tabular}
\end{table}

% -------------------------------------------------------
\section{考察と今後の課題}
% -------------------------------------------------------

\subsection{達成された目標}

seq2pipe は以下の目標を達成した。

\begin{itemize}
  \item Python 標準ライブラリのみを用いたゼロ依存実装
  \item ローカル LLM による完全オフライン動作
  \item 3 OS（macOS / Linux / Windows）への完全対応
  \item FASTQ データ構造の自動認識と適応的パイプライン生成
  \item QIIME2 コマンドを conda 環境 / Docker 経由で安全に実行するコマンド確認機構
  \item \textbf{vibe-local 方式ツール呼び出し型コード生成エージェント}:
    LLM がファイルを先に読んでから Python コードを生成するため、
    盲目的な 1 ショット生成で頻発したフォーマット不一致エラーを根本解消
  \item \textbf{自動エラー修正（NEVER GIVE UP）}:
    \texttt{run\_python} が失敗するとエージェントがトレースバックを読んで
    コードを修正し、\texttt{EXIT CODE: 0} になるまで再実行
  \item \textbf{5 フェーズ・14 種類の図を自動生成}:
    クオリティ確認・α多様性（4 指標）・β多様性（PCoA + CLR-PCA + NMDS + レアファクション）・
    分類組成（3 図）・サンプル相関
  \item \textbf{ModuleNotFoundError 自動復旧}:
    不足パッケージを検出してユーザーに承認を求め、\texttt{pip install} を実行
  \item 2 つの操作モード: 指定解析（モード 1）・完全自律（モード 2 / \texttt{--auto}）
  \item \texttt{ANALYSIS\_LOG} からの日英 TeX/PDF レポート自動生成（LLM 不使用）
  \item 日本語 / 英語の起動時言語選択 UI
  \item 接続エラー・タイムアウト・空レスポンス・ファイルシステムエラーへの堅牢な処理
\end{itemize}

\subsection{現在の制限}

\begin{itemize}
  \item LLM の生成するコマンドの正確性はモデルに依存し、
    誤ったパラメータが生成される可能性がある。
  \item 大規模データセット（100 サンプル以上）に対するパフォーマンスは未検証である。
  \item SILVA 138 参照ファイルのダウンロードには約 30 GB のディスク容量と数時間を要する。
  \item TeX レポートのコンパイルには tectonic のインストールが必要である。
  \item \texttt{qiime2\_agent.py}（QIIME2 パイプライン生成）には read-first パターンが未適用であり、
    \texttt{code\_agent.py} との統合が今後の課題である。
\end{itemize}

\subsection{今後の課題}

\begin{itemize}
  \item Web UI の提供（Streamlit GUI — \texttt{app.py} として実装予定）
  \item ITS / 18S rRNA など他のマーカー遺伝子への対応
  \item パイプライン実行結果のフィードバックループ実装
  \item 並列ツール実行による処理の高速化
  \item 差次存在量解析（volcano plot）・機械学習（Random Forest）の
    自律タスクへの追加
  \item 統計的検定結果の図への自動アノテーション
\end{itemize}

% -------------------------------------------------------
\section{おわりに}
% -------------------------------------------------------

本稿では、ローカル LLM と QIIME2 を統合した自動解析エージェント
seq2pipe の設計・実装を報告した。
本システムは 2 つの相補的な LLM エージェントを組み合わせる:
\texttt{qiime2\_agent.py} が 11 ツールで QIIME2 パイプライン生成を担い、
\texttt{code\_agent.py} が vibe-local 方式の 5 ツールで
実際のファイル内容を読んでから Python 解析コードを生成し、
エラーが出ても自動修正して最終的に成功させる。
これにより研究者は生 FASTQ データから 5 フェーズ・14 種類の出版品質の図と
日英 PDF レポートまでを、完全オフラインかつクラウド不要の環境で自動化できる。

seq2pipe はオープンソース（MIT ライセンス）として公開されており、
\texttt{https://github.com/Rhizobium-gits/seq2pipe} からアクセスできる。

% -------------------------------------------------------
% 参考文献
% -------------------------------------------------------
\begin{thebibliography}{9}

\bibitem{qiime2}
  Bolyen, E., et al. (2019).
  \textit{Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2}.
  Nature Biotechnology, 37, 852--857.

\bibitem{brown2020gpt3}
  Brown, T. B., et al. (2020).
  \textit{Language Models are Few-Shot Learners}.
  Advances in Neural Information Processing Systems, 33, 1877--1901.

\bibitem{yao2023react}
  Yao, S., et al. (2023).
  \textit{ReAct: Synergizing Reasoning and Acting in Language Models}.
  International Conference on Learning Representations (ICLR 2023).

\bibitem{ollama}
  Ollama (2023).
  \textit{Ollama: Get up and running with large language models locally}.
  \url{https://ollama.com/}

\bibitem{silva}
  Quast, C., et al. (2013).
  \textit{The SILVA ribosomal RNA gene database project: improved data processing and web-based tools}.
  Nucleic Acids Research, 41(D1), D590--D596.

\bibitem{dada2}
  Callahan, B. J., et al. (2016).
  \textit{DADA2: High-resolution sample inference from Illumina amplicon data}.
  Nature Methods, 13(7), 581--583.

\end{thebibliography}

\end{document}
